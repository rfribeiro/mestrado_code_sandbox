{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shift = 4320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hog_features = 'C:\\\\Users\\\\rafae\\\\Desktop\\\\Coleta\\\\features\\\\features_hog_*.csv'\n",
    "lbp_features = 'C:\\\\Users\\\\rafae\\\\Desktop\\\\Coleta\\\\features\\\\features_lbp_*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n",
      "513\n"
     ]
    }
   ],
   "source": [
    "hog_files_list = glob.glob(hog_features)\n",
    "print(len(hog_files_list))\n",
    "\n",
    "lbp_files_list = glob.glob(lbp_features)\n",
    "print(len(lbp_files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read features\n",
    "frames = [pd.read_csv(f, header=0) for f in hog_files_list ]\n",
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7680, 17282)\n"
     ]
    }
   ],
   "source": [
    "print(ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17272</th>\n",
       "      <th>17273</th>\n",
       "      <th>17274</th>\n",
       "      <th>17275</th>\n",
       "      <th>17276</th>\n",
       "      <th>17277</th>\n",
       "      <th>17278</th>\n",
       "      <th>17279</th>\n",
       "      <th>17280</th>\n",
       "      <th>17281</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.042653</td>\n",
       "      <td>0.053141</td>\n",
       "      <td>0.059538</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.127822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069514</td>\n",
       "      <td>0.119957</td>\n",
       "      <td>0.119957</td>\n",
       "      <td>0.119957</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.051084</td>\n",
       "      <td>0.053452</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.010533</td>\n",
       "      <td>0.119957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103166</td>\n",
       "      <td>0.037546</td>\n",
       "      <td>0.044686</td>\n",
       "      <td>0.122575</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.104095</td>\n",
       "      <td>0.087389</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.009884</td>\n",
       "      <td>0.017647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.091092</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.123109</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.050142</td>\n",
       "      <td>0.042432</td>\n",
       "      <td>0.028560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052366</td>\n",
       "      <td>0.038015</td>\n",
       "      <td>0.034714</td>\n",
       "      <td>0.061695</td>\n",
       "      <td>0.040908</td>\n",
       "      <td>0.070168</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.023819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 17282 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1   2   3   4   5   6   7   8   9    ...        17272     17273  \\\n",
       "0  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.071793  0.090339   \n",
       "1  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.069514  0.119957   \n",
       "2  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.103166  0.037546   \n",
       "3  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.132490  0.091092   \n",
       "4  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.052366  0.038015   \n",
       "\n",
       "      17274     17275     17276     17277     17278     17279     17280  \\\n",
       "0  0.127822  0.127822  0.042653  0.053141  0.059538  0.003019  0.019839   \n",
       "1  0.119957  0.119957  0.024254  0.051084  0.053452  0.014108  0.010533   \n",
       "2  0.044686  0.122575  0.033838  0.104095  0.087389  0.003405  0.009884   \n",
       "3  0.132490  0.123109  0.038147  0.132490  0.132490  0.050142  0.042432   \n",
       "4  0.034714  0.061695  0.040908  0.070168  0.032586  0.004734  0.009425   \n",
       "\n",
       "      17281  \n",
       "0  0.127822  \n",
       "1  0.119957  \n",
       "2  0.017647  \n",
       "3  0.028560  \n",
       "4  0.023819  \n",
       "\n",
       "[5 rows x 17282 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4320            NaN\n",
       "4321            NaN\n",
       "4322        1.06381\n",
       "4323       0.763937\n",
       "4324        1.23607\n",
       "4325        1.10417\n",
       "4326        1.35889\n",
       "4327         1.1433\n",
       "4328       0.199255\n",
       "4329       0.640129\n",
       "4330       0.215195\n",
       "4331        1.35889\n",
       "4332        1.35889\n",
       "4333        1.35889\n",
       "4334         1.3145\n",
       "4335       0.501547\n",
       "4336        1.27396\n",
       "4337       0.264652\n",
       "4338        1.02082\n",
       "4339       0.482439\n",
       "4340        1.35889\n",
       "4341        1.35889\n",
       "4342        1.27933\n",
       "4343       0.560927\n",
       "4344       0.536768\n",
       "4345       0.117171\n",
       "4346        0.41077\n",
       "4347       0.634241\n",
       "4348       0.800554\n",
       "4349        1.09175\n",
       "            ...    \n",
       "17252     0.0326264\n",
       "17253      0.127822\n",
       "17254     0.0405616\n",
       "17255      0.118196\n",
       "17256      0.127822\n",
       "17257     0.0726997\n",
       "17258     0.0319039\n",
       "17259     0.0129902\n",
       "17260     0.0240762\n",
       "17261    0.00839542\n",
       "17262     0.0407411\n",
       "17263       0.12265\n",
       "17264    0.00428808\n",
       "17265    0.00883046\n",
       "17266             0\n",
       "17267     0.0227766\n",
       "17268     0.0859962\n",
       "17269      0.127822\n",
       "17270      0.127822\n",
       "17271      0.127822\n",
       "17272     0.0717932\n",
       "17273     0.0903394\n",
       "17274      0.127822\n",
       "17275      0.127822\n",
       "17276     0.0426533\n",
       "17277     0.0531408\n",
       "17278     0.0595381\n",
       "17279    0.00301907\n",
       "17280     0.0198394\n",
       "17281      0.127822\n",
       "Name: 0, Length: 12962, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.iloc[0,4320:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert points do binary Eyes ON or OFF Road\n",
    "on_road_points = [13, 14, 16, 17]\n",
    "on_off_road_bit = np.where(ds['0'].isin(on_road_points), 1,0)\n",
    "print(ds['0'].isin(on_road_points).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yData = on_off_road_bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xData = ds.iloc[:,2:2+shift+shift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(yData.shape, xData.shape, type(yData), type(xData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, rotation=45)\n",
    "    plt.yticks(tick_marks)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Compute confusion matrix\n",
    "def plot_confusion(yTest, yTestPred, name):\n",
    "    cm = confusion_matrix(yTest, yTestPred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "    cm_normalized = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "    print('Classification report')\n",
    "    print(classification_report(yTest, yTestPred))\n",
    "    print('Normalized confusion matrix')\n",
    "    print(cm_normalized)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix (%s)' % (name))\n",
    "\n",
    "    plt.show()\n",
    "    # plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer,precision_recall_fscore_support\n",
    "\n",
    "def search(X_train, X_test, y_train, y_test, group_data_train):\n",
    "    \n",
    "    # normalize data\n",
    "    print(\"Normalizing data!\")\n",
    "    stdScale = preprocessing.StandardScaler().fit(X_train)\n",
    "    xTrain = stdScale.transform(X_train)\n",
    "    xTest = stdScale.transform(X_test)\n",
    "    \n",
    "    print(\"Grid Search Classifiers!\")\n",
    "    \n",
    "    knc = KNeighborsClassifier()\n",
    "    svc = SVC()\n",
    "    rfc = RandomForestClassifier()\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    clf1 = SVC()\n",
    "    clf2 = RandomForestClassifier(random_state=1)\n",
    "    clf3 = GaussianNB()\n",
    "    vt = VotingClassifier(estimators=[('svc', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "    kncp = [{'n_neighbors': [3, 5, 7],\n",
    "             'weights': ['uniform','distance'],\n",
    "              'algorithm': ['auto','ball_tree', 'kd_tree', 'brute']}]\n",
    "    svcp = [{'kernel': ['rbf'], #{'kernel': ['rbf','linear'], \n",
    "             'class_weight':['balanced'],\n",
    "             'gamma': [0.0001],#'gamma': ['auto',1e-3, 1e-4],\n",
    "             'C': [1000]}]#'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 1, 10, 100, 1000]}]\n",
    "    rfcp = [{'n_estimators': [10, 20, 50, 100,200,300], \n",
    "            'max_depth': [None, 1, 10, 100],\n",
    "            'bootstrap': [True, False],\n",
    "            'criterion': [\"gini\", \"entropy\"]}]\n",
    "    gbp = [{#'loss' : ['deviance', 'exponential'],\n",
    "           'n_estimators': [50,100],\n",
    "           'learning_rate': [0.1,1.0,10],\n",
    "           'max_depth' : [3,5,10]\n",
    "            }]\n",
    "           #'min_impurity_decrease': [0.0]}]#, 0.1]}]\n",
    "    vtp = [{'svc__C': [1.0, 100.0], \n",
    "            'rf__n_estimators': [20, 200],}]\n",
    "   \n",
    "    classifiers = [#('kNN', knc, kncp),                                 \n",
    "                    ('Support Vector', svc, svcp),\n",
    "                    #('Random Forest', rfc, rfcp),\n",
    "                    #('Gradient Boosting', gb, gbp),\n",
    "                    #('Vooting', vt, vtp)\n",
    "    ]\n",
    "    \n",
    "    for name, classifier, params in classifiers:\n",
    "        print(name)\n",
    "        clf = GridSearchCV(classifier, params, cv=5, scoring=['f1_weighted','accuracy','precision_weighted', 'recall_weighted'], refit='f1_weighted' , verbose = 10)\n",
    "\n",
    "        clf.fit(X_train, y_train, groups=group_data_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print(clf.best_score_)\n",
    "        print()\n",
    "        print(clf.cv_results_.keys())\n",
    "        #means = clf.cv_results_['mean_test_score']\n",
    "        #stds = clf.cv_results_['std_test_score']\n",
    "        #for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        #    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        #          % (mean, std * 2, params))\n",
    "        #print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        yTrue, yPred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(yTrue, yPred))\n",
    "        plot_confusion(yTrue, yPred, name)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(yData)\n",
    "print(le.classes_)\n",
    "yDataBin = le.transform(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=2, test_size=0.3, random_state=0)\n",
    "\n",
    "for train_index, test_index in gss.split(xData, yDataBin, groups=ds['1']):\n",
    "    print(type(train_index), train_index.shape, test_index.shape)\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(np.unique(ds.iloc[train_index,1]))\n",
    "    print(np.unique(ds.iloc[test_index,1]))\n",
    "    print(ds.iloc[train_index,0].describe())\n",
    "    print(ds.iloc[test_index,0].describe())\n",
    "    search(xData.iloc[train_index,:], xData.iloc[test_index,:], \n",
    "           yDataBin[train_index], yDataBin[test_index], \n",
    "           ds.iloc[train_index,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(xData, yDataBin, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape,\n",
    "      type(X_train.shape), type(X_test.shape), type(y_train.shape), type(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(nds.iloc[:,2:])\n",
    "nnds = pca.transform(nds.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
