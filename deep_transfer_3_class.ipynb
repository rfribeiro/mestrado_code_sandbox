{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os \n",
    "#import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "from random import shuffle\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.applications\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_checkpoint = 'deep_transfer_3_class_v3_ck.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "## Callback for loss logging per epoch\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "history = LossHistory()\n",
    "\n",
    "## Callback for early stopping the training\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(model_weights_checkpoint, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 299, 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could also be the output a different Keras model or layer\n",
    "input_tensor = Input(shape=(img_width, img_height, 3))  # this assumes K.image_data_format() == 'channels_last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "#base_model = keras.applications.ResNet50(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "#base_model = keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add a fully-connected layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(3, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "#model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 864         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 9216        activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 6 18432       activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 6 0           activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 8 5120        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 8 240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, None, None, 8 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 1 138240      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, None, None, 1 0           activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 6 192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, None, None, 6 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 9 55296       activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 4 144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, None, None, 4 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, None, None, 9 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, None, None, 1 0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 9 18432       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 76800       activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 9 82944       activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 6 12288       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 9 288         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 9 288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 6 192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, None, None, 9 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, None, None, 9 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, None, None, 6 0           batch_normalization_12[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "mixed_5b (Concatenate)          (None, None, None, 3 0           activation_153[0][0]             \n",
      "                                                                 activation_155[0][0]             \n",
      "                                                                 activation_158[0][0]             \n",
      "                                                                 activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 3 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 3 96          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, None, None, 3 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 3 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 4 13824       activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 3 96          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 4 144         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, None, None, 3 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, None, None, 4 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 3 10240       mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 3 9216        activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 6 27648       activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 3 96          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 3 96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 6 192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, None, None, 3 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, None, None, 3 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, None, None, 6 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_mixed (Concatenate)   (None, None, None, 1 0           activation_160[0][0]             \n",
      "                                                                 activation_162[0][0]             \n",
      "                                                                 activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_conv (Conv2D)         (None, None, None, 3 41280       block35_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_1 (Lambda)              (None, None, None, 3 0           mixed_5b[0][0]                   \n",
      "                                                                 block35_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_ac (Activation)       (None, None, None, 3 0           block35_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 3 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 3 96          conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, None, None, 3 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 3 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 4 13824       activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 3 96          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 4 144         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, None, None, 3 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, None, None, 4 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 3 10240       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 3 9216        activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 6 27648       activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 3 96          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 3 96          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 6 192         conv2d_24[0][0]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, None, None, 3 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, None, None, 3 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, None, None, 6 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_mixed (Concatenate)   (None, None, None, 1 0           activation_166[0][0]             \n",
      "                                                                 activation_168[0][0]             \n",
      "                                                                 activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_conv (Conv2D)         (None, None, None, 3 41280       block35_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_2 (Lambda)              (None, None, None, 3 0           block35_1_ac[0][0]               \n",
      "                                                                 block35_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_ac (Activation)       (None, None, None, 3 0           block35_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 3 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 3 96          conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, None, None, 3 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 3 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 4 13824       activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 3 96          conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 4 144         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, None, None, 3 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, None, None, 4 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 3 10240       block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 3 9216        activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 6 27648       activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 3 96          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 3 96          conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 6 192         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, None, None, 3 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, None, None, 3 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, None, None, 6 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_mixed (Concatenate)   (None, None, None, 1 0           activation_172[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "                                                                 activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_conv (Conv2D)         (None, None, None, 3 41280       block35_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_3 (Lambda)              (None, None, None, 3 0           block35_2_ac[0][0]               \n",
      "                                                                 block35_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_ac (Activation)       (None, None, None, 3 0           block35_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 3 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 3 96          conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, None, None, 3 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 3 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 4 13824       activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 3 96          conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 4 144         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, None, None, 3 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, None, None, 4 0           batch_normalization_35[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 3 10240       block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 3 9216        activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 6 27648       activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 3 96          conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 3 96          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 6 192         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, None, None, 3 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, None, None, 3 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, None, None, 6 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_mixed (Concatenate)   (None, None, None, 1 0           activation_178[0][0]             \n",
      "                                                                 activation_180[0][0]             \n",
      "                                                                 activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_conv (Conv2D)         (None, None, None, 3 41280       block35_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_4 (Lambda)              (None, None, None, 3 0           block35_3_ac[0][0]               \n",
      "                                                                 block35_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_ac (Activation)       (None, None, None, 3 0           block35_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 3 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 3 96          conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, None, None, 3 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 3 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 4 13824       activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 3 96          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 4 144         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, None, None, 3 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, None, None, 4 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 3 10240       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 3 9216        activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 6 27648       activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 3 96          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 3 96          conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 6 192         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, None, None, 3 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, None, None, 3 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, None, None, 6 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_mixed (Concatenate)   (None, None, None, 1 0           activation_184[0][0]             \n",
      "                                                                 activation_186[0][0]             \n",
      "                                                                 activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_conv (Conv2D)         (None, None, None, 3 41280       block35_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_5 (Lambda)              (None, None, None, 3 0           block35_4_ac[0][0]               \n",
      "                                                                 block35_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_ac (Activation)       (None, None, None, 3 0           block35_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 3 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 3 96          conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, None, None, 3 0           batch_normalization_46[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 3 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 4 13824       activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 3 96          conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 4 144         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, None, None, 3 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, None, None, 4 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 3 10240       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 3 9216        activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 6 27648       activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 3 96          conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 3 96          conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 6 192         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, None, None, 3 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, None, None, 3 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, None, None, 6 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_mixed (Concatenate)   (None, None, None, 1 0           activation_190[0][0]             \n",
      "                                                                 activation_192[0][0]             \n",
      "                                                                 activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_conv (Conv2D)         (None, None, None, 3 41280       block35_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_6 (Lambda)              (None, None, None, 3 0           block35_5_ac[0][0]               \n",
      "                                                                 block35_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_ac (Activation)       (None, None, None, 3 0           block35_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 3 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 3 96          conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, None, None, 3 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 3 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 4 13824       activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 3 96          conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 4 144         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, None, None, 3 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, None, None, 4 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 3 10240       block35_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 3 9216        activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 6 27648       activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 3 96          conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 3 96          conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 6 192         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, None, None, 3 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, None, None, 3 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, None, None, 6 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_mixed (Concatenate)   (None, None, None, 1 0           activation_196[0][0]             \n",
      "                                                                 activation_198[0][0]             \n",
      "                                                                 activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block35_7_conv (Conv2D)         (None, None, None, 3 41280       block35_7_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_7 (Lambda)              (None, None, None, 3 0           block35_6_ac[0][0]               \n",
      "                                                                 block35_7_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_7_ac (Activation)       (None, None, None, 3 0           block35_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 3 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 3 96          conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, None, None, 3 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 3 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 4 13824       activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 3 96          conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 4 144         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, None, None, 3 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, None, None, 4 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 3 10240       block35_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 3 9216        activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 6 27648       activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 3 96          conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 3 96          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 6 192         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, None, None, 3 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, None, None, 3 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, None, None, 6 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_mixed (Concatenate)   (None, None, None, 1 0           activation_202[0][0]             \n",
      "                                                                 activation_204[0][0]             \n",
      "                                                                 activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_conv (Conv2D)         (None, None, None, 3 41280       block35_8_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_8 (Lambda)              (None, None, None, 3 0           block35_7_ac[0][0]               \n",
      "                                                                 block35_8_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_8_ac (Activation)       (None, None, None, 3 0           block35_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 3 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 3 96          conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, None, None, 3 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 3 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 4 13824       activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 3 96          conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 4 144         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, None, None, 3 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, None, None, 4 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 3 10240       block35_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 3 9216        activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 6 27648       activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 3 96          conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 3 96          conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_66 (BatchNo (None, None, None, 6 192         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, None, None, 3 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, None, None, 3 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, None, None, 6 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_mixed (Concatenate)   (None, None, None, 1 0           activation_208[0][0]             \n",
      "                                                                 activation_210[0][0]             \n",
      "                                                                 activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_conv (Conv2D)         (None, None, None, 3 41280       block35_9_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_9 (Lambda)              (None, None, None, 3 0           block35_8_ac[0][0]               \n",
      "                                                                 block35_9_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_9_ac (Activation)       (None, None, None, 3 0           block35_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 3 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 3 96          conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, None, None, 3 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 3 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 4 13824       activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 3 96          conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 4 144         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, None, None, 3 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, None, None, 4 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 3 10240       block35_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 3 9216        activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 6 27648       activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 3 96          conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 3 96          conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 6 192         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, None, None, 3 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, None, None, 3 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, None, None, 6 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_mixed (Concatenate)  (None, None, None, 1 0           activation_214[0][0]             \n",
      "                                                                 activation_216[0][0]             \n",
      "                                                                 activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_conv (Conv2D)        (None, None, None, 3 41280       block35_10_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block35_10 (Lambda)             (None, None, None, 3 0           block35_9_ac[0][0]               \n",
      "                                                                 block35_10_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_10_ac (Activation)      (None, None, None, 3 0           block35_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 2 81920       block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, None, None, 2 768         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, None, None, 2 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 2 589824      activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, None, 2 768         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, None, None, 2 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 3 1105920     block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, None, None, 3 884736      activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_73 (BatchNo (None, None, None, 3 1152        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, None, 3 1152        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, None, None, 3 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, None, None, 3 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, None, None, 3 0           block35_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mixed_6a (Concatenate)          (None, None, None, 1 0           activation_220[0][0]             \n",
      "                                                                 activation_223[0][0]             \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, None, None, 1 139264      mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, None, 1 384         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_225 (Activation)     (None, None, None, 1 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, None, None, 1 143360      activation_225[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, None, 1 480         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_226 (Activation)     (None, None, None, 1 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, None, None, 1 208896      mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, None, None, 1 215040      activation_226[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, None, 1 576         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, None, 1 576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_224 (Activation)     (None, None, None, 1 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_227 (Activation)     (None, None, None, 1 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_mixed (Concatenate)   (None, None, None, 3 0           activation_224[0][0]             \n",
      "                                                                 activation_227[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_conv (Conv2D)         (None, None, None, 1 418880      block17_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_1 (Lambda)              (None, None, None, 1 0           mixed_6a[0][0]                   \n",
      "                                                                 block17_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_1_ac (Activation)       (None, None, None, 1 0           block17_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, None, None, 1 139264      block17_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, None, None, 1 384         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_229 (Activation)     (None, None, None, 1 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, None, None, 1 143360      activation_229[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, None, None, 1 480         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_230 (Activation)     (None, None, None, 1 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, None, None, 1 208896      block17_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, None, None, 1 215040      activation_230[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, None, None, 1 576         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, None, None, 1 576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_228 (Activation)     (None, None, None, 1 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_231 (Activation)     (None, None, None, 1 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_mixed (Concatenate)   (None, None, None, 3 0           activation_228[0][0]             \n",
      "                                                                 activation_231[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_conv (Conv2D)         (None, None, None, 1 418880      block17_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_2 (Lambda)              (None, None, None, 1 0           block17_1_ac[0][0]               \n",
      "                                                                 block17_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_2_ac (Activation)       (None, None, None, 1 0           block17_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_86 (Conv2D)              (None, None, None, 1 139264      block17_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, None, None, 1 384         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_233 (Activation)     (None, None, None, 1 0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, None, None, 1 143360      activation_233[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, None, None, 1 480         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_234 (Activation)     (None, None, None, 1 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, None, None, 1 208896      block17_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, None, None, 1 215040      activation_234[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, None, None, 1 576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, None, 1 576         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_232 (Activation)     (None, None, None, 1 0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_235 (Activation)     (None, None, None, 1 0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_mixed (Concatenate)   (None, None, None, 3 0           activation_232[0][0]             \n",
      "                                                                 activation_235[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_conv (Conv2D)         (None, None, None, 1 418880      block17_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_3 (Lambda)              (None, None, None, 1 0           block17_2_ac[0][0]               \n",
      "                                                                 block17_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_3_ac (Activation)       (None, None, None, 1 0           block17_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, None, None, 1 139264      block17_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, None, 1 384         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_237 (Activation)     (None, None, None, 1 0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, None, None, 1 143360      activation_237[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, None, 1 480         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_238 (Activation)     (None, None, None, 1 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, None, None, 1 208896      block17_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, None, None, 1 215040      activation_238[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, None, 1 576         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, None, 1 576         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_236 (Activation)     (None, None, None, 1 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_239 (Activation)     (None, None, None, 1 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_mixed (Concatenate)   (None, None, None, 3 0           activation_236[0][0]             \n",
      "                                                                 activation_239[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_conv (Conv2D)         (None, None, None, 1 418880      block17_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_4 (Lambda)              (None, None, None, 1 0           block17_3_ac[0][0]               \n",
      "                                                                 block17_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_4_ac (Activation)       (None, None, None, 1 0           block17_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, None, None, 1 139264      block17_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, None, 1 384         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_241 (Activation)     (None, None, None, 1 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, None, None, 1 143360      activation_241[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, None, None, 1 480         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_242 (Activation)     (None, None, None, 1 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, None, None, 1 208896      block17_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_96 (Conv2D)              (None, None, None, 1 215040      activation_242[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, None, 1 576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, None, None, 1 576         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_240 (Activation)     (None, None, None, 1 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, None, None, 1 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_mixed (Concatenate)   (None, None, None, 3 0           activation_240[0][0]             \n",
      "                                                                 activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_conv (Conv2D)         (None, None, None, 1 418880      block17_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_5 (Lambda)              (None, None, None, 1 0           block17_4_ac[0][0]               \n",
      "                                                                 block17_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_5_ac (Activation)       (None, None, None, 1 0           block17_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, None, None, 1 139264      block17_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, None, None, 1 384         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, None, None, 1 0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, None, None, 1 143360      activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, None, None, 1 480         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, None, None, 1 0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, None, None, 1 208896      block17_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, None, None, 1 215040      activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, None, None, 1 576         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, None, None, 1 576         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, None, None, 1 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, None, None, 1 0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_mixed (Concatenate)   (None, None, None, 3 0           activation_244[0][0]             \n",
      "                                                                 activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_conv (Conv2D)         (None, None, None, 1 418880      block17_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_6 (Lambda)              (None, None, None, 1 0           block17_5_ac[0][0]               \n",
      "                                                                 block17_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_6_ac (Activation)       (None, None, None, 1 0           block17_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, None, None, 1 139264      block17_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, None, None, 1 384         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, None, None, 1 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, None, None, 1 143360      activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, None, None, 1 480         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, None, None, 1 0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, None, None, 1 208896      block17_6_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, None, None, 1 215040      activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, None, None, 1 576         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, None, None, 1 576         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, None, None, 1 0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, None, None, 1 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_mixed (Concatenate)   (None, None, None, 3 0           activation_248[0][0]             \n",
      "                                                                 activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_conv (Conv2D)         (None, None, None, 1 418880      block17_7_mixed[0][0]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "block17_7 (Lambda)              (None, None, None, 1 0           block17_6_ac[0][0]               \n",
      "                                                                 block17_7_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_7_ac (Activation)       (None, None, None, 1 0           block17_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, None, None, 1 139264      block17_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, None, None, 1 384         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, None, None, 1 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, None, None, 1 143360      activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, None, None, 1 480         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, None, None, 1 0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, None, None, 1 208896      block17_7_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, None, None, 1 215040      activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, None, None, 1 576         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, None, None, 1 576         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, None, None, 1 0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, None, None, 1 0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_mixed (Concatenate)   (None, None, None, 3 0           activation_252[0][0]             \n",
      "                                                                 activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_conv (Conv2D)         (None, None, None, 1 418880      block17_8_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_8 (Lambda)              (None, None, None, 1 0           block17_7_ac[0][0]               \n",
      "                                                                 block17_8_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_8_ac (Activation)       (None, None, None, 1 0           block17_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, None, None, 1 139264      block17_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, None, None, 1 384         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, None, None, 1 0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, None, None, 1 143360      activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, None, None, 1 480         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, None, None, 1 0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, None, None, 1 208896      block17_8_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, None, None, 1 215040      activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, None, None, 1 576         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, None, None, 1 576         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, None, None, 1 0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, None, None, 1 0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_mixed (Concatenate)   (None, None, None, 3 0           activation_256[0][0]             \n",
      "                                                                 activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_conv (Conv2D)         (None, None, None, 1 418880      block17_9_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_9 (Lambda)              (None, None, None, 1 0           block17_8_ac[0][0]               \n",
      "                                                                 block17_9_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_9_ac (Activation)       (None, None, None, 1 0           block17_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, None, None, 1 139264      block17_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, None, None, 1 384         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, None, None, 1 0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, None, None, 1 143360      activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_115 (BatchN (None, None, None, 1 480         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, None, None, 1 0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, None, None, 1 208896      block17_9_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, None, None, 1 215040      activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, None, None, 1 576         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, None, None, 1 576         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, None, None, 1 0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, None, None, 1 0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_mixed (Concatenate)  (None, None, None, 3 0           activation_260[0][0]             \n",
      "                                                                 activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_conv (Conv2D)        (None, None, None, 1 418880      block17_10_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_10 (Lambda)             (None, None, None, 1 0           block17_9_ac[0][0]               \n",
      "                                                                 block17_10_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_10_ac (Activation)      (None, None, None, 1 0           block17_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, None, None, 1 139264      block17_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, None, None, 1 384         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, None, None, 1 0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, None, None, 1 143360      activation_265[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, None, None, 1 480         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, None, None, 1 0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, None, None, 1 208896      block17_10_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, None, None, 1 215040      activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, None, None, 1 576         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, None, None, 1 576         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, None, None, 1 0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, None, None, 1 0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_mixed (Concatenate)  (None, None, None, 3 0           activation_264[0][0]             \n",
      "                                                                 activation_267[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_conv (Conv2D)        (None, None, None, 1 418880      block17_11_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_11 (Lambda)             (None, None, None, 1 0           block17_10_ac[0][0]              \n",
      "                                                                 block17_11_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_11_ac (Activation)      (None, None, None, 1 0           block17_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, None, None, 1 139264      block17_11_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, None, None, 1 384         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, None, None, 1 0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, None, None, 1 143360      activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, None, None, 1 480         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, None, None, 1 0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, None, None, 1 208896      block17_11_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, None, None, 1 215040      activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, None, None, 1 576         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, None, None, 1 576         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, None, None, 1 0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_271 (Activation)     (None, None, None, 1 0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_mixed (Concatenate)  (None, None, None, 3 0           activation_268[0][0]             \n",
      "                                                                 activation_271[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_conv (Conv2D)        (None, None, None, 1 418880      block17_12_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_12 (Lambda)             (None, None, None, 1 0           block17_11_ac[0][0]              \n",
      "                                                                 block17_12_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_12_ac (Activation)      (None, None, None, 1 0           block17_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, None, None, 1 139264      block17_12_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, None, None, 1 384         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, None, None, 1 0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, None, None, 1 143360      activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, None, None, 1 480         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, None, None, 1 0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, None, None, 1 208896      block17_12_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, None, None, 1 215040      activation_274[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, None, None, 1 576         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, None, None, 1 576         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, None, None, 1 0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, None, None, 1 0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_mixed (Concatenate)  (None, None, None, 3 0           activation_272[0][0]             \n",
      "                                                                 activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_conv (Conv2D)        (None, None, None, 1 418880      block17_13_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_13 (Lambda)             (None, None, None, 1 0           block17_12_ac[0][0]              \n",
      "                                                                 block17_13_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_13_ac (Activation)      (None, None, None, 1 0           block17_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, None, None, 1 139264      block17_13_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, None, None, 1 384         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, None, None, 1 0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, None, None, 1 143360      activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, None, None, 1 480         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, None, None, 1 0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, None, None, 1 208896      block17_13_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, None, None, 1 215040      activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, None, None, 1 576         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, None, None, 1 576         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, None, None, 1 0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, None, None, 1 0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_mixed (Concatenate)  (None, None, None, 3 0           activation_276[0][0]             \n",
      "                                                                 activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_conv (Conv2D)        (None, None, None, 1 418880      block17_14_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_14 (Lambda)             (None, None, None, 1 0           block17_13_ac[0][0]              \n",
      "                                                                 block17_14_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_14_ac (Activation)      (None, None, None, 1 0           block17_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, None, None, 1 139264      block17_14_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_134 (BatchN (None, None, None, 1 384         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, None, None, 1 0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, None, None, 1 143360      activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, None, None, 1 480         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, None, None, 1 0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, None, None, 1 208896      block17_14_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, None, None, 1 215040      activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, None, None, 1 576         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, None, None, 1 576         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, None, None, 1 0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, None, None, 1 0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_mixed (Concatenate)  (None, None, None, 3 0           activation_280[0][0]             \n",
      "                                                                 activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_conv (Conv2D)        (None, None, None, 1 418880      block17_15_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_15 (Lambda)             (None, None, None, 1 0           block17_14_ac[0][0]              \n",
      "                                                                 block17_15_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_15_ac (Activation)      (None, None, None, 1 0           block17_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, None, None, 1 139264      block17_15_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, None, None, 1 384         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, None, None, 1 0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, None, None, 1 143360      activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, None, None, 1 480         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, None, None, 1 0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, None, None, 1 208896      block17_15_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, None, None, 1 215040      activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, None, None, 1 576         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, None, None, 1 576         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, None, None, 1 0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, None, None, 1 0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_mixed (Concatenate)  (None, None, None, 3 0           activation_284[0][0]             \n",
      "                                                                 activation_287[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_conv (Conv2D)        (None, None, None, 1 418880      block17_16_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_16 (Lambda)             (None, None, None, 1 0           block17_15_ac[0][0]              \n",
      "                                                                 block17_16_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_16_ac (Activation)      (None, None, None, 1 0           block17_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, None, None, 1 139264      block17_16_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, None, None, 1 384         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, None, None, 1 0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, None, None, 1 143360      activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, None, None, 1 480         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, None, None, 1 0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, None, None, 1 208896      block17_16_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, None, None, 1 215040      activation_290[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_141 (BatchN (None, None, None, 1 576         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, None, None, 1 576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, None, None, 1 0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, None, None, 1 0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_mixed (Concatenate)  (None, None, None, 3 0           activation_288[0][0]             \n",
      "                                                                 activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_conv (Conv2D)        (None, None, None, 1 418880      block17_17_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_17 (Lambda)             (None, None, None, 1 0           block17_16_ac[0][0]              \n",
      "                                                                 block17_17_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_17_ac (Activation)      (None, None, None, 1 0           block17_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, None, None, 1 139264      block17_17_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, None, None, 1 384         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, None, None, 1 0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, None, None, 1 143360      activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, None, None, 1 480         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, None, None, 1 0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, None, None, 1 208896      block17_17_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, None, None, 1 215040      activation_294[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, None, None, 1 576         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, None, None, 1 576         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, None, None, 1 0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, None, None, 1 0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_mixed (Concatenate)  (None, None, None, 3 0           activation_292[0][0]             \n",
      "                                                                 activation_295[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_conv (Conv2D)        (None, None, None, 1 418880      block17_18_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_18 (Lambda)             (None, None, None, 1 0           block17_17_ac[0][0]              \n",
      "                                                                 block17_18_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_18_ac (Activation)      (None, None, None, 1 0           block17_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, None, None, 1 139264      block17_18_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, None, None, 1 384         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, None, None, 1 0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, None, None, 1 143360      activation_297[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, None, None, 1 480         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, None, None, 1 0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, None, None, 1 208896      block17_18_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, None, None, 1 215040      activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, None, None, 1 576         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, None, None, 1 576         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, None, None, 1 0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, None, None, 1 0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_mixed (Concatenate)  (None, None, None, 3 0           activation_296[0][0]             \n",
      "                                                                 activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_conv (Conv2D)        (None, None, None, 1 418880      block17_19_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_19 (Lambda)             (None, None, None, 1 0           block17_18_ac[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 block17_19_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_19_ac (Activation)      (None, None, None, 1 0           block17_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, None, None, 1 139264      block17_19_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, None, None, 1 384         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, None, None, 1 0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, None, None, 1 143360      activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, None, None, 1 480         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, None, None, 1 0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, None, None, 1 208896      block17_19_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, None, None, 1 215040      activation_302[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, None, None, 1 576         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, None, None, 1 576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, None, None, 1 0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, None, None, 1 0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_mixed (Concatenate)  (None, None, None, 3 0           activation_300[0][0]             \n",
      "                                                                 activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_conv (Conv2D)        (None, None, None, 1 418880      block17_20_mixed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block17_20 (Lambda)             (None, None, None, 1 0           block17_19_ac[0][0]              \n",
      "                                                                 block17_20_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block17_20_ac (Activation)      (None, None, None, 1 0           block17_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, None, None, 2 278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, None, None, 2 768         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, None, None, 2 0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, None, None, 2 278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, None, None, 2 278528      block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, None, None, 2 663552      activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, None, None, 2 768         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, None, None, 2 768         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, None, None, 2 864         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, None, None, 2 0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, None, None, 2 0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, None, None, 2 0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, None, None, 3 884736      activation_304[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, None, None, 2 663552      activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, None, None, 3 829440      activation_309[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, None, None, 3 1152        conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, None, None, 2 864         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, None, None, 3 960         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, None, None, 3 0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, None, None, 2 0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, None, None, 3 0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, None, None, 1 0           block17_20_ac[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed_7a (Concatenate)          (None, None, None, 2 0           activation_305[0][0]             \n",
      "                                                                 activation_307[0][0]             \n",
      "                                                                 activation_310[0][0]             \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, None, None, 1 399360      mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, None, None, 1 576         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, None, None, 1 0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, None, None, 2 129024      activation_312[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, None, None, 2 672         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, None, None, 2 0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, None, None, 1 399360      mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, None, None, 2 172032      activation_313[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, None, None, 1 576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, None, None, 2 768         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, None, None, 1 0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, None, None, 2 0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_mixed (Concatenate)    (None, None, None, 4 0           activation_311[0][0]             \n",
      "                                                                 activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_conv (Conv2D)          (None, None, None, 2 933920      block8_1_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_1 (Lambda)               (None, None, None, 2 0           mixed_7a[0][0]                   \n",
      "                                                                 block8_1_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_1_ac (Activation)        (None, None, None, 2 0           block8_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, None, None, 1 399360      block8_1_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, None, None, 1 576         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, None, None, 1 0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, None, None, 2 129024      activation_316[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, None, None, 2 672         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, None, None, 2 0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, None, None, 1 399360      block8_1_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, None, None, 2 172032      activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, None, None, 1 576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, None, None, 2 768         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, None, None, 1 0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, None, None, 2 0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_mixed (Concatenate)    (None, None, None, 4 0           activation_315[0][0]             \n",
      "                                                                 activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_conv (Conv2D)          (None, None, None, 2 933920      block8_2_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_2 (Lambda)               (None, None, None, 2 0           block8_1_ac[0][0]                \n",
      "                                                                 block8_2_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_2_ac (Activation)        (None, None, None, 2 0           block8_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, None, None, 1 399360      block8_2_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, None, None, 1 576         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, None, None, 1 0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, None, None, 2 129024      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, None, None, 2 672         conv2d_174[0][0]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, None, None, 2 0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, None, None, 1 399360      block8_2_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, None, None, 2 172032      activation_321[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, None, None, 1 576         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, None, None, 2 768         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, None, None, 1 0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, None, None, 2 0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_mixed (Concatenate)    (None, None, None, 4 0           activation_319[0][0]             \n",
      "                                                                 activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_conv (Conv2D)          (None, None, None, 2 933920      block8_3_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_3 (Lambda)               (None, None, None, 2 0           block8_2_ac[0][0]                \n",
      "                                                                 block8_3_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_3_ac (Activation)        (None, None, None, 2 0           block8_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, None, None, 1 399360      block8_3_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, None, None, 1 576         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, None, None, 1 0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, None, None, 2 129024      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, None, None, 2 672         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, None, None, 2 0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, None, None, 1 399360      block8_3_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, None, None, 2 172032      activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, None, None, 1 576         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, None, None, 2 768         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, None, None, 1 0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, None, None, 2 0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_mixed (Concatenate)    (None, None, None, 4 0           activation_323[0][0]             \n",
      "                                                                 activation_326[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_conv (Conv2D)          (None, None, None, 2 933920      block8_4_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_4 (Lambda)               (None, None, None, 2 0           block8_3_ac[0][0]                \n",
      "                                                                 block8_4_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_4_ac (Activation)        (None, None, None, 2 0           block8_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, None, None, 1 399360      block8_4_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, None, None, 1 576         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, None, None, 1 0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, None, None, 2 129024      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, None, None, 2 672         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, None, None, 2 0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, None, None, 1 399360      block8_4_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, None, None, 2 172032      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, None, None, 1 576         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, None, None, 2 768         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, None, None, 1 0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, None, None, 2 0           batch_normalization_183[0][0]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "block8_5_mixed (Concatenate)    (None, None, None, 4 0           activation_327[0][0]             \n",
      "                                                                 activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_conv (Conv2D)          (None, None, None, 2 933920      block8_5_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_5 (Lambda)               (None, None, None, 2 0           block8_4_ac[0][0]                \n",
      "                                                                 block8_5_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_5_ac (Activation)        (None, None, None, 2 0           block8_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, None, None, 1 399360      block8_5_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, None, None, 1 576         conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, None, None, 1 0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, None, None, 2 129024      activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, None, None, 2 672         conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, None, None, 2 0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, None, None, 1 399360      block8_5_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, None, None, 2 172032      activation_333[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, None, None, 1 576         conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, None, None, 2 768         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, None, None, 1 0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, None, None, 2 0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_mixed (Concatenate)    (None, None, None, 4 0           activation_331[0][0]             \n",
      "                                                                 activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_conv (Conv2D)          (None, None, None, 2 933920      block8_6_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_6 (Lambda)               (None, None, None, 2 0           block8_5_ac[0][0]                \n",
      "                                                                 block8_6_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_6_ac (Activation)        (None, None, None, 2 0           block8_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, None, None, 1 399360      block8_6_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, None, None, 1 576         conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, None, None, 1 0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, None, None, 2 129024      activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, None, None, 2 672         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, None, None, 2 0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, None, None, 1 399360      block8_6_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, None, None, 2 172032      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, None, None, 1 576         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, None, None, 2 768         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, None, None, 1 0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, None, None, 2 0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_mixed (Concatenate)    (None, None, None, 4 0           activation_335[0][0]             \n",
      "                                                                 activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_conv (Conv2D)          (None, None, None, 2 933920      block8_7_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_7 (Lambda)               (None, None, None, 2 0           block8_6_ac[0][0]                \n",
      "                                                                 block8_7_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_7_ac (Activation)        (None, None, None, 2 0           block8_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, None, None, 1 399360      block8_7_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, None, None, 1 576         conv2d_193[0][0]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, None, None, 1 0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, None, None, 2 129024      activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, None, None, 2 672         conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, None, None, 2 0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, None, None, 1 399360      block8_7_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, None, None, 2 172032      activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, None, None, 1 576         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, None, None, 2 768         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, None, None, 1 0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, None, None, 2 0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_mixed (Concatenate)    (None, None, None, 4 0           activation_339[0][0]             \n",
      "                                                                 activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_conv (Conv2D)          (None, None, None, 2 933920      block8_8_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_8 (Lambda)               (None, None, None, 2 0           block8_7_ac[0][0]                \n",
      "                                                                 block8_8_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_8_ac (Activation)        (None, None, None, 2 0           block8_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, None, None, 1 399360      block8_8_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, None, None, 1 576         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, None, None, 1 0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, None, None, 2 129024      activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, None, None, 2 672         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, None, None, 2 0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, None, None, 1 399360      block8_8_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, None, None, 2 172032      activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, None, None, 1 576         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, None, None, 2 768         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, None, None, 1 0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, None, None, 2 0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_mixed (Concatenate)    (None, None, None, 4 0           activation_343[0][0]             \n",
      "                                                                 activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_conv (Conv2D)          (None, None, None, 2 933920      block8_9_mixed[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_9 (Lambda)               (None, None, None, 2 0           block8_8_ac[0][0]                \n",
      "                                                                 block8_9_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block8_9_ac (Activation)        (None, None, None, 2 0           block8_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, None, None, 1 399360      block8_9_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, None, None, 1 576         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, None, None, 1 0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, None, None, 2 129024      activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, None, None, 2 672         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, None, None, 2 0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, None, None, 1 399360      block8_9_ac[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, None, None, 2 172032      activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, None, None, 1 576         conv2d_200[0][0]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, None, None, 2 768         conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, None, None, 1 0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, None, None, 2 0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block8_10_mixed (Concatenate)   (None, None, None, 4 0           activation_347[0][0]             \n",
      "                                                                 activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block8_10_conv (Conv2D)         (None, None, None, 2 933920      block8_10_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_10 (Lambda)              (None, None, None, 2 0           block8_9_ac[0][0]                \n",
      "                                                                 block8_10_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b (Conv2D)                (None, None, None, 1 3194880     block8_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_bn (BatchNormalization) (None, None, None, 1 4608        conv_7b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_ac (Activation)         (None, None, None, 1 0           conv_7b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 1536)         0           conv_7b_ac[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         1573888     global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            3075        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 55,913,699\n",
      "Trainable params: 1,576,963\n",
      "Non-trainable params: 54,336,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'C:\\\\Users\\\\rafae\\\\Desktop\\\\Coleta\\\\data\\\\rgb-subset\\\\train'\n",
    "validation_data_dir = 'C:\\\\Users\\\\rafae\\\\Desktop\\\\Coleta\\\\data\\\\rgb-subset\\\\validation'\n",
    "nb_train_samples = 2500\n",
    "nb_validation_samples = 560\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "samples_per_epoch= 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    #shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    #horizontal_flip=True\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 3 classes.\n",
      "Found 560 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/78 [============================>.] - ETA: 20:09 - loss: 0.6781 - acc: 0.65 - ETA: 12:59 - loss: 0.6706 - acc: 0.63 - ETA: 10:33 - loss: 0.6410 - acc: 0.65 - ETA: 9:17 - loss: 0.6574 - acc: 0.6406 - ETA: 8:30 - loss: 0.6701 - acc: 0.643 - ETA: 7:57 - loss: 0.6810 - acc: 0.633 - ETA: 7:32 - loss: 0.6791 - acc: 0.635 - ETA: 7:12 - loss: 0.6741 - acc: 0.638 - ETA: 6:55 - loss: 0.6750 - acc: 0.636 - ETA: 6:41 - loss: 0.6797 - acc: 0.635 - ETA: 6:28 - loss: 0.6835 - acc: 0.635 - ETA: 6:17 - loss: 0.6829 - acc: 0.635 - ETA: 6:07 - loss: 0.6759 - acc: 0.641 - ETA: 5:57 - loss: 0.6755 - acc: 0.641 - ETA: 5:49 - loss: 0.6805 - acc: 0.636 - ETA: 5:40 - loss: 0.6812 - acc: 0.635 - ETA: 5:32 - loss: 0.6813 - acc: 0.637 - ETA: 5:25 - loss: 0.6790 - acc: 0.639 - ETA: 5:18 - loss: 0.6779 - acc: 0.640 - ETA: 5:11 - loss: 0.6743 - acc: 0.641 - ETA: 5:04 - loss: 0.6735 - acc: 0.642 - ETA: 4:57 - loss: 0.6738 - acc: 0.641 - ETA: 4:51 - loss: 0.6736 - acc: 0.642 - ETA: 4:44 - loss: 0.6737 - acc: 0.641 - ETA: 4:38 - loss: 0.6737 - acc: 0.640 - ETA: 4:32 - loss: 0.6735 - acc: 0.639 - ETA: 4:26 - loss: 0.6743 - acc: 0.637 - ETA: 4:20 - loss: 0.6733 - acc: 0.637 - ETA: 4:14 - loss: 0.6724 - acc: 0.637 - ETA: 4:08 - loss: 0.6753 - acc: 0.636 - ETA: 4:02 - loss: 0.6751 - acc: 0.636 - ETA: 3:57 - loss: 0.6731 - acc: 0.636 - ETA: 3:51 - loss: 0.6729 - acc: 0.637 - ETA: 3:46 - loss: 0.6721 - acc: 0.637 - ETA: 3:40 - loss: 0.6729 - acc: 0.637 - ETA: 3:35 - loss: 0.6729 - acc: 0.638 - ETA: 3:29 - loss: 0.6728 - acc: 0.638 - ETA: 3:24 - loss: 0.6718 - acc: 0.638 - ETA: 3:18 - loss: 0.6728 - acc: 0.638 - ETA: 3:13 - loss: 0.6733 - acc: 0.638 - ETA: 3:08 - loss: 0.6731 - acc: 0.638 - ETA: 3:02 - loss: 0.6730 - acc: 0.637 - ETA: 2:57 - loss: 0.6721 - acc: 0.638 - ETA: 2:52 - loss: 0.6739 - acc: 0.637 - ETA: 2:47 - loss: 0.6739 - acc: 0.637 - ETA: 2:41 - loss: 0.6746 - acc: 0.636 - ETA: 2:36 - loss: 0.6742 - acc: 0.636 - ETA: 2:31 - loss: 0.6730 - acc: 0.637 - ETA: 2:26 - loss: 0.6731 - acc: 0.637 - ETA: 2:21 - loss: 0.6732 - acc: 0.637 - ETA: 2:15 - loss: 0.6727 - acc: 0.637 - ETA: 2:10 - loss: 0.6720 - acc: 0.638 - ETA: 2:05 - loss: 0.6716 - acc: 0.638 - ETA: 2:00 - loss: 0.6714 - acc: 0.639 - ETA: 1:55 - loss: 0.6709 - acc: 0.639 - ETA: 1:50 - loss: 0.6709 - acc: 0.639 - ETA: 1:45 - loss: 0.6699 - acc: 0.641 - ETA: 1:39 - loss: 0.6708 - acc: 0.640 - ETA: 1:34 - loss: 0.6710 - acc: 0.640 - ETA: 1:29 - loss: 0.6701 - acc: 0.641 - ETA: 1:24 - loss: 0.6699 - acc: 0.641 - ETA: 1:19 - loss: 0.6693 - acc: 0.642 - ETA: 1:14 - loss: 0.6688 - acc: 0.642 - ETA: 1:09 - loss: 0.6686 - acc: 0.642 - ETA: 1:04 - loss: 0.6688 - acc: 0.641 - ETA: 59s - loss: 0.6685 - acc: 0.642 - ETA: 54s - loss: 0.6683 - acc: 0.64 - ETA: 49s - loss: 0.6683 - acc: 0.64 - ETA: 44s - loss: 0.6669 - acc: 0.64 - ETA: 39s - loss: 0.6667 - acc: 0.64 - ETA: 34s - loss: 0.6665 - acc: 0.64 - ETA: 29s - loss: 0.6669 - acc: 0.64 - ETA: 24s - loss: 0.6661 - acc: 0.64 - ETA: 19s - loss: 0.6654 - acc: 0.64 - ETA: 14s - loss: 0.6647 - acc: 0.64 - ETA: 9s - loss: 0.6635 - acc: 0.6445 - ETA: 4s - loss: 0.6641 - acc: 0.6441\n",
      "Epoch 00001: val_loss improved from inf to 0.64487, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 593s 8s/step - loss: 0.6640 - acc: 0.6440 - val_loss: 0.6449 - val_acc: 0.6532\n",
      "Epoch 2/100\n",
      "77/78 [============================>.] - ETA: 6:12 - loss: 0.6910 - acc: 0.625 - ETA: 6:07 - loss: 0.6711 - acc: 0.635 - ETA: 6:02 - loss: 0.6692 - acc: 0.645 - ETA: 5:57 - loss: 0.6678 - acc: 0.648 - ETA: 5:52 - loss: 0.6663 - acc: 0.645 - ETA: 5:47 - loss: 0.6625 - acc: 0.651 - ETA: 5:42 - loss: 0.6576 - acc: 0.651 - ETA: 5:37 - loss: 0.6602 - acc: 0.649 - ETA: 5:33 - loss: 0.6554 - acc: 0.653 - ETA: 5:28 - loss: 0.6504 - acc: 0.657 - ETA: 5:23 - loss: 0.6514 - acc: 0.656 - ETA: 5:18 - loss: 0.6506 - acc: 0.656 - ETA: 5:13 - loss: 0.6506 - acc: 0.657 - ETA: 5:08 - loss: 0.6473 - acc: 0.658 - ETA: 5:03 - loss: 0.6471 - acc: 0.659 - ETA: 4:59 - loss: 0.6454 - acc: 0.657 - ETA: 4:54 - loss: 0.6450 - acc: 0.655 - ETA: 4:49 - loss: 0.6428 - acc: 0.658 - ETA: 4:44 - loss: 0.6400 - acc: 0.660 - ETA: 4:39 - loss: 0.6403 - acc: 0.660 - ETA: 4:34 - loss: 0.6396 - acc: 0.660 - ETA: 4:30 - loss: 0.6396 - acc: 0.658 - ETA: 4:25 - loss: 0.6383 - acc: 0.659 - ETA: 4:20 - loss: 0.6394 - acc: 0.658 - ETA: 4:15 - loss: 0.6400 - acc: 0.657 - ETA: 4:10 - loss: 0.6382 - acc: 0.658 - ETA: 4:06 - loss: 0.6398 - acc: 0.657 - ETA: 4:01 - loss: 0.6386 - acc: 0.659 - ETA: 3:56 - loss: 0.6406 - acc: 0.659 - ETA: 3:51 - loss: 0.6436 - acc: 0.657 - ETA: 3:46 - loss: 0.6427 - acc: 0.657 - ETA: 3:41 - loss: 0.6440 - acc: 0.655 - ETA: 3:37 - loss: 0.6458 - acc: 0.654 - ETA: 3:32 - loss: 0.6464 - acc: 0.654 - ETA: 3:27 - loss: 0.6460 - acc: 0.654 - ETA: 3:22 - loss: 0.6460 - acc: 0.654 - ETA: 3:17 - loss: 0.6469 - acc: 0.654 - ETA: 3:12 - loss: 0.6466 - acc: 0.654 - ETA: 3:03 - loss: 0.6497 - acc: 0.650 - ETA: 2:59 - loss: 0.6497 - acc: 0.651 - ETA: 2:54 - loss: 0.6504 - acc: 0.650 - ETA: 2:50 - loss: 0.6507 - acc: 0.650 - ETA: 2:45 - loss: 0.6495 - acc: 0.650 - ETA: 2:40 - loss: 0.6503 - acc: 0.650 - ETA: 2:36 - loss: 0.6508 - acc: 0.650 - ETA: 2:31 - loss: 0.6517 - acc: 0.650 - ETA: 2:26 - loss: 0.6510 - acc: 0.650 - ETA: 2:22 - loss: 0.6505 - acc: 0.650 - ETA: 2:17 - loss: 0.6506 - acc: 0.650 - ETA: 2:12 - loss: 0.6507 - acc: 0.651 - ETA: 2:08 - loss: 0.6501 - acc: 0.651 - ETA: 2:03 - loss: 0.6505 - acc: 0.651 - ETA: 1:58 - loss: 0.6503 - acc: 0.651 - ETA: 1:53 - loss: 0.6504 - acc: 0.650 - ETA: 1:49 - loss: 0.6493 - acc: 0.651 - ETA: 1:44 - loss: 0.6488 - acc: 0.651 - ETA: 1:39 - loss: 0.6480 - acc: 0.651 - ETA: 1:35 - loss: 0.6467 - acc: 0.651 - ETA: 1:30 - loss: 0.6466 - acc: 0.652 - ETA: 1:25 - loss: 0.6464 - acc: 0.652 - ETA: 1:20 - loss: 0.6469 - acc: 0.652 - ETA: 1:16 - loss: 0.6474 - acc: 0.652 - ETA: 1:11 - loss: 0.6474 - acc: 0.652 - ETA: 1:06 - loss: 0.6469 - acc: 0.652 - ETA: 1:01 - loss: 0.6472 - acc: 0.652 - ETA: 57s - loss: 0.6479 - acc: 0.651 - ETA: 52s - loss: 0.6475 - acc: 0.65 - ETA: 47s - loss: 0.6471 - acc: 0.65 - ETA: 42s - loss: 0.6469 - acc: 0.65 - ETA: 38s - loss: 0.6464 - acc: 0.65 - ETA: 33s - loss: 0.6464 - acc: 0.65 - ETA: 28s - loss: 0.6460 - acc: 0.65 - ETA: 23s - loss: 0.6460 - acc: 0.65 - ETA: 19s - loss: 0.6457 - acc: 0.65 - ETA: 14s - loss: 0.6456 - acc: 0.65 - ETA: 9s - loss: 0.6456 - acc: 0.6549 - ETA: 4s - loss: 0.6460 - acc: 0.6540\n",
      "Epoch 00002: val_loss improved from 0.64487 to 0.64115, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 467s 6s/step - loss: 0.6459 - acc: 0.6540 - val_loss: 0.6412 - val_acc: 0.6566\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.6503 - acc: 0.635 - ETA: 6:06 - loss: 0.6284 - acc: 0.661 - ETA: 6:01 - loss: 0.6394 - acc: 0.652 - ETA: 5:57 - loss: 0.6469 - acc: 0.653 - ETA: 5:52 - loss: 0.6399 - acc: 0.658 - ETA: 5:47 - loss: 0.6438 - acc: 0.663 - ETA: 5:42 - loss: 0.6517 - acc: 0.657 - ETA: 5:37 - loss: 0.6495 - acc: 0.658 - ETA: 5:32 - loss: 0.6461 - acc: 0.660 - ETA: 5:28 - loss: 0.6448 - acc: 0.660 - ETA: 5:23 - loss: 0.6426 - acc: 0.662 - ETA: 5:18 - loss: 0.6430 - acc: 0.661 - ETA: 5:13 - loss: 0.6428 - acc: 0.659 - ETA: 5:08 - loss: 0.6404 - acc: 0.660 - ETA: 5:04 - loss: 0.6408 - acc: 0.659 - ETA: 4:59 - loss: 0.6422 - acc: 0.660 - ETA: 4:54 - loss: 0.6405 - acc: 0.661 - ETA: 4:49 - loss: 0.6423 - acc: 0.659 - ETA: 4:44 - loss: 0.6423 - acc: 0.660 - ETA: 4:39 - loss: 0.6406 - acc: 0.658 - ETA: 4:23 - loss: 0.6461 - acc: 0.659 - ETA: 4:19 - loss: 0.6479 - acc: 0.659 - ETA: 4:15 - loss: 0.6485 - acc: 0.658 - ETA: 4:11 - loss: 0.6481 - acc: 0.657 - ETA: 4:06 - loss: 0.6476 - acc: 0.657 - ETA: 4:02 - loss: 0.6459 - acc: 0.657 - ETA: 3:58 - loss: 0.6467 - acc: 0.659 - ETA: 3:53 - loss: 0.6462 - acc: 0.659 - ETA: 3:49 - loss: 0.6478 - acc: 0.658 - ETA: 3:44 - loss: 0.6475 - acc: 0.658 - ETA: 3:40 - loss: 0.6476 - acc: 0.658 - ETA: 3:35 - loss: 0.6481 - acc: 0.658 - ETA: 3:31 - loss: 0.6471 - acc: 0.658 - ETA: 3:26 - loss: 0.6457 - acc: 0.659 - ETA: 3:22 - loss: 0.6455 - acc: 0.659 - ETA: 3:17 - loss: 0.6460 - acc: 0.659 - ETA: 3:13 - loss: 0.6458 - acc: 0.659 - ETA: 3:08 - loss: 0.6454 - acc: 0.659 - ETA: 3:03 - loss: 0.6460 - acc: 0.660 - ETA: 2:59 - loss: 0.6472 - acc: 0.659 - ETA: 2:54 - loss: 0.6464 - acc: 0.659 - ETA: 2:50 - loss: 0.6469 - acc: 0.660 - ETA: 2:45 - loss: 0.6466 - acc: 0.660 - ETA: 2:40 - loss: 0.6472 - acc: 0.659 - ETA: 2:36 - loss: 0.6464 - acc: 0.661 - ETA: 2:31 - loss: 0.6464 - acc: 0.660 - ETA: 2:26 - loss: 0.6474 - acc: 0.660 - ETA: 2:22 - loss: 0.6460 - acc: 0.661 - ETA: 2:17 - loss: 0.6456 - acc: 0.661 - ETA: 2:12 - loss: 0.6443 - acc: 0.661 - ETA: 2:08 - loss: 0.6442 - acc: 0.661 - ETA: 2:03 - loss: 0.6441 - acc: 0.661 - ETA: 1:58 - loss: 0.6440 - acc: 0.661 - ETA: 1:53 - loss: 0.6426 - acc: 0.663 - ETA: 1:49 - loss: 0.6422 - acc: 0.663 - ETA: 1:44 - loss: 0.6419 - acc: 0.663 - ETA: 1:39 - loss: 0.6428 - acc: 0.663 - ETA: 1:35 - loss: 0.6421 - acc: 0.663 - ETA: 1:30 - loss: 0.6422 - acc: 0.663 - ETA: 1:25 - loss: 0.6419 - acc: 0.663 - ETA: 1:20 - loss: 0.6413 - acc: 0.663 - ETA: 1:16 - loss: 0.6406 - acc: 0.664 - ETA: 1:11 - loss: 0.6405 - acc: 0.663 - ETA: 1:06 - loss: 0.6404 - acc: 0.663 - ETA: 1:01 - loss: 0.6403 - acc: 0.663 - ETA: 57s - loss: 0.6398 - acc: 0.664 - ETA: 52s - loss: 0.6397 - acc: 0.66 - ETA: 47s - loss: 0.6400 - acc: 0.66 - ETA: 42s - loss: 0.6393 - acc: 0.66 - ETA: 38s - loss: 0.6398 - acc: 0.66 - ETA: 33s - loss: 0.6398 - acc: 0.66 - ETA: 28s - loss: 0.6396 - acc: 0.66 - ETA: 23s - loss: 0.6393 - acc: 0.66 - ETA: 19s - loss: 0.6395 - acc: 0.66 - ETA: 14s - loss: 0.6394 - acc: 0.66 - ETA: 9s - loss: 0.6392 - acc: 0.6620 - ETA: 4s - loss: 0.6390 - acc: 0.6617\n",
      "Epoch 00003: val_loss improved from 0.64115 to 0.63686, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.6394 - acc: 0.6615 - val_loss: 0.6369 - val_acc: 0.6534\n",
      "Epoch 4/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.6063 - acc: 0.666 - ETA: 6:05 - loss: 0.6244 - acc: 0.677 - ETA: 6:01 - loss: 0.6214 - acc: 0.673 - ETA: 5:56 - loss: 0.6149 - acc: 0.671 - ETA: 5:51 - loss: 0.6274 - acc: 0.658 - ETA: 5:47 - loss: 0.6251 - acc: 0.658 - ETA: 5:42 - loss: 0.6216 - acc: 0.659 - ETA: 5:37 - loss: 0.6227 - acc: 0.661 - ETA: 5:32 - loss: 0.6224 - acc: 0.662 - ETA: 5:27 - loss: 0.6211 - acc: 0.666 - ETA: 5:23 - loss: 0.6190 - acc: 0.668 - ETA: 5:18 - loss: 0.6190 - acc: 0.666 - ETA: 5:13 - loss: 0.6209 - acc: 0.664 - ETA: 5:08 - loss: 0.6208 - acc: 0.664 - ETA: 5:03 - loss: 0.6220 - acc: 0.664 - ETA: 4:58 - loss: 0.6223 - acc: 0.664 - ETA: 4:54 - loss: 0.6241 - acc: 0.662 - ETA: 4:49 - loss: 0.6248 - acc: 0.662 - ETA: 4:44 - loss: 0.6252 - acc: 0.663 - ETA: 4:39 - loss: 0.6243 - acc: 0.662 - ETA: 4:34 - loss: 0.6247 - acc: 0.663 - ETA: 4:29 - loss: 0.6276 - acc: 0.661 - ETA: 4:25 - loss: 0.6273 - acc: 0.660 - ETA: 4:20 - loss: 0.6275 - acc: 0.659 - ETA: 4:15 - loss: 0.6287 - acc: 0.658 - ETA: 4:10 - loss: 0.6283 - acc: 0.658 - ETA: 4:05 - loss: 0.6284 - acc: 0.658 - ETA: 4:01 - loss: 0.6279 - acc: 0.658 - ETA: 3:56 - loss: 0.6277 - acc: 0.659 - ETA: 3:51 - loss: 0.6267 - acc: 0.658 - ETA: 3:46 - loss: 0.6279 - acc: 0.658 - ETA: 3:41 - loss: 0.6280 - acc: 0.658 - ETA: 3:36 - loss: 0.6272 - acc: 0.658 - ETA: 3:32 - loss: 0.6269 - acc: 0.659 - ETA: 3:27 - loss: 0.6266 - acc: 0.659 - ETA: 3:22 - loss: 0.6264 - acc: 0.658 - ETA: 3:17 - loss: 0.6263 - acc: 0.658 - ETA: 3:12 - loss: 0.6256 - acc: 0.659 - ETA: 3:08 - loss: 0.6254 - acc: 0.659 - ETA: 3:03 - loss: 0.6254 - acc: 0.659 - ETA: 2:58 - loss: 0.6255 - acc: 0.660 - ETA: 2:53 - loss: 0.6257 - acc: 0.661 - ETA: 2:48 - loss: 0.6259 - acc: 0.661 - ETA: 2:43 - loss: 0.6263 - acc: 0.661 - ETA: 2:39 - loss: 0.6267 - acc: 0.661 - ETA: 2:34 - loss: 0.6261 - acc: 0.662 - ETA: 2:29 - loss: 0.6279 - acc: 0.661 - ETA: 2:24 - loss: 0.6279 - acc: 0.661 - ETA: 2:19 - loss: 0.6273 - acc: 0.661 - ETA: 2:14 - loss: 0.6274 - acc: 0.661 - ETA: 2:10 - loss: 0.6273 - acc: 0.661 - ETA: 2:05 - loss: 0.6270 - acc: 0.661 - ETA: 2:00 - loss: 0.6279 - acc: 0.660 - ETA: 1:55 - loss: 0.6275 - acc: 0.660 - ETA: 1:50 - loss: 0.6280 - acc: 0.660 - ETA: 1:46 - loss: 0.6279 - acc: 0.661 - ETA: 1:41 - loss: 0.6277 - acc: 0.661 - ETA: 1:36 - loss: 0.6274 - acc: 0.661 - ETA: 1:31 - loss: 0.6270 - acc: 0.661 - ETA: 1:26 - loss: 0.6263 - acc: 0.662 - ETA: 1:21 - loss: 0.6259 - acc: 0.662 - ETA: 1:17 - loss: 0.6262 - acc: 0.662 - ETA: 1:12 - loss: 0.6257 - acc: 0.662 - ETA: 1:07 - loss: 0.6261 - acc: 0.662 - ETA: 1:02 - loss: 0.6256 - acc: 0.662 - ETA: 57s - loss: 0.6249 - acc: 0.662 - ETA: 53s - loss: 0.6249 - acc: 0.66 - ETA: 48s - loss: 0.6250 - acc: 0.66 - ETA: 43s - loss: 0.6252 - acc: 0.66 - ETA: 38s - loss: 0.6254 - acc: 0.66 - ETA: 33s - loss: 0.6242 - acc: 0.66 - ETA: 28s - loss: 0.6239 - acc: 0.66 - ETA: 23s - loss: 0.6239 - acc: 0.66 - ETA: 19s - loss: 0.6236 - acc: 0.66 - ETA: 14s - loss: 0.6236 - acc: 0.66 - ETA: 9s - loss: 0.6237 - acc: 0.6639 - ETA: 4s - loss: 0.6240 - acc: 0.6641\n",
      "Epoch 00004: val_loss improved from 0.63686 to 0.63106, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.6237 - acc: 0.6644 - val_loss: 0.6311 - val_acc: 0.6610\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.6342 - acc: 0.666 - ETA: 6:05 - loss: 0.6410 - acc: 0.656 - ETA: 6:01 - loss: 0.6380 - acc: 0.659 - ETA: 5:56 - loss: 0.6356 - acc: 0.671 - ETA: 5:51 - loss: 0.6345 - acc: 0.677 - ETA: 5:46 - loss: 0.6252 - acc: 0.687 - ETA: 5:42 - loss: 0.6224 - acc: 0.686 - ETA: 5:37 - loss: 0.6237 - acc: 0.684 - ETA: 5:32 - loss: 0.6234 - acc: 0.684 - ETA: 5:27 - loss: 0.6200 - acc: 0.684 - ETA: 5:22 - loss: 0.6192 - acc: 0.685 - ETA: 5:18 - loss: 0.6157 - acc: 0.685 - ETA: 5:13 - loss: 0.6148 - acc: 0.684 - ETA: 5:08 - loss: 0.6134 - acc: 0.685 - ETA: 5:03 - loss: 0.6135 - acc: 0.686 - ETA: 4:58 - loss: 0.6172 - acc: 0.682 - ETA: 4:53 - loss: 0.6205 - acc: 0.679 - ETA: 4:49 - loss: 0.6199 - acc: 0.679 - ETA: 4:44 - loss: 0.6196 - acc: 0.677 - ETA: 4:39 - loss: 0.6191 - acc: 0.676 - ETA: 4:34 - loss: 0.6204 - acc: 0.674 - ETA: 4:29 - loss: 0.6203 - acc: 0.673 - ETA: 4:25 - loss: 0.6204 - acc: 0.672 - ETA: 4:20 - loss: 0.6201 - acc: 0.672 - ETA: 4:15 - loss: 0.6196 - acc: 0.672 - ETA: 4:10 - loss: 0.6196 - acc: 0.672 - ETA: 4:05 - loss: 0.6218 - acc: 0.672 - ETA: 4:00 - loss: 0.6215 - acc: 0.673 - ETA: 3:56 - loss: 0.6211 - acc: 0.672 - ETA: 3:51 - loss: 0.6217 - acc: 0.672 - ETA: 3:46 - loss: 0.6223 - acc: 0.672 - ETA: 3:41 - loss: 0.6235 - acc: 0.670 - ETA: 3:36 - loss: 0.6225 - acc: 0.671 - ETA: 3:32 - loss: 0.6224 - acc: 0.671 - ETA: 3:27 - loss: 0.6216 - acc: 0.672 - ETA: 3:22 - loss: 0.6210 - acc: 0.671 - ETA: 3:17 - loss: 0.6212 - acc: 0.671 - ETA: 3:12 - loss: 0.6204 - acc: 0.671 - ETA: 3:07 - loss: 0.6206 - acc: 0.670 - ETA: 3:03 - loss: 0.6206 - acc: 0.670 - ETA: 2:58 - loss: 0.6197 - acc: 0.671 - ETA: 2:53 - loss: 0.6198 - acc: 0.670 - ETA: 2:48 - loss: 0.6182 - acc: 0.671 - ETA: 2:43 - loss: 0.6179 - acc: 0.671 - ETA: 2:39 - loss: 0.6187 - acc: 0.671 - ETA: 2:31 - loss: 0.6191 - acc: 0.673 - ETA: 2:26 - loss: 0.6190 - acc: 0.673 - ETA: 2:22 - loss: 0.6190 - acc: 0.674 - ETA: 2:17 - loss: 0.6193 - acc: 0.673 - ETA: 2:12 - loss: 0.6194 - acc: 0.674 - ETA: 2:07 - loss: 0.6193 - acc: 0.673 - ETA: 2:03 - loss: 0.6193 - acc: 0.673 - ETA: 1:58 - loss: 0.6191 - acc: 0.673 - ETA: 1:53 - loss: 0.6192 - acc: 0.673 - ETA: 1:49 - loss: 0.6191 - acc: 0.674 - ETA: 1:44 - loss: 0.6190 - acc: 0.674 - ETA: 1:39 - loss: 0.6193 - acc: 0.674 - ETA: 1:34 - loss: 0.6190 - acc: 0.674 - ETA: 1:30 - loss: 0.6194 - acc: 0.673 - ETA: 1:25 - loss: 0.6182 - acc: 0.673 - ETA: 1:20 - loss: 0.6176 - acc: 0.673 - ETA: 1:16 - loss: 0.6180 - acc: 0.673 - ETA: 1:11 - loss: 0.6180 - acc: 0.674 - ETA: 1:06 - loss: 0.6170 - acc: 0.674 - ETA: 1:01 - loss: 0.6173 - acc: 0.675 - ETA: 57s - loss: 0.6167 - acc: 0.674 - ETA: 52s - loss: 0.6170 - acc: 0.67 - ETA: 47s - loss: 0.6169 - acc: 0.67 - ETA: 42s - loss: 0.6170 - acc: 0.67 - ETA: 38s - loss: 0.6180 - acc: 0.67 - ETA: 33s - loss: 0.6182 - acc: 0.67 - ETA: 28s - loss: 0.6179 - acc: 0.67 - ETA: 23s - loss: 0.6177 - acc: 0.67 - ETA: 19s - loss: 0.6175 - acc: 0.67 - ETA: 14s - loss: 0.6176 - acc: 0.67 - ETA: 9s - loss: 0.6175 - acc: 0.6745 - ETA: 4s - loss: 0.6172 - acc: 0.6744\n",
      "Epoch 00005: val_loss improved from 0.63106 to 0.62868, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.6168 - acc: 0.6749 - val_loss: 0.6287 - val_acc: 0.6534\n",
      "Epoch 6/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.5890 - acc: 0.666 - ETA: 6:06 - loss: 0.6149 - acc: 0.666 - ETA: 6:01 - loss: 0.6050 - acc: 0.666 - ETA: 5:57 - loss: 0.5937 - acc: 0.674 - ETA: 5:52 - loss: 0.5958 - acc: 0.675 - ETA: 5:47 - loss: 0.5973 - acc: 0.678 - ETA: 5:42 - loss: 0.5981 - acc: 0.675 - ETA: 5:37 - loss: 0.5989 - acc: 0.675 - ETA: 5:33 - loss: 0.6021 - acc: 0.674 - ETA: 5:28 - loss: 0.6024 - acc: 0.669 - ETA: 5:23 - loss: 0.6036 - acc: 0.668 - ETA: 5:18 - loss: 0.6036 - acc: 0.669 - ETA: 5:13 - loss: 0.5988 - acc: 0.671 - ETA: 5:08 - loss: 0.6041 - acc: 0.669 - ETA: 5:03 - loss: 0.6042 - acc: 0.670 - ETA: 4:59 - loss: 0.6043 - acc: 0.669 - ETA: 4:54 - loss: 0.6058 - acc: 0.669 - ETA: 4:49 - loss: 0.6074 - acc: 0.668 - ETA: 4:44 - loss: 0.6055 - acc: 0.671 - ETA: 4:39 - loss: 0.6075 - acc: 0.668 - ETA: 4:34 - loss: 0.6074 - acc: 0.669 - ETA: 4:30 - loss: 0.6074 - acc: 0.669 - ETA: 4:25 - loss: 0.6087 - acc: 0.669 - ETA: 4:20 - loss: 0.6091 - acc: 0.670 - ETA: 4:15 - loss: 0.6100 - acc: 0.669 - ETA: 4:10 - loss: 0.6096 - acc: 0.670 - ETA: 4:05 - loss: 0.6089 - acc: 0.670 - ETA: 4:01 - loss: 0.6087 - acc: 0.671 - ETA: 3:56 - loss: 0.6067 - acc: 0.672 - ETA: 3:51 - loss: 0.6070 - acc: 0.671 - ETA: 3:46 - loss: 0.6078 - acc: 0.671 - ETA: 3:41 - loss: 0.6085 - acc: 0.671 - ETA: 3:36 - loss: 0.6080 - acc: 0.671 - ETA: 3:32 - loss: 0.6083 - acc: 0.671 - ETA: 3:27 - loss: 0.6078 - acc: 0.672 - ETA: 3:22 - loss: 0.6075 - acc: 0.672 - ETA: 3:17 - loss: 0.6069 - acc: 0.673 - ETA: 3:12 - loss: 0.6066 - acc: 0.673 - ETA: 3:08 - loss: 0.6067 - acc: 0.673 - ETA: 3:03 - loss: 0.6069 - acc: 0.673 - ETA: 2:58 - loss: 0.6071 - acc: 0.673 - ETA: 2:53 - loss: 0.6066 - acc: 0.673 - ETA: 2:48 - loss: 0.6078 - acc: 0.673 - ETA: 2:43 - loss: 0.6083 - acc: 0.673 - ETA: 2:39 - loss: 0.6082 - acc: 0.673 - ETA: 2:34 - loss: 0.6087 - acc: 0.673 - ETA: 2:29 - loss: 0.6086 - acc: 0.673 - ETA: 2:24 - loss: 0.6097 - acc: 0.673 - ETA: 2:19 - loss: 0.6100 - acc: 0.672 - ETA: 2:14 - loss: 0.6099 - acc: 0.672 - ETA: 2:10 - loss: 0.6101 - acc: 0.672 - ETA: 2:05 - loss: 0.6100 - acc: 0.672 - ETA: 2:00 - loss: 0.6100 - acc: 0.672 - ETA: 1:55 - loss: 0.6096 - acc: 0.672 - ETA: 1:50 - loss: 0.6095 - acc: 0.673 - ETA: 1:46 - loss: 0.6090 - acc: 0.673 - ETA: 1:41 - loss: 0.6085 - acc: 0.673 - ETA: 1:36 - loss: 0.6076 - acc: 0.674 - ETA: 1:31 - loss: 0.6072 - acc: 0.674 - ETA: 1:26 - loss: 0.6079 - acc: 0.674 - ETA: 1:20 - loss: 0.6089 - acc: 0.673 - ETA: 1:16 - loss: 0.6091 - acc: 0.673 - ETA: 1:11 - loss: 0.6090 - acc: 0.672 - ETA: 1:06 - loss: 0.6094 - acc: 0.672 - ETA: 1:01 - loss: 0.6088 - acc: 0.672 - ETA: 57s - loss: 0.6085 - acc: 0.673 - ETA: 52s - loss: 0.6085 - acc: 0.67 - ETA: 47s - loss: 0.6088 - acc: 0.67 - ETA: 42s - loss: 0.6077 - acc: 0.67 - ETA: 38s - loss: 0.6071 - acc: 0.67 - ETA: 33s - loss: 0.6069 - acc: 0.67 - ETA: 28s - loss: 0.6067 - acc: 0.67 - ETA: 23s - loss: 0.6061 - acc: 0.67 - ETA: 19s - loss: 0.6063 - acc: 0.67 - ETA: 14s - loss: 0.6061 - acc: 0.67 - ETA: 9s - loss: 0.6055 - acc: 0.6752 - ETA: 4s - loss: 0.6055 - acc: 0.6755\n",
      "Epoch 00006: val_loss improved from 0.62868 to 0.62547, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.6051 - acc: 0.6758 - val_loss: 0.6255 - val_acc: 0.6591\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:12 - loss: 0.6103 - acc: 0.666 - ETA: 6:06 - loss: 0.5948 - acc: 0.677 - ETA: 6:01 - loss: 0.5984 - acc: 0.677 - ETA: 5:56 - loss: 0.5985 - acc: 0.682 - ETA: 5:51 - loss: 0.6034 - acc: 0.677 - ETA: 5:46 - loss: 0.6036 - acc: 0.678 - ETA: 5:42 - loss: 0.6027 - acc: 0.677 - ETA: 5:37 - loss: 0.6059 - acc: 0.677 - ETA: 5:32 - loss: 0.6096 - acc: 0.677 - ETA: 5:27 - loss: 0.6086 - acc: 0.679 - ETA: 5:22 - loss: 0.6038 - acc: 0.682 - ETA: 5:18 - loss: 0.6008 - acc: 0.684 - ETA: 5:13 - loss: 0.5994 - acc: 0.685 - ETA: 5:08 - loss: 0.6007 - acc: 0.684 - ETA: 5:03 - loss: 0.6019 - acc: 0.684 - ETA: 4:58 - loss: 0.6024 - acc: 0.685 - ETA: 4:54 - loss: 0.6022 - acc: 0.686 - ETA: 4:49 - loss: 0.6002 - acc: 0.688 - ETA: 4:44 - loss: 0.5997 - acc: 0.688 - ETA: 4:39 - loss: 0.5996 - acc: 0.687 - ETA: 4:34 - loss: 0.5999 - acc: 0.687 - ETA: 4:29 - loss: 0.5977 - acc: 0.688 - ETA: 4:25 - loss: 0.5982 - acc: 0.688 - ETA: 4:20 - loss: 0.6004 - acc: 0.686 - ETA: 4:15 - loss: 0.6011 - acc: 0.686 - ETA: 4:10 - loss: 0.6020 - acc: 0.684 - ETA: 4:05 - loss: 0.6026 - acc: 0.684 - ETA: 4:00 - loss: 0.6024 - acc: 0.683 - ETA: 3:56 - loss: 0.6020 - acc: 0.683 - ETA: 3:51 - loss: 0.6023 - acc: 0.683 - ETA: 3:46 - loss: 0.6023 - acc: 0.682 - ETA: 3:41 - loss: 0.6027 - acc: 0.682 - ETA: 3:36 - loss: 0.6030 - acc: 0.681 - ETA: 3:32 - loss: 0.6024 - acc: 0.682 - ETA: 3:27 - loss: 0.6027 - acc: 0.681 - ETA: 3:17 - loss: 0.6023 - acc: 0.681 - ETA: 3:12 - loss: 0.6031 - acc: 0.681 - ETA: 3:08 - loss: 0.6034 - acc: 0.682 - ETA: 3:03 - loss: 0.6029 - acc: 0.682 - ETA: 2:59 - loss: 0.6022 - acc: 0.682 - ETA: 2:54 - loss: 0.6015 - acc: 0.683 - ETA: 2:49 - loss: 0.6025 - acc: 0.684 - ETA: 2:45 - loss: 0.6030 - acc: 0.683 - ETA: 2:40 - loss: 0.6021 - acc: 0.684 - ETA: 2:35 - loss: 0.6020 - acc: 0.684 - ETA: 2:31 - loss: 0.6026 - acc: 0.683 - ETA: 2:26 - loss: 0.6032 - acc: 0.683 - ETA: 2:21 - loss: 0.6024 - acc: 0.683 - ETA: 2:17 - loss: 0.6021 - acc: 0.683 - ETA: 2:12 - loss: 0.6020 - acc: 0.684 - ETA: 2:07 - loss: 0.6017 - acc: 0.684 - ETA: 2:03 - loss: 0.6019 - acc: 0.684 - ETA: 1:58 - loss: 0.6019 - acc: 0.684 - ETA: 1:53 - loss: 0.6009 - acc: 0.684 - ETA: 1:49 - loss: 0.6004 - acc: 0.685 - ETA: 1:44 - loss: 0.6000 - acc: 0.685 - ETA: 1:39 - loss: 0.6005 - acc: 0.684 - ETA: 1:34 - loss: 0.6003 - acc: 0.684 - ETA: 1:30 - loss: 0.6002 - acc: 0.684 - ETA: 1:25 - loss: 0.6004 - acc: 0.685 - ETA: 1:20 - loss: 0.6003 - acc: 0.684 - ETA: 1:16 - loss: 0.6007 - acc: 0.684 - ETA: 1:11 - loss: 0.6001 - acc: 0.685 - ETA: 1:06 - loss: 0.6006 - acc: 0.684 - ETA: 1:01 - loss: 0.6007 - acc: 0.684 - ETA: 57s - loss: 0.6004 - acc: 0.684 - ETA: 52s - loss: 0.6006 - acc: 0.68 - ETA: 47s - loss: 0.6007 - acc: 0.68 - ETA: 42s - loss: 0.6008 - acc: 0.68 - ETA: 38s - loss: 0.6005 - acc: 0.68 - ETA: 33s - loss: 0.6000 - acc: 0.68 - ETA: 28s - loss: 0.6001 - acc: 0.68 - ETA: 23s - loss: 0.6000 - acc: 0.68 - ETA: 19s - loss: 0.5999 - acc: 0.68 - ETA: 14s - loss: 0.5999 - acc: 0.68 - ETA: 9s - loss: 0.5995 - acc: 0.6842 - ETA: 4s - loss: 0.5999 - acc: 0.6836\n",
      "Epoch 00007: val_loss improved from 0.62547 to 0.61976, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.5995 - acc: 0.6836 - val_loss: 0.6198 - val_acc: 0.6604\n",
      "Epoch 8/100\n",
      "77/78 [============================>.] - ETA: 6:12 - loss: 0.5790 - acc: 0.708 - ETA: 6:06 - loss: 0.5813 - acc: 0.697 - ETA: 6:01 - loss: 0.5937 - acc: 0.687 - ETA: 5:56 - loss: 0.5996 - acc: 0.682 - ETA: 5:51 - loss: 0.5959 - acc: 0.687 - ETA: 5:47 - loss: 0.5951 - acc: 0.685 - ETA: 5:42 - loss: 0.5935 - acc: 0.693 - ETA: 5:37 - loss: 0.5924 - acc: 0.694 - ETA: 5:01 - loss: 0.5845 - acc: 0.691 - ETA: 4:59 - loss: 0.5860 - acc: 0.691 - ETA: 4:57 - loss: 0.5886 - acc: 0.687 - ETA: 4:55 - loss: 0.5864 - acc: 0.689 - ETA: 4:52 - loss: 0.5854 - acc: 0.689 - ETA: 4:49 - loss: 0.5865 - acc: 0.688 - ETA: 4:46 - loss: 0.5864 - acc: 0.688 - ETA: 4:42 - loss: 0.5868 - acc: 0.686 - ETA: 4:39 - loss: 0.5878 - acc: 0.686 - ETA: 4:35 - loss: 0.5880 - acc: 0.685 - ETA: 4:31 - loss: 0.5881 - acc: 0.686 - ETA: 4:27 - loss: 0.5889 - acc: 0.686 - ETA: 4:23 - loss: 0.5908 - acc: 0.686 - ETA: 4:19 - loss: 0.5930 - acc: 0.685 - ETA: 4:15 - loss: 0.5920 - acc: 0.686 - ETA: 4:10 - loss: 0.5932 - acc: 0.685 - ETA: 4:06 - loss: 0.5933 - acc: 0.685 - ETA: 4:02 - loss: 0.5946 - acc: 0.685 - ETA: 3:57 - loss: 0.5931 - acc: 0.686 - ETA: 3:53 - loss: 0.5929 - acc: 0.686 - ETA: 3:49 - loss: 0.5938 - acc: 0.685 - ETA: 3:44 - loss: 0.5947 - acc: 0.685 - ETA: 3:40 - loss: 0.5938 - acc: 0.686 - ETA: 3:35 - loss: 0.5931 - acc: 0.687 - ETA: 3:31 - loss: 0.5921 - acc: 0.688 - ETA: 3:26 - loss: 0.5937 - acc: 0.686 - ETA: 3:22 - loss: 0.5944 - acc: 0.686 - ETA: 3:17 - loss: 0.5938 - acc: 0.686 - ETA: 3:13 - loss: 0.5935 - acc: 0.686 - ETA: 3:08 - loss: 0.5942 - acc: 0.685 - ETA: 3:03 - loss: 0.5936 - acc: 0.685 - ETA: 2:59 - loss: 0.5935 - acc: 0.685 - ETA: 2:54 - loss: 0.5935 - acc: 0.685 - ETA: 2:49 - loss: 0.5931 - acc: 0.686 - ETA: 2:45 - loss: 0.5932 - acc: 0.686 - ETA: 2:40 - loss: 0.5929 - acc: 0.686 - ETA: 2:36 - loss: 0.5926 - acc: 0.686 - ETA: 2:31 - loss: 0.5922 - acc: 0.687 - ETA: 2:26 - loss: 0.5917 - acc: 0.687 - ETA: 2:22 - loss: 0.5919 - acc: 0.687 - ETA: 2:17 - loss: 0.5915 - acc: 0.688 - ETA: 2:12 - loss: 0.5913 - acc: 0.688 - ETA: 2:07 - loss: 0.5917 - acc: 0.689 - ETA: 2:03 - loss: 0.5915 - acc: 0.689 - ETA: 1:58 - loss: 0.5920 - acc: 0.689 - ETA: 1:53 - loss: 0.5918 - acc: 0.689 - ETA: 1:49 - loss: 0.5919 - acc: 0.689 - ETA: 1:44 - loss: 0.5915 - acc: 0.689 - ETA: 1:39 - loss: 0.5918 - acc: 0.689 - ETA: 1:34 - loss: 0.5921 - acc: 0.688 - ETA: 1:30 - loss: 0.5925 - acc: 0.688 - ETA: 1:25 - loss: 0.5922 - acc: 0.689 - ETA: 1:20 - loss: 0.5927 - acc: 0.688 - ETA: 1:16 - loss: 0.5928 - acc: 0.688 - ETA: 1:11 - loss: 0.5925 - acc: 0.687 - ETA: 1:06 - loss: 0.5924 - acc: 0.688 - ETA: 1:01 - loss: 0.5925 - acc: 0.688 - ETA: 57s - loss: 0.5919 - acc: 0.688 - ETA: 52s - loss: 0.5929 - acc: 0.68 - ETA: 47s - loss: 0.5927 - acc: 0.68 - ETA: 42s - loss: 0.5926 - acc: 0.68 - ETA: 38s - loss: 0.5926 - acc: 0.68 - ETA: 33s - loss: 0.5928 - acc: 0.68 - ETA: 28s - loss: 0.5925 - acc: 0.68 - ETA: 23s - loss: 0.5915 - acc: 0.68 - ETA: 19s - loss: 0.5909 - acc: 0.68 - ETA: 14s - loss: 0.5917 - acc: 0.68 - ETA: 9s - loss: 0.5919 - acc: 0.6872 - ETA: 4s - loss: 0.5917 - acc: 0.6874\n",
      "Epoch 00008: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.5921 - acc: 0.6872 - val_loss: 0.6220 - val_acc: 0.6641\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.6019 - acc: 0.677 - ETA: 6:06 - loss: 0.5863 - acc: 0.697 - ETA: 6:01 - loss: 0.5820 - acc: 0.708 - ETA: 5:56 - loss: 0.5760 - acc: 0.705 - ETA: 5:51 - loss: 0.5835 - acc: 0.693 - ETA: 5:46 - loss: 0.5798 - acc: 0.699 - ETA: 5:41 - loss: 0.5778 - acc: 0.696 - ETA: 5:37 - loss: 0.5793 - acc: 0.691 - ETA: 5:32 - loss: 0.5805 - acc: 0.688 - ETA: 5:27 - loss: 0.5765 - acc: 0.690 - ETA: 4:57 - loss: 0.5841 - acc: 0.688 - ETA: 4:55 - loss: 0.5815 - acc: 0.688 - ETA: 4:52 - loss: 0.5809 - acc: 0.687 - ETA: 4:49 - loss: 0.5800 - acc: 0.686 - ETA: 4:46 - loss: 0.5807 - acc: 0.684 - ETA: 4:42 - loss: 0.5806 - acc: 0.684 - ETA: 4:39 - loss: 0.5829 - acc: 0.685 - ETA: 4:35 - loss: 0.5818 - acc: 0.685 - ETA: 4:31 - loss: 0.5791 - acc: 0.688 - ETA: 4:27 - loss: 0.5802 - acc: 0.690 - ETA: 4:23 - loss: 0.5808 - acc: 0.690 - ETA: 4:19 - loss: 0.5828 - acc: 0.689 - ETA: 4:15 - loss: 0.5813 - acc: 0.692 - ETA: 4:11 - loss: 0.5819 - acc: 0.692 - ETA: 4:06 - loss: 0.5829 - acc: 0.691 - ETA: 4:02 - loss: 0.5841 - acc: 0.689 - ETA: 3:58 - loss: 0.5838 - acc: 0.690 - ETA: 3:53 - loss: 0.5825 - acc: 0.691 - ETA: 3:49 - loss: 0.5822 - acc: 0.691 - ETA: 3:44 - loss: 0.5824 - acc: 0.691 - ETA: 3:40 - loss: 0.5830 - acc: 0.691 - ETA: 3:35 - loss: 0.5835 - acc: 0.691 - ETA: 3:31 - loss: 0.5828 - acc: 0.691 - ETA: 3:26 - loss: 0.5821 - acc: 0.692 - ETA: 3:22 - loss: 0.5815 - acc: 0.693 - ETA: 3:17 - loss: 0.5824 - acc: 0.693 - ETA: 3:13 - loss: 0.5821 - acc: 0.694 - ETA: 3:08 - loss: 0.5813 - acc: 0.695 - ETA: 3:03 - loss: 0.5805 - acc: 0.695 - ETA: 2:59 - loss: 0.5801 - acc: 0.696 - ETA: 2:54 - loss: 0.5799 - acc: 0.696 - ETA: 2:50 - loss: 0.5812 - acc: 0.695 - ETA: 2:45 - loss: 0.5819 - acc: 0.695 - ETA: 2:40 - loss: 0.5819 - acc: 0.695 - ETA: 2:36 - loss: 0.5817 - acc: 0.695 - ETA: 2:31 - loss: 0.5810 - acc: 0.695 - ETA: 2:26 - loss: 0.5806 - acc: 0.696 - ETA: 2:22 - loss: 0.5811 - acc: 0.696 - ETA: 2:17 - loss: 0.5811 - acc: 0.696 - ETA: 2:12 - loss: 0.5814 - acc: 0.696 - ETA: 2:07 - loss: 0.5814 - acc: 0.696 - ETA: 2:03 - loss: 0.5809 - acc: 0.696 - ETA: 1:58 - loss: 0.5808 - acc: 0.696 - ETA: 1:53 - loss: 0.5801 - acc: 0.697 - ETA: 1:49 - loss: 0.5791 - acc: 0.698 - ETA: 1:44 - loss: 0.5790 - acc: 0.697 - ETA: 1:39 - loss: 0.5791 - acc: 0.697 - ETA: 1:35 - loss: 0.5790 - acc: 0.697 - ETA: 1:30 - loss: 0.5786 - acc: 0.697 - ETA: 1:25 - loss: 0.5786 - acc: 0.697 - ETA: 1:20 - loss: 0.5783 - acc: 0.698 - ETA: 1:16 - loss: 0.5785 - acc: 0.698 - ETA: 1:11 - loss: 0.5785 - acc: 0.698 - ETA: 1:06 - loss: 0.5782 - acc: 0.698 - ETA: 1:01 - loss: 0.5782 - acc: 0.698 - ETA: 57s - loss: 0.5787 - acc: 0.697 - ETA: 52s - loss: 0.5784 - acc: 0.69 - ETA: 47s - loss: 0.5789 - acc: 0.69 - ETA: 42s - loss: 0.5788 - acc: 0.69 - ETA: 38s - loss: 0.5796 - acc: 0.69 - ETA: 33s - loss: 0.5797 - acc: 0.69 - ETA: 28s - loss: 0.5798 - acc: 0.69 - ETA: 23s - loss: 0.5796 - acc: 0.69 - ETA: 19s - loss: 0.5795 - acc: 0.69 - ETA: 14s - loss: 0.5794 - acc: 0.69 - ETA: 9s - loss: 0.5788 - acc: 0.6956 - ETA: 4s - loss: 0.5787 - acc: 0.6956\n",
      "Epoch 00009: val_loss improved from 0.61976 to 0.61703, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.5788 - acc: 0.6961 - val_loss: 0.6170 - val_acc: 0.6654\n",
      "Epoch 10/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.5393 - acc: 0.729 - ETA: 6:05 - loss: 0.5473 - acc: 0.718 - ETA: 6:01 - loss: 0.5769 - acc: 0.694 - ETA: 5:56 - loss: 0.5770 - acc: 0.692 - ETA: 5:51 - loss: 0.5703 - acc: 0.695 - ETA: 5:46 - loss: 0.5732 - acc: 0.694 - ETA: 5:42 - loss: 0.5737 - acc: 0.694 - ETA: 5:37 - loss: 0.5771 - acc: 0.697 - ETA: 5:32 - loss: 0.5794 - acc: 0.696 - ETA: 5:27 - loss: 0.5782 - acc: 0.699 - ETA: 5:22 - loss: 0.5768 - acc: 0.698 - ETA: 5:17 - loss: 0.5761 - acc: 0.697 - ETA: 5:13 - loss: 0.5773 - acc: 0.698 - ETA: 5:08 - loss: 0.5806 - acc: 0.695 - ETA: 5:03 - loss: 0.5832 - acc: 0.695 - ETA: 4:58 - loss: 0.5855 - acc: 0.694 - ETA: 4:53 - loss: 0.5848 - acc: 0.694 - ETA: 4:49 - loss: 0.5835 - acc: 0.695 - ETA: 4:44 - loss: 0.5845 - acc: 0.695 - ETA: 4:39 - loss: 0.5829 - acc: 0.695 - ETA: 4:34 - loss: 0.5832 - acc: 0.696 - ETA: 4:29 - loss: 0.5817 - acc: 0.697 - ETA: 4:25 - loss: 0.5821 - acc: 0.697 - ETA: 4:20 - loss: 0.5806 - acc: 0.697 - ETA: 4:15 - loss: 0.5821 - acc: 0.696 - ETA: 4:10 - loss: 0.5823 - acc: 0.695 - ETA: 4:05 - loss: 0.5824 - acc: 0.695 - ETA: 4:00 - loss: 0.5825 - acc: 0.696 - ETA: 3:56 - loss: 0.5807 - acc: 0.697 - ETA: 3:51 - loss: 0.5800 - acc: 0.697 - ETA: 3:46 - loss: 0.5796 - acc: 0.698 - ETA: 3:41 - loss: 0.5792 - acc: 0.699 - ETA: 3:36 - loss: 0.5784 - acc: 0.698 - ETA: 3:32 - loss: 0.5775 - acc: 0.698 - ETA: 3:27 - loss: 0.5782 - acc: 0.698 - ETA: 3:22 - loss: 0.5790 - acc: 0.696 - ETA: 3:17 - loss: 0.5792 - acc: 0.697 - ETA: 3:12 - loss: 0.5792 - acc: 0.697 - ETA: 3:07 - loss: 0.5793 - acc: 0.697 - ETA: 3:03 - loss: 0.5770 - acc: 0.699 - ETA: 2:58 - loss: 0.5760 - acc: 0.698 - ETA: 2:49 - loss: 0.5738 - acc: 0.700 - ETA: 2:45 - loss: 0.5739 - acc: 0.699 - ETA: 2:40 - loss: 0.5733 - acc: 0.699 - ETA: 2:35 - loss: 0.5722 - acc: 0.700 - ETA: 2:31 - loss: 0.5725 - acc: 0.700 - ETA: 2:26 - loss: 0.5720 - acc: 0.700 - ETA: 2:21 - loss: 0.5715 - acc: 0.701 - ETA: 2:17 - loss: 0.5716 - acc: 0.701 - ETA: 2:12 - loss: 0.5720 - acc: 0.700 - ETA: 2:07 - loss: 0.5726 - acc: 0.701 - ETA: 2:03 - loss: 0.5727 - acc: 0.701 - ETA: 1:58 - loss: 0.5732 - acc: 0.701 - ETA: 1:53 - loss: 0.5732 - acc: 0.700 - ETA: 1:49 - loss: 0.5730 - acc: 0.700 - ETA: 1:44 - loss: 0.5731 - acc: 0.700 - ETA: 1:39 - loss: 0.5734 - acc: 0.700 - ETA: 1:34 - loss: 0.5732 - acc: 0.700 - ETA: 1:30 - loss: 0.5719 - acc: 0.701 - ETA: 1:25 - loss: 0.5714 - acc: 0.701 - ETA: 1:20 - loss: 0.5709 - acc: 0.701 - ETA: 1:16 - loss: 0.5711 - acc: 0.701 - ETA: 1:11 - loss: 0.5715 - acc: 0.701 - ETA: 1:06 - loss: 0.5726 - acc: 0.701 - ETA: 1:01 - loss: 0.5726 - acc: 0.701 - ETA: 57s - loss: 0.5722 - acc: 0.701 - ETA: 52s - loss: 0.5729 - acc: 0.70 - ETA: 47s - loss: 0.5733 - acc: 0.70 - ETA: 42s - loss: 0.5728 - acc: 0.70 - ETA: 38s - loss: 0.5721 - acc: 0.70 - ETA: 33s - loss: 0.5716 - acc: 0.70 - ETA: 28s - loss: 0.5715 - acc: 0.70 - ETA: 23s - loss: 0.5713 - acc: 0.70 - ETA: 19s - loss: 0.5717 - acc: 0.70 - ETA: 14s - loss: 0.5718 - acc: 0.70 - ETA: 9s - loss: 0.5723 - acc: 0.7027 - ETA: 4s - loss: 0.5720 - acc: 0.7032\n",
      "Epoch 00010: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.5718 - acc: 0.7031 - val_loss: 0.6261 - val_acc: 0.6509\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.5584 - acc: 0.739 - ETA: 6:06 - loss: 0.5361 - acc: 0.755 - ETA: 6:01 - loss: 0.5598 - acc: 0.718 - ETA: 5:56 - loss: 0.5717 - acc: 0.716 - ETA: 5:51 - loss: 0.5664 - acc: 0.718 - ETA: 5:46 - loss: 0.5677 - acc: 0.713 - ETA: 5:42 - loss: 0.5646 - acc: 0.718 - ETA: 5:37 - loss: 0.5673 - acc: 0.718 - ETA: 5:32 - loss: 0.5711 - acc: 0.710 - ETA: 5:27 - loss: 0.5691 - acc: 0.707 - ETA: 5:22 - loss: 0.5696 - acc: 0.708 - ETA: 5:18 - loss: 0.5680 - acc: 0.710 - ETA: 5:13 - loss: 0.5703 - acc: 0.708 - ETA: 5:08 - loss: 0.5679 - acc: 0.711 - ETA: 5:03 - loss: 0.5685 - acc: 0.710 - ETA: 4:58 - loss: 0.5681 - acc: 0.710 - ETA: 4:53 - loss: 0.5698 - acc: 0.709 - ETA: 4:49 - loss: 0.5684 - acc: 0.710 - ETA: 4:44 - loss: 0.5678 - acc: 0.708 - ETA: 4:39 - loss: 0.5678 - acc: 0.708 - ETA: 4:34 - loss: 0.5691 - acc: 0.707 - ETA: 4:29 - loss: 0.5674 - acc: 0.708 - ETA: 4:25 - loss: 0.5663 - acc: 0.710 - ETA: 4:20 - loss: 0.5673 - acc: 0.710 - ETA: 4:15 - loss: 0.5688 - acc: 0.709 - ETA: 4:10 - loss: 0.5699 - acc: 0.708 - ETA: 4:05 - loss: 0.5690 - acc: 0.708 - ETA: 4:00 - loss: 0.5680 - acc: 0.708 - ETA: 3:56 - loss: 0.5673 - acc: 0.709 - ETA: 3:51 - loss: 0.5687 - acc: 0.708 - ETA: 3:46 - loss: 0.5699 - acc: 0.706 - ETA: 3:41 - loss: 0.5683 - acc: 0.707 - ETA: 3:36 - loss: 0.5668 - acc: 0.709 - ETA: 3:32 - loss: 0.5677 - acc: 0.707 - ETA: 3:27 - loss: 0.5666 - acc: 0.708 - ETA: 3:22 - loss: 0.5666 - acc: 0.707 - ETA: 3:17 - loss: 0.5661 - acc: 0.707 - ETA: 3:12 - loss: 0.5653 - acc: 0.707 - ETA: 3:07 - loss: 0.5653 - acc: 0.708 - ETA: 3:03 - loss: 0.5645 - acc: 0.709 - ETA: 2:58 - loss: 0.5651 - acc: 0.709 - ETA: 2:53 - loss: 0.5643 - acc: 0.708 - ETA: 2:48 - loss: 0.5637 - acc: 0.709 - ETA: 2:43 - loss: 0.5638 - acc: 0.709 - ETA: 2:39 - loss: 0.5644 - acc: 0.709 - ETA: 2:31 - loss: 0.5643 - acc: 0.709 - ETA: 2:26 - loss: 0.5645 - acc: 0.709 - ETA: 2:21 - loss: 0.5640 - acc: 0.710 - ETA: 2:17 - loss: 0.5638 - acc: 0.710 - ETA: 2:12 - loss: 0.5648 - acc: 0.709 - ETA: 2:07 - loss: 0.5649 - acc: 0.708 - ETA: 2:03 - loss: 0.5647 - acc: 0.708 - ETA: 1:58 - loss: 0.5652 - acc: 0.707 - ETA: 1:53 - loss: 0.5644 - acc: 0.708 - ETA: 1:49 - loss: 0.5638 - acc: 0.709 - ETA: 1:44 - loss: 0.5622 - acc: 0.710 - ETA: 1:39 - loss: 0.5619 - acc: 0.711 - ETA: 1:34 - loss: 0.5625 - acc: 0.710 - ETA: 1:30 - loss: 0.5629 - acc: 0.710 - ETA: 1:25 - loss: 0.5630 - acc: 0.710 - ETA: 1:20 - loss: 0.5633 - acc: 0.709 - ETA: 1:16 - loss: 0.5638 - acc: 0.709 - ETA: 1:11 - loss: 0.5628 - acc: 0.710 - ETA: 1:06 - loss: 0.5626 - acc: 0.711 - ETA: 1:01 - loss: 0.5628 - acc: 0.710 - ETA: 57s - loss: 0.5626 - acc: 0.711 - ETA: 52s - loss: 0.5624 - acc: 0.71 - ETA: 47s - loss: 0.5625 - acc: 0.71 - ETA: 42s - loss: 0.5622 - acc: 0.71 - ETA: 38s - loss: 0.5622 - acc: 0.71 - ETA: 33s - loss: 0.5618 - acc: 0.71 - ETA: 28s - loss: 0.5619 - acc: 0.71 - ETA: 23s - loss: 0.5620 - acc: 0.71 - ETA: 19s - loss: 0.5621 - acc: 0.71 - ETA: 14s - loss: 0.5624 - acc: 0.71 - ETA: 9s - loss: 0.5622 - acc: 0.7119 - ETA: 4s - loss: 0.5629 - acc: 0.7110\n",
      "Epoch 00011: val_loss improved from 0.61703 to 0.61044, saving model to deep_transfer_3_class_v3_ck.h5\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.5629 - acc: 0.7114 - val_loss: 0.6104 - val_acc: 0.6585\n",
      "Epoch 12/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.5398 - acc: 0.750 - ETA: 6:05 - loss: 0.5469 - acc: 0.734 - ETA: 6:01 - loss: 0.5498 - acc: 0.722 - ETA: 5:56 - loss: 0.5526 - acc: 0.718 - ETA: 5:51 - loss: 0.5677 - acc: 0.704 - ETA: 5:46 - loss: 0.5778 - acc: 0.692 - ETA: 5:41 - loss: 0.5816 - acc: 0.689 - ETA: 5:37 - loss: 0.5740 - acc: 0.700 - ETA: 5:32 - loss: 0.5721 - acc: 0.701 - ETA: 5:27 - loss: 0.5648 - acc: 0.708 - ETA: 5:22 - loss: 0.5624 - acc: 0.709 - ETA: 5:17 - loss: 0.5606 - acc: 0.709 - ETA: 5:13 - loss: 0.5599 - acc: 0.709 - ETA: 5:08 - loss: 0.5645 - acc: 0.706 - ETA: 5:03 - loss: 0.5649 - acc: 0.708 - ETA: 4:58 - loss: 0.5620 - acc: 0.710 - ETA: 4:53 - loss: 0.5619 - acc: 0.710 - ETA: 4:49 - loss: 0.5627 - acc: 0.710 - ETA: 4:44 - loss: 0.5624 - acc: 0.710 - ETA: 4:39 - loss: 0.5620 - acc: 0.708 - ETA: 4:34 - loss: 0.5612 - acc: 0.708 - ETA: 4:29 - loss: 0.5625 - acc: 0.706 - ETA: 4:25 - loss: 0.5623 - acc: 0.706 - ETA: 4:20 - loss: 0.5605 - acc: 0.707 - ETA: 4:15 - loss: 0.5595 - acc: 0.707 - ETA: 4:02 - loss: 0.5588 - acc: 0.706 - ETA: 3:57 - loss: 0.5591 - acc: 0.707 - ETA: 3:53 - loss: 0.5594 - acc: 0.705 - ETA: 3:49 - loss: 0.5593 - acc: 0.705 - ETA: 3:44 - loss: 0.5590 - acc: 0.705 - ETA: 3:40 - loss: 0.5605 - acc: 0.705 - ETA: 3:35 - loss: 0.5627 - acc: 0.703 - ETA: 3:31 - loss: 0.5629 - acc: 0.703 - ETA: 3:26 - loss: 0.5629 - acc: 0.703 - ETA: 3:22 - loss: 0.5631 - acc: 0.703 - ETA: 3:17 - loss: 0.5619 - acc: 0.704 - ETA: 3:12 - loss: 0.5611 - acc: 0.706 - ETA: 3:08 - loss: 0.5611 - acc: 0.707 - ETA: 3:03 - loss: 0.5608 - acc: 0.708 - ETA: 2:59 - loss: 0.5606 - acc: 0.707 - ETA: 2:54 - loss: 0.5614 - acc: 0.706 - ETA: 2:49 - loss: 0.5615 - acc: 0.706 - ETA: 2:45 - loss: 0.5618 - acc: 0.705 - ETA: 2:40 - loss: 0.5621 - acc: 0.705 - ETA: 2:35 - loss: 0.5614 - acc: 0.706 - ETA: 2:31 - loss: 0.5605 - acc: 0.706 - ETA: 2:26 - loss: 0.5601 - acc: 0.706 - ETA: 2:21 - loss: 0.5602 - acc: 0.705 - ETA: 2:17 - loss: 0.5615 - acc: 0.704 - ETA: 2:12 - loss: 0.5609 - acc: 0.704 - ETA: 2:07 - loss: 0.5609 - acc: 0.704 - ETA: 2:03 - loss: 0.5618 - acc: 0.704 - ETA: 1:58 - loss: 0.5614 - acc: 0.704 - ETA: 1:53 - loss: 0.5608 - acc: 0.704 - ETA: 1:49 - loss: 0.5618 - acc: 0.704 - ETA: 1:44 - loss: 0.5610 - acc: 0.705 - ETA: 1:39 - loss: 0.5608 - acc: 0.705 - ETA: 1:34 - loss: 0.5607 - acc: 0.706 - ETA: 1:30 - loss: 0.5600 - acc: 0.707 - ETA: 1:25 - loss: 0.5601 - acc: 0.707 - ETA: 1:20 - loss: 0.5596 - acc: 0.708 - ETA: 1:16 - loss: 0.5590 - acc: 0.708 - ETA: 1:11 - loss: 0.5589 - acc: 0.708 - ETA: 1:06 - loss: 0.5585 - acc: 0.709 - ETA: 1:01 - loss: 0.5577 - acc: 0.709 - ETA: 57s - loss: 0.5586 - acc: 0.708 - ETA: 52s - loss: 0.5592 - acc: 0.70 - ETA: 47s - loss: 0.5591 - acc: 0.70 - ETA: 42s - loss: 0.5591 - acc: 0.70 - ETA: 38s - loss: 0.5584 - acc: 0.70 - ETA: 33s - loss: 0.5582 - acc: 0.71 - ETA: 28s - loss: 0.5580 - acc: 0.71 - ETA: 23s - loss: 0.5581 - acc: 0.71 - ETA: 19s - loss: 0.5585 - acc: 0.71 - ETA: 14s - loss: 0.5583 - acc: 0.71 - ETA: 9s - loss: 0.5580 - acc: 0.7115 - ETA: 4s - loss: 0.5580 - acc: 0.7114\n",
      "Epoch 00012: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.5578 - acc: 0.7118 - val_loss: 0.6203 - val_acc: 0.6528\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.5179 - acc: 0.739 - ETA: 6:05 - loss: 0.5389 - acc: 0.729 - ETA: 6:01 - loss: 0.5456 - acc: 0.718 - ETA: 5:56 - loss: 0.5488 - acc: 0.713 - ETA: 5:51 - loss: 0.5466 - acc: 0.725 - ETA: 5:46 - loss: 0.5493 - acc: 0.722 - ETA: 5:41 - loss: 0.5494 - acc: 0.726 - ETA: 5:37 - loss: 0.5496 - acc: 0.726 - ETA: 5:32 - loss: 0.5452 - acc: 0.729 - ETA: 5:27 - loss: 0.5449 - acc: 0.727 - ETA: 5:22 - loss: 0.5479 - acc: 0.723 - ETA: 5:17 - loss: 0.5529 - acc: 0.720 - ETA: 5:13 - loss: 0.5538 - acc: 0.718 - ETA: 5:08 - loss: 0.5563 - acc: 0.718 - ETA: 5:03 - loss: 0.5552 - acc: 0.721 - ETA: 4:58 - loss: 0.5565 - acc: 0.720 - ETA: 4:53 - loss: 0.5558 - acc: 0.718 - ETA: 4:49 - loss: 0.5559 - acc: 0.717 - ETA: 4:44 - loss: 0.5577 - acc: 0.717 - ETA: 4:39 - loss: 0.5569 - acc: 0.717 - ETA: 4:34 - loss: 0.5573 - acc: 0.717 - ETA: 4:29 - loss: 0.5587 - acc: 0.715 - ETA: 4:24 - loss: 0.5598 - acc: 0.715 - ETA: 4:20 - loss: 0.5581 - acc: 0.716 - ETA: 4:15 - loss: 0.5566 - acc: 0.717 - ETA: 4:10 - loss: 0.5574 - acc: 0.716 - ETA: 4:05 - loss: 0.5576 - acc: 0.714 - ETA: 4:00 - loss: 0.5577 - acc: 0.713 - ETA: 3:56 - loss: 0.5572 - acc: 0.713 - ETA: 3:51 - loss: 0.5555 - acc: 0.714 - ETA: 3:46 - loss: 0.5555 - acc: 0.715 - ETA: 3:41 - loss: 0.5554 - acc: 0.714 - ETA: 3:36 - loss: 0.5550 - acc: 0.715 - ETA: 3:32 - loss: 0.5537 - acc: 0.716 - ETA: 3:27 - loss: 0.5537 - acc: 0.715 - ETA: 3:22 - loss: 0.5528 - acc: 0.715 - ETA: 3:17 - loss: 0.5526 - acc: 0.715 - ETA: 3:12 - loss: 0.5528 - acc: 0.717 - ETA: 3:07 - loss: 0.5530 - acc: 0.717 - ETA: 3:03 - loss: 0.5528 - acc: 0.717 - ETA: 2:58 - loss: 0.5542 - acc: 0.717 - ETA: 2:53 - loss: 0.5546 - acc: 0.716 - ETA: 2:48 - loss: 0.5547 - acc: 0.716 - ETA: 2:43 - loss: 0.5541 - acc: 0.717 - ETA: 2:39 - loss: 0.5539 - acc: 0.718 - ETA: 2:34 - loss: 0.5549 - acc: 0.717 - ETA: 2:29 - loss: 0.5545 - acc: 0.717 - ETA: 2:24 - loss: 0.5539 - acc: 0.718 - ETA: 2:19 - loss: 0.5539 - acc: 0.718 - ETA: 2:14 - loss: 0.5534 - acc: 0.718 - ETA: 2:10 - loss: 0.5542 - acc: 0.717 - ETA: 2:05 - loss: 0.5543 - acc: 0.717 - ETA: 2:00 - loss: 0.5538 - acc: 0.717 - ETA: 1:55 - loss: 0.5539 - acc: 0.717 - ETA: 1:50 - loss: 0.5547 - acc: 0.716 - ETA: 1:46 - loss: 0.5544 - acc: 0.716 - ETA: 1:41 - loss: 0.5539 - acc: 0.717 - ETA: 1:36 - loss: 0.5542 - acc: 0.716 - ETA: 1:31 - loss: 0.5537 - acc: 0.717 - ETA: 1:26 - loss: 0.5537 - acc: 0.717 - ETA: 1:21 - loss: 0.5545 - acc: 0.716 - ETA: 1:17 - loss: 0.5545 - acc: 0.717 - ETA: 1:12 - loss: 0.5539 - acc: 0.717 - ETA: 1:07 - loss: 0.5532 - acc: 0.718 - ETA: 1:02 - loss: 0.5531 - acc: 0.717 - ETA: 57s - loss: 0.5533 - acc: 0.717 - ETA: 53s - loss: 0.5533 - acc: 0.71 - ETA: 48s - loss: 0.5527 - acc: 0.71 - ETA: 43s - loss: 0.5527 - acc: 0.71 - ETA: 38s - loss: 0.5535 - acc: 0.71 - ETA: 33s - loss: 0.5538 - acc: 0.71 - ETA: 28s - loss: 0.5543 - acc: 0.71 - ETA: 24s - loss: 0.5542 - acc: 0.71 - ETA: 19s - loss: 0.5542 - acc: 0.71 - ETA: 14s - loss: 0.5542 - acc: 0.71 - ETA: 9s - loss: 0.5536 - acc: 0.7175 - ETA: 4s - loss: 0.5539 - acc: 0.7170\n",
      "Epoch 00013: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.5541 - acc: 0.7165 - val_loss: 0.6134 - val_acc: 0.6654\n",
      "Epoch 14/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.5665 - acc: 0.729 - ETA: 6:06 - loss: 0.5496 - acc: 0.734 - ETA: 6:01 - loss: 0.5257 - acc: 0.750 - ETA: 5:57 - loss: 0.5256 - acc: 0.752 - ETA: 5:52 - loss: 0.5203 - acc: 0.754 - ETA: 5:47 - loss: 0.5247 - acc: 0.744 - ETA: 5:42 - loss: 0.5212 - acc: 0.750 - ETA: 5:37 - loss: 0.5185 - acc: 0.756 - ETA: 5:32 - loss: 0.5255 - acc: 0.750 - ETA: 5:27 - loss: 0.5278 - acc: 0.745 - ETA: 5:22 - loss: 0.5289 - acc: 0.746 - ETA: 5:17 - loss: 0.5305 - acc: 0.746 - ETA: 5:13 - loss: 0.5307 - acc: 0.746 - ETA: 5:08 - loss: 0.5288 - acc: 0.749 - ETA: 5:03 - loss: 0.5353 - acc: 0.742 - ETA: 4:58 - loss: 0.5337 - acc: 0.743 - ETA: 4:53 - loss: 0.5371 - acc: 0.739 - ETA: 4:49 - loss: 0.5383 - acc: 0.738 - ETA: 4:44 - loss: 0.5393 - acc: 0.737 - ETA: 4:39 - loss: 0.5408 - acc: 0.737 - ETA: 4:34 - loss: 0.5399 - acc: 0.739 - ETA: 4:29 - loss: 0.5397 - acc: 0.739 - ETA: 4:24 - loss: 0.5397 - acc: 0.738 - ETA: 4:20 - loss: 0.5397 - acc: 0.739 - ETA: 4:15 - loss: 0.5408 - acc: 0.737 - ETA: 4:10 - loss: 0.5428 - acc: 0.736 - ETA: 4:05 - loss: 0.5405 - acc: 0.737 - ETA: 4:00 - loss: 0.5413 - acc: 0.736 - ETA: 3:56 - loss: 0.5411 - acc: 0.736 - ETA: 3:51 - loss: 0.5412 - acc: 0.735 - ETA: 3:46 - loss: 0.5431 - acc: 0.733 - ETA: 3:41 - loss: 0.5432 - acc: 0.733 - ETA: 3:36 - loss: 0.5429 - acc: 0.733 - ETA: 3:32 - loss: 0.5416 - acc: 0.734 - ETA: 3:27 - loss: 0.5416 - acc: 0.733 - ETA: 3:22 - loss: 0.5414 - acc: 0.732 - ETA: 3:17 - loss: 0.5426 - acc: 0.730 - ETA: 3:12 - loss: 0.5440 - acc: 0.730 - ETA: 3:07 - loss: 0.5447 - acc: 0.730 - ETA: 3:03 - loss: 0.5452 - acc: 0.729 - ETA: 2:58 - loss: 0.5447 - acc: 0.729 - ETA: 2:53 - loss: 0.5444 - acc: 0.729 - ETA: 2:48 - loss: 0.5441 - acc: 0.728 - ETA: 2:43 - loss: 0.5440 - acc: 0.728 - ETA: 2:39 - loss: 0.5433 - acc: 0.729 - ETA: 2:34 - loss: 0.5432 - acc: 0.729 - ETA: 2:29 - loss: 0.5425 - acc: 0.729 - ETA: 2:24 - loss: 0.5424 - acc: 0.730 - ETA: 2:19 - loss: 0.5417 - acc: 0.729 - ETA: 2:14 - loss: 0.5410 - acc: 0.730 - ETA: 2:10 - loss: 0.5413 - acc: 0.730 - ETA: 2:05 - loss: 0.5420 - acc: 0.729 - ETA: 2:00 - loss: 0.5414 - acc: 0.730 - ETA: 1:55 - loss: 0.5409 - acc: 0.731 - ETA: 1:50 - loss: 0.5409 - acc: 0.731 - ETA: 1:46 - loss: 0.5415 - acc: 0.731 - ETA: 1:39 - loss: 0.5411 - acc: 0.733 - ETA: 1:34 - loss: 0.5402 - acc: 0.733 - ETA: 1:30 - loss: 0.5406 - acc: 0.733 - ETA: 1:25 - loss: 0.5415 - acc: 0.732 - ETA: 1:20 - loss: 0.5424 - acc: 0.731 - ETA: 1:16 - loss: 0.5426 - acc: 0.731 - ETA: 1:11 - loss: 0.5421 - acc: 0.731 - ETA: 1:06 - loss: 0.5421 - acc: 0.731 - ETA: 1:01 - loss: 0.5420 - acc: 0.731 - ETA: 57s - loss: 0.5423 - acc: 0.730 - ETA: 52s - loss: 0.5422 - acc: 0.73 - ETA: 47s - loss: 0.5420 - acc: 0.73 - ETA: 42s - loss: 0.5417 - acc: 0.73 - ETA: 38s - loss: 0.5414 - acc: 0.73 - ETA: 33s - loss: 0.5410 - acc: 0.73 - ETA: 28s - loss: 0.5409 - acc: 0.73 - ETA: 23s - loss: 0.5409 - acc: 0.73 - ETA: 19s - loss: 0.5416 - acc: 0.73 - ETA: 14s - loss: 0.5416 - acc: 0.73 - ETA: 9s - loss: 0.5416 - acc: 0.7308 - ETA: 4s - loss: 0.5410 - acc: 0.7311\n",
      "Epoch 00014: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.5404 - acc: 0.7318 - val_loss: 0.6116 - val_acc: 0.6635\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.5190 - acc: 0.739 - ETA: 6:05 - loss: 0.5248 - acc: 0.734 - ETA: 6:00 - loss: 0.5234 - acc: 0.739 - ETA: 5:56 - loss: 0.5244 - acc: 0.742 - ETA: 5:51 - loss: 0.5186 - acc: 0.741 - ETA: 5:46 - loss: 0.5231 - acc: 0.739 - ETA: 5:41 - loss: 0.5241 - acc: 0.741 - ETA: 5:36 - loss: 0.5214 - acc: 0.742 - ETA: 5:32 - loss: 0.5313 - acc: 0.739 - ETA: 5:27 - loss: 0.5340 - acc: 0.734 - ETA: 5:22 - loss: 0.5339 - acc: 0.734 - ETA: 5:17 - loss: 0.5347 - acc: 0.734 - ETA: 5:12 - loss: 0.5336 - acc: 0.734 - ETA: 5:08 - loss: 0.5346 - acc: 0.730 - ETA: 5:03 - loss: 0.5320 - acc: 0.734 - ETA: 4:58 - loss: 0.5332 - acc: 0.735 - ETA: 4:53 - loss: 0.5320 - acc: 0.737 - ETA: 4:48 - loss: 0.5340 - acc: 0.736 - ETA: 4:44 - loss: 0.5347 - acc: 0.736 - ETA: 4:39 - loss: 0.5353 - acc: 0.734 - ETA: 4:34 - loss: 0.5350 - acc: 0.733 - ETA: 4:29 - loss: 0.5378 - acc: 0.731 - ETA: 4:24 - loss: 0.5385 - acc: 0.731 - ETA: 4:20 - loss: 0.5386 - acc: 0.730 - ETA: 4:15 - loss: 0.5362 - acc: 0.732 - ETA: 4:10 - loss: 0.5363 - acc: 0.733 - ETA: 4:05 - loss: 0.5349 - acc: 0.734 - ETA: 4:00 - loss: 0.5341 - acc: 0.734 - ETA: 3:55 - loss: 0.5329 - acc: 0.737 - ETA: 3:51 - loss: 0.5347 - acc: 0.736 - ETA: 3:46 - loss: 0.5365 - acc: 0.734 - ETA: 3:41 - loss: 0.5363 - acc: 0.733 - ETA: 3:36 - loss: 0.5372 - acc: 0.732 - ETA: 3:32 - loss: 0.5377 - acc: 0.731 - ETA: 3:27 - loss: 0.5366 - acc: 0.732 - ETA: 3:22 - loss: 0.5365 - acc: 0.732 - ETA: 3:17 - loss: 0.5369 - acc: 0.731 - ETA: 3:12 - loss: 0.5373 - acc: 0.731 - ETA: 3:07 - loss: 0.5369 - acc: 0.732 - ETA: 3:03 - loss: 0.5370 - acc: 0.731 - ETA: 2:58 - loss: 0.5377 - acc: 0.731 - ETA: 2:53 - loss: 0.5378 - acc: 0.731 - ETA: 2:48 - loss: 0.5388 - acc: 0.730 - ETA: 2:43 - loss: 0.5388 - acc: 0.730 - ETA: 2:39 - loss: 0.5384 - acc: 0.729 - ETA: 2:34 - loss: 0.5375 - acc: 0.730 - ETA: 2:29 - loss: 0.5378 - acc: 0.730 - ETA: 2:24 - loss: 0.5373 - acc: 0.730 - ETA: 2:19 - loss: 0.5371 - acc: 0.730 - ETA: 2:14 - loss: 0.5375 - acc: 0.729 - ETA: 2:10 - loss: 0.5372 - acc: 0.729 - ETA: 2:05 - loss: 0.5362 - acc: 0.730 - ETA: 2:00 - loss: 0.5360 - acc: 0.730 - ETA: 1:55 - loss: 0.5370 - acc: 0.729 - ETA: 1:50 - loss: 0.5363 - acc: 0.730 - ETA: 1:46 - loss: 0.5370 - acc: 0.730 - ETA: 1:41 - loss: 0.5363 - acc: 0.731 - ETA: 1:36 - loss: 0.5361 - acc: 0.731 - ETA: 1:30 - loss: 0.5380 - acc: 0.730 - ETA: 1:25 - loss: 0.5375 - acc: 0.730 - ETA: 1:20 - loss: 0.5373 - acc: 0.731 - ETA: 1:16 - loss: 0.5374 - acc: 0.731 - ETA: 1:11 - loss: 0.5380 - acc: 0.730 - ETA: 1:06 - loss: 0.5379 - acc: 0.730 - ETA: 1:01 - loss: 0.5380 - acc: 0.730 - ETA: 57s - loss: 0.5381 - acc: 0.730 - ETA: 52s - loss: 0.5377 - acc: 0.73 - ETA: 47s - loss: 0.5379 - acc: 0.73 - ETA: 42s - loss: 0.5376 - acc: 0.73 - ETA: 38s - loss: 0.5374 - acc: 0.73 - ETA: 33s - loss: 0.5373 - acc: 0.73 - ETA: 28s - loss: 0.5373 - acc: 0.73 - ETA: 23s - loss: 0.5375 - acc: 0.73 - ETA: 19s - loss: 0.5379 - acc: 0.73 - ETA: 14s - loss: 0.5379 - acc: 0.73 - ETA: 9s - loss: 0.5384 - acc: 0.7312 - ETA: 4s - loss: 0.5379 - acc: 0.7316\n",
      "Epoch 00015: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.5386 - acc: 0.7309 - val_loss: 0.6122 - val_acc: 0.6679\n",
      "Epoch 16/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4906 - acc: 0.781 - ETA: 6:06 - loss: 0.5195 - acc: 0.739 - ETA: 6:01 - loss: 0.5149 - acc: 0.743 - ETA: 5:56 - loss: 0.5280 - acc: 0.734 - ETA: 5:51 - loss: 0.5244 - acc: 0.747 - ETA: 5:46 - loss: 0.5245 - acc: 0.748 - ETA: 5:41 - loss: 0.5237 - acc: 0.748 - ETA: 5:37 - loss: 0.5255 - acc: 0.744 - ETA: 5:32 - loss: 0.5330 - acc: 0.736 - ETA: 5:27 - loss: 0.5405 - acc: 0.731 - ETA: 5:22 - loss: 0.5431 - acc: 0.726 - ETA: 5:17 - loss: 0.5409 - acc: 0.725 - ETA: 5:13 - loss: 0.5407 - acc: 0.727 - ETA: 5:08 - loss: 0.5408 - acc: 0.728 - ETA: 5:03 - loss: 0.5402 - acc: 0.727 - ETA: 4:58 - loss: 0.5413 - acc: 0.725 - ETA: 4:53 - loss: 0.5394 - acc: 0.729 - ETA: 4:49 - loss: 0.5418 - acc: 0.728 - ETA: 4:44 - loss: 0.5410 - acc: 0.729 - ETA: 4:39 - loss: 0.5404 - acc: 0.730 - ETA: 4:34 - loss: 0.5406 - acc: 0.730 - ETA: 4:29 - loss: 0.5396 - acc: 0.733 - ETA: 4:24 - loss: 0.5383 - acc: 0.733 - ETA: 4:20 - loss: 0.5402 - acc: 0.731 - ETA: 4:15 - loss: 0.5377 - acc: 0.732 - ETA: 4:10 - loss: 0.5373 - acc: 0.733 - ETA: 4:05 - loss: 0.5383 - acc: 0.731 - ETA: 4:00 - loss: 0.5387 - acc: 0.731 - ETA: 3:56 - loss: 0.5387 - acc: 0.732 - ETA: 3:51 - loss: 0.5376 - acc: 0.733 - ETA: 3:46 - loss: 0.5383 - acc: 0.732 - ETA: 3:41 - loss: 0.5374 - acc: 0.732 - ETA: 3:36 - loss: 0.5384 - acc: 0.732 - ETA: 3:32 - loss: 0.5369 - acc: 0.732 - ETA: 3:27 - loss: 0.5370 - acc: 0.732 - ETA: 3:22 - loss: 0.5363 - acc: 0.732 - ETA: 3:17 - loss: 0.5357 - acc: 0.733 - ETA: 3:12 - loss: 0.5357 - acc: 0.733 - ETA: 3:07 - loss: 0.5347 - acc: 0.733 - ETA: 3:03 - loss: 0.5355 - acc: 0.732 - ETA: 2:58 - loss: 0.5359 - acc: 0.732 - ETA: 2:53 - loss: 0.5359 - acc: 0.732 - ETA: 2:48 - loss: 0.5353 - acc: 0.733 - ETA: 2:43 - loss: 0.5355 - acc: 0.733 - ETA: 2:38 - loss: 0.5350 - acc: 0.732 - ETA: 2:34 - loss: 0.5338 - acc: 0.733 - ETA: 2:29 - loss: 0.5337 - acc: 0.733 - ETA: 2:24 - loss: 0.5346 - acc: 0.733 - ETA: 2:19 - loss: 0.5333 - acc: 0.734 - ETA: 2:14 - loss: 0.5327 - acc: 0.735 - ETA: 2:10 - loss: 0.5326 - acc: 0.735 - ETA: 2:05 - loss: 0.5328 - acc: 0.735 - ETA: 2:00 - loss: 0.5328 - acc: 0.735 - ETA: 1:55 - loss: 0.5328 - acc: 0.735 - ETA: 1:50 - loss: 0.5332 - acc: 0.735 - ETA: 1:46 - loss: 0.5322 - acc: 0.735 - ETA: 1:41 - loss: 0.5319 - acc: 0.735 - ETA: 1:36 - loss: 0.5317 - acc: 0.736 - ETA: 1:31 - loss: 0.5317 - acc: 0.735 - ETA: 1:26 - loss: 0.5318 - acc: 0.735 - ETA: 1:21 - loss: 0.5321 - acc: 0.735 - ETA: 1:17 - loss: 0.5320 - acc: 0.735 - ETA: 1:12 - loss: 0.5322 - acc: 0.735 - ETA: 1:07 - loss: 0.5325 - acc: 0.735 - ETA: 1:02 - loss: 0.5326 - acc: 0.736 - ETA: 57s - loss: 0.5326 - acc: 0.736 - ETA: 53s - loss: 0.5327 - acc: 0.73 - ETA: 48s - loss: 0.5334 - acc: 0.73 - ETA: 43s - loss: 0.5333 - acc: 0.73 - ETA: 38s - loss: 0.5333 - acc: 0.73 - ETA: 33s - loss: 0.5330 - acc: 0.73 - ETA: 28s - loss: 0.5335 - acc: 0.73 - ETA: 24s - loss: 0.5337 - acc: 0.73 - ETA: 19s - loss: 0.5340 - acc: 0.73 - ETA: 14s - loss: 0.5338 - acc: 0.73 - ETA: 9s - loss: 0.5328 - acc: 0.7370 - ETA: 4s - loss: 0.5330 - acc: 0.7365\n",
      "Epoch 00016: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.5329 - acc: 0.7365 - val_loss: 0.6180 - val_acc: 0.6477\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.5203 - acc: 0.708 - ETA: 6:06 - loss: 0.5140 - acc: 0.739 - ETA: 6:01 - loss: 0.5122 - acc: 0.746 - ETA: 5:56 - loss: 0.5169 - acc: 0.744 - ETA: 5:51 - loss: 0.5211 - acc: 0.743 - ETA: 5:47 - loss: 0.5136 - acc: 0.753 - ETA: 5:42 - loss: 0.5113 - acc: 0.756 - ETA: 5:37 - loss: 0.5124 - acc: 0.751 - ETA: 5:32 - loss: 0.5148 - acc: 0.746 - ETA: 5:27 - loss: 0.5142 - acc: 0.744 - ETA: 5:22 - loss: 0.5133 - acc: 0.746 - ETA: 5:17 - loss: 0.5117 - acc: 0.749 - ETA: 5:13 - loss: 0.5139 - acc: 0.748 - ETA: 5:08 - loss: 0.5174 - acc: 0.742 - ETA: 5:03 - loss: 0.5162 - acc: 0.745 - ETA: 4:58 - loss: 0.5148 - acc: 0.746 - ETA: 4:53 - loss: 0.5149 - acc: 0.746 - ETA: 4:49 - loss: 0.5176 - acc: 0.744 - ETA: 4:44 - loss: 0.5184 - acc: 0.742 - ETA: 4:39 - loss: 0.5201 - acc: 0.740 - ETA: 4:34 - loss: 0.5215 - acc: 0.739 - ETA: 4:29 - loss: 0.5206 - acc: 0.741 - ETA: 4:24 - loss: 0.5212 - acc: 0.740 - ETA: 4:20 - loss: 0.5205 - acc: 0.741 - ETA: 4:15 - loss: 0.5199 - acc: 0.741 - ETA: 4:10 - loss: 0.5189 - acc: 0.742 - ETA: 4:05 - loss: 0.5195 - acc: 0.743 - ETA: 4:00 - loss: 0.5214 - acc: 0.741 - ETA: 3:56 - loss: 0.5232 - acc: 0.739 - ETA: 3:51 - loss: 0.5238 - acc: 0.739 - ETA: 3:46 - loss: 0.5232 - acc: 0.739 - ETA: 3:41 - loss: 0.5235 - acc: 0.740 - ETA: 3:36 - loss: 0.5232 - acc: 0.740 - ETA: 3:31 - loss: 0.5227 - acc: 0.741 - ETA: 3:27 - loss: 0.5225 - acc: 0.741 - ETA: 3:22 - loss: 0.5228 - acc: 0.741 - ETA: 3:17 - loss: 0.5220 - acc: 0.741 - ETA: 3:12 - loss: 0.5222 - acc: 0.741 - ETA: 3:07 - loss: 0.5233 - acc: 0.740 - ETA: 3:03 - loss: 0.5232 - acc: 0.740 - ETA: 2:58 - loss: 0.5248 - acc: 0.739 - ETA: 2:53 - loss: 0.5253 - acc: 0.738 - ETA: 2:48 - loss: 0.5258 - acc: 0.738 - ETA: 2:43 - loss: 0.5250 - acc: 0.739 - ETA: 2:38 - loss: 0.5260 - acc: 0.737 - ETA: 2:34 - loss: 0.5251 - acc: 0.738 - ETA: 2:29 - loss: 0.5249 - acc: 0.738 - ETA: 2:24 - loss: 0.5244 - acc: 0.739 - ETA: 2:19 - loss: 0.5249 - acc: 0.738 - ETA: 2:14 - loss: 0.5252 - acc: 0.738 - ETA: 2:10 - loss: 0.5253 - acc: 0.738 - ETA: 2:05 - loss: 0.5248 - acc: 0.738 - ETA: 2:00 - loss: 0.5251 - acc: 0.738 - ETA: 1:55 - loss: 0.5249 - acc: 0.738 - ETA: 1:50 - loss: 0.5243 - acc: 0.739 - ETA: 1:45 - loss: 0.5256 - acc: 0.737 - ETA: 1:41 - loss: 0.5258 - acc: 0.737 - ETA: 1:36 - loss: 0.5263 - acc: 0.738 - ETA: 1:31 - loss: 0.5262 - acc: 0.738 - ETA: 1:26 - loss: 0.5256 - acc: 0.738 - ETA: 1:21 - loss: 0.5250 - acc: 0.739 - ETA: 1:17 - loss: 0.5259 - acc: 0.739 - ETA: 1:12 - loss: 0.5259 - acc: 0.739 - ETA: 1:07 - loss: 0.5259 - acc: 0.738 - ETA: 1:02 - loss: 0.5261 - acc: 0.738 - ETA: 57s - loss: 0.5256 - acc: 0.739 - ETA: 52s - loss: 0.5251 - acc: 0.74 - ETA: 48s - loss: 0.5249 - acc: 0.74 - ETA: 43s - loss: 0.5249 - acc: 0.74 - ETA: 38s - loss: 0.5246 - acc: 0.74 - ETA: 33s - loss: 0.5244 - acc: 0.74 - ETA: 28s - loss: 0.5246 - acc: 0.74 - ETA: 24s - loss: 0.5247 - acc: 0.74 - ETA: 19s - loss: 0.5248 - acc: 0.74 - ETA: 14s - loss: 0.5242 - acc: 0.74 - ETA: 9s - loss: 0.5242 - acc: 0.7410 - ETA: 4s - loss: 0.5245 - acc: 0.7404\n",
      "Epoch 00017: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.5242 - acc: 0.7403 - val_loss: 0.6177 - val_acc: 0.6553\n",
      "Epoch 18/100\n",
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.5441 - acc: 0.770 - ETA: 6:05 - loss: 0.5260 - acc: 0.760 - ETA: 4:17 - loss: 0.5806 - acc: 0.729 - ETA: 4:39 - loss: 0.5575 - acc: 0.744 - ETA: 4:51 - loss: 0.5562 - acc: 0.750 - ETA: 4:57 - loss: 0.5460 - acc: 0.753 - ETA: 5:00 - loss: 0.5485 - acc: 0.744 - ETA: 5:00 - loss: 0.5448 - acc: 0.739 - ETA: 5:00 - loss: 0.5438 - acc: 0.738 - ETA: 4:59 - loss: 0.5419 - acc: 0.739 - ETA: 4:57 - loss: 0.5399 - acc: 0.741 - ETA: 4:55 - loss: 0.5420 - acc: 0.738 - ETA: 4:52 - loss: 0.5372 - acc: 0.742 - ETA: 4:49 - loss: 0.5337 - acc: 0.747 - ETA: 4:46 - loss: 0.5357 - acc: 0.747 - ETA: 4:42 - loss: 0.5378 - acc: 0.742 - ETA: 4:39 - loss: 0.5345 - acc: 0.743 - ETA: 4:35 - loss: 0.5339 - acc: 0.744 - ETA: 4:31 - loss: 0.5310 - acc: 0.746 - ETA: 4:27 - loss: 0.5286 - acc: 0.748 - ETA: 4:23 - loss: 0.5259 - acc: 0.749 - ETA: 4:19 - loss: 0.5256 - acc: 0.749 - ETA: 4:05 - loss: 0.5211 - acc: 0.753 - ETA: 4:01 - loss: 0.5215 - acc: 0.753 - ETA: 3:57 - loss: 0.5198 - acc: 0.753 - ETA: 3:54 - loss: 0.5186 - acc: 0.754 - ETA: 3:50 - loss: 0.5188 - acc: 0.753 - ETA: 3:46 - loss: 0.5209 - acc: 0.751 - ETA: 3:42 - loss: 0.5207 - acc: 0.752 - ETA: 3:38 - loss: 0.5211 - acc: 0.751 - ETA: 3:33 - loss: 0.5200 - acc: 0.752 - ETA: 3:29 - loss: 0.5198 - acc: 0.752 - ETA: 3:25 - loss: 0.5207 - acc: 0.750 - ETA: 3:21 - loss: 0.5205 - acc: 0.751 - ETA: 3:17 - loss: 0.5204 - acc: 0.750 - ETA: 3:12 - loss: 0.5220 - acc: 0.749 - ETA: 3:08 - loss: 0.5205 - acc: 0.750 - ETA: 3:04 - loss: 0.5193 - acc: 0.752 - ETA: 2:59 - loss: 0.5205 - acc: 0.751 - ETA: 2:55 - loss: 0.5195 - acc: 0.752 - ETA: 2:50 - loss: 0.5176 - acc: 0.754 - ETA: 2:46 - loss: 0.5175 - acc: 0.753 - ETA: 2:41 - loss: 0.5171 - acc: 0.753 - ETA: 2:37 - loss: 0.5167 - acc: 0.754 - ETA: 2:32 - loss: 0.5165 - acc: 0.754 - ETA: 2:28 - loss: 0.5171 - acc: 0.754 - ETA: 2:23 - loss: 0.5176 - acc: 0.753 - ETA: 2:19 - loss: 0.5182 - acc: 0.753 - ETA: 2:14 - loss: 0.5177 - acc: 0.753 - ETA: 2:10 - loss: 0.5183 - acc: 0.752 - ETA: 2:05 - loss: 0.5177 - acc: 0.752 - ETA: 2:01 - loss: 0.5183 - acc: 0.751 - ETA: 1:56 - loss: 0.5184 - acc: 0.751 - ETA: 1:52 - loss: 0.5191 - acc: 0.750 - ETA: 1:47 - loss: 0.5199 - acc: 0.749 - ETA: 1:42 - loss: 0.5202 - acc: 0.749 - ETA: 1:38 - loss: 0.5201 - acc: 0.748 - ETA: 1:33 - loss: 0.5201 - acc: 0.749 - ETA: 1:28 - loss: 0.5199 - acc: 0.749 - ETA: 1:24 - loss: 0.5197 - acc: 0.749 - ETA: 1:19 - loss: 0.5189 - acc: 0.749 - ETA: 1:15 - loss: 0.5184 - acc: 0.750 - ETA: 1:10 - loss: 0.5181 - acc: 0.750 - ETA: 1:05 - loss: 0.5178 - acc: 0.750 - ETA: 1:01 - loss: 0.5183 - acc: 0.750 - ETA: 56s - loss: 0.5179 - acc: 0.750 - ETA: 51s - loss: 0.5181 - acc: 0.75 - ETA: 47s - loss: 0.5186 - acc: 0.74 - ETA: 42s - loss: 0.5194 - acc: 0.74 - ETA: 37s - loss: 0.5194 - acc: 0.74 - ETA: 32s - loss: 0.5190 - acc: 0.74 - ETA: 28s - loss: 0.5190 - acc: 0.74 - ETA: 23s - loss: 0.5192 - acc: 0.74 - ETA: 18s - loss: 0.5197 - acc: 0.74 - ETA: 14s - loss: 0.5194 - acc: 0.74 - ETA: 9s - loss: 0.5196 - acc: 0.7492 - ETA: 4s - loss: 0.5186 - acc: 0.7500\n",
      "Epoch 00018: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.5184 - acc: 0.7503 - val_loss: 0.6163 - val_acc: 0.6553\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.4813 - acc: 0.781 - ETA: 6:06 - loss: 0.5019 - acc: 0.776 - ETA: 6:01 - loss: 0.4968 - acc: 0.767 - ETA: 5:57 - loss: 0.4910 - acc: 0.770 - ETA: 5:52 - loss: 0.4897 - acc: 0.775 - ETA: 5:47 - loss: 0.4924 - acc: 0.769 - ETA: 5:42 - loss: 0.4924 - acc: 0.767 - ETA: 5:38 - loss: 0.4985 - acc: 0.757 - ETA: 5:33 - loss: 0.5040 - acc: 0.750 - ETA: 5:28 - loss: 0.5043 - acc: 0.752 - ETA: 5:23 - loss: 0.5081 - acc: 0.750 - ETA: 5:18 - loss: 0.5094 - acc: 0.748 - ETA: 5:13 - loss: 0.5107 - acc: 0.749 - ETA: 5:08 - loss: 0.5127 - acc: 0.748 - ETA: 5:04 - loss: 0.5109 - acc: 0.749 - ETA: 4:59 - loss: 0.5136 - acc: 0.744 - ETA: 4:54 - loss: 0.5101 - acc: 0.748 - ETA: 4:49 - loss: 0.5109 - acc: 0.748 - ETA: 4:44 - loss: 0.5113 - acc: 0.748 - ETA: 4:39 - loss: 0.5123 - acc: 0.748 - ETA: 4:35 - loss: 0.5129 - acc: 0.750 - ETA: 4:30 - loss: 0.5121 - acc: 0.752 - ETA: 4:25 - loss: 0.5124 - acc: 0.752 - ETA: 4:20 - loss: 0.5124 - acc: 0.752 - ETA: 4:15 - loss: 0.5149 - acc: 0.750 - ETA: 4:10 - loss: 0.5124 - acc: 0.752 - ETA: 4:06 - loss: 0.5140 - acc: 0.750 - ETA: 4:01 - loss: 0.5131 - acc: 0.750 - ETA: 3:56 - loss: 0.5148 - acc: 0.748 - ETA: 3:51 - loss: 0.5149 - acc: 0.750 - ETA: 3:46 - loss: 0.5161 - acc: 0.750 - ETA: 3:41 - loss: 0.5165 - acc: 0.750 - ETA: 3:37 - loss: 0.5164 - acc: 0.751 - ETA: 3:32 - loss: 0.5171 - acc: 0.750 - ETA: 3:27 - loss: 0.5166 - acc: 0.750 - ETA: 3:22 - loss: 0.5165 - acc: 0.750 - ETA: 3:17 - loss: 0.5153 - acc: 0.750 - ETA: 3:13 - loss: 0.5155 - acc: 0.749 - ETA: 3:08 - loss: 0.5153 - acc: 0.750 - ETA: 3:03 - loss: 0.5157 - acc: 0.749 - ETA: 2:58 - loss: 0.5165 - acc: 0.749 - ETA: 2:53 - loss: 0.5166 - acc: 0.748 - ETA: 2:48 - loss: 0.5172 - acc: 0.748 - ETA: 2:44 - loss: 0.5173 - acc: 0.748 - ETA: 2:39 - loss: 0.5173 - acc: 0.748 - ETA: 2:34 - loss: 0.5175 - acc: 0.748 - ETA: 2:29 - loss: 0.5192 - acc: 0.746 - ETA: 2:24 - loss: 0.5194 - acc: 0.745 - ETA: 2:19 - loss: 0.5194 - acc: 0.746 - ETA: 2:15 - loss: 0.5198 - acc: 0.745 - ETA: 2:10 - loss: 0.5198 - acc: 0.745 - ETA: 2:05 - loss: 0.5210 - acc: 0.744 - ETA: 2:00 - loss: 0.5210 - acc: 0.745 - ETA: 1:55 - loss: 0.5214 - acc: 0.744 - ETA: 1:50 - loss: 0.5210 - acc: 0.744 - ETA: 1:46 - loss: 0.5216 - acc: 0.745 - ETA: 1:41 - loss: 0.5215 - acc: 0.745 - ETA: 1:36 - loss: 0.5218 - acc: 0.744 - ETA: 1:31 - loss: 0.5216 - acc: 0.744 - ETA: 1:26 - loss: 0.5219 - acc: 0.744 - ETA: 1:22 - loss: 0.5218 - acc: 0.745 - ETA: 1:17 - loss: 0.5219 - acc: 0.744 - ETA: 1:11 - loss: 0.5251 - acc: 0.742 - ETA: 1:06 - loss: 0.5251 - acc: 0.741 - ETA: 1:01 - loss: 0.5251 - acc: 0.742 - ETA: 57s - loss: 0.5255 - acc: 0.742 - ETA: 52s - loss: 0.5258 - acc: 0.74 - ETA: 47s - loss: 0.5250 - acc: 0.74 - ETA: 42s - loss: 0.5252 - acc: 0.74 - ETA: 38s - loss: 0.5241 - acc: 0.74 - ETA: 33s - loss: 0.5243 - acc: 0.74 - ETA: 28s - loss: 0.5242 - acc: 0.74 - ETA: 23s - loss: 0.5240 - acc: 0.74 - ETA: 19s - loss: 0.5232 - acc: 0.74 - ETA: 14s - loss: 0.5232 - acc: 0.74 - ETA: 9s - loss: 0.5226 - acc: 0.7442 - ETA: 4s - loss: 0.5220 - acc: 0.7450\n",
      "Epoch 00019: val_loss did not improve\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.5228 - acc: 0.7445 - val_loss: 0.6153 - val_acc: 0.6550\n",
      "Epoch 20/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.5495 - acc: 0.760 - ETA: 6:06 - loss: 0.5594 - acc: 0.724 - ETA: 6:01 - loss: 0.5368 - acc: 0.739 - ETA: 5:56 - loss: 0.5330 - acc: 0.739 - ETA: 5:51 - loss: 0.5282 - acc: 0.739 - ETA: 5:46 - loss: 0.5211 - acc: 0.750 - ETA: 5:41 - loss: 0.5220 - acc: 0.747 - ETA: 5:37 - loss: 0.5251 - acc: 0.740 - ETA: 5:32 - loss: 0.5202 - acc: 0.745 - ETA: 5:27 - loss: 0.5190 - acc: 0.746 - ETA: 5:22 - loss: 0.5189 - acc: 0.745 - ETA: 5:17 - loss: 0.5192 - acc: 0.743 - ETA: 5:13 - loss: 0.5148 - acc: 0.749 - ETA: 5:08 - loss: 0.5150 - acc: 0.747 - ETA: 5:03 - loss: 0.5171 - acc: 0.745 - ETA: 4:58 - loss: 0.5217 - acc: 0.739 - ETA: 4:54 - loss: 0.5219 - acc: 0.739 - ETA: 4:49 - loss: 0.5206 - acc: 0.739 - ETA: 4:44 - loss: 0.5174 - acc: 0.742 - ETA: 4:39 - loss: 0.5164 - acc: 0.743 - ETA: 4:34 - loss: 0.5178 - acc: 0.742 - ETA: 4:30 - loss: 0.5188 - acc: 0.742 - ETA: 4:25 - loss: 0.5192 - acc: 0.742 - ETA: 4:20 - loss: 0.5175 - acc: 0.744 - ETA: 4:15 - loss: 0.5181 - acc: 0.743 - ETA: 4:10 - loss: 0.5188 - acc: 0.744 - ETA: 4:05 - loss: 0.5196 - acc: 0.743 - ETA: 4:01 - loss: 0.5188 - acc: 0.745 - ETA: 3:56 - loss: 0.5172 - acc: 0.746 - ETA: 3:51 - loss: 0.5171 - acc: 0.747 - ETA: 3:46 - loss: 0.5169 - acc: 0.747 - ETA: 3:41 - loss: 0.5155 - acc: 0.748 - ETA: 3:37 - loss: 0.5141 - acc: 0.750 - ETA: 3:32 - loss: 0.5163 - acc: 0.748 - ETA: 3:27 - loss: 0.5162 - acc: 0.748 - ETA: 3:22 - loss: 0.5149 - acc: 0.750 - ETA: 3:17 - loss: 0.5155 - acc: 0.749 - ETA: 3:12 - loss: 0.5154 - acc: 0.750 - ETA: 3:08 - loss: 0.5158 - acc: 0.749 - ETA: 3:03 - loss: 0.5152 - acc: 0.750 - ETA: 2:58 - loss: 0.5158 - acc: 0.749 - ETA: 2:53 - loss: 0.5158 - acc: 0.750 - ETA: 2:48 - loss: 0.5153 - acc: 0.750 - ETA: 2:43 - loss: 0.5151 - acc: 0.750 - ETA: 2:39 - loss: 0.5147 - acc: 0.751 - ETA: 2:34 - loss: 0.5146 - acc: 0.751 - ETA: 2:29 - loss: 0.5139 - acc: 0.752 - ETA: 2:24 - loss: 0.5129 - acc: 0.752 - ETA: 2:19 - loss: 0.5127 - acc: 0.753 - ETA: 2:15 - loss: 0.5119 - acc: 0.754 - ETA: 2:10 - loss: 0.5125 - acc: 0.752 - ETA: 2:05 - loss: 0.5110 - acc: 0.754 - ETA: 2:00 - loss: 0.5104 - acc: 0.754 - ETA: 1:55 - loss: 0.5100 - acc: 0.754 - ETA: 1:50 - loss: 0.5096 - acc: 0.754 - ETA: 1:46 - loss: 0.5111 - acc: 0.753 - ETA: 1:41 - loss: 0.5115 - acc: 0.752 - ETA: 1:36 - loss: 0.5114 - acc: 0.752 - ETA: 1:31 - loss: 0.5108 - acc: 0.752 - ETA: 1:26 - loss: 0.5104 - acc: 0.753 - ETA: 1:22 - loss: 0.5101 - acc: 0.753 - ETA: 1:17 - loss: 0.5110 - acc: 0.752 - ETA: 1:12 - loss: 0.5111 - acc: 0.752 - ETA: 1:07 - loss: 0.5115 - acc: 0.751 - ETA: 1:02 - loss: 0.5120 - acc: 0.751 - ETA: 57s - loss: 0.5116 - acc: 0.751 - ETA: 53s - loss: 0.5116 - acc: 0.75 - ETA: 48s - loss: 0.5117 - acc: 0.75 - ETA: 43s - loss: 0.5126 - acc: 0.75 - ETA: 38s - loss: 0.5118 - acc: 0.75 - ETA: 33s - loss: 0.5114 - acc: 0.75 - ETA: 28s - loss: 0.5115 - acc: 0.75 - ETA: 24s - loss: 0.5108 - acc: 0.75 - ETA: 19s - loss: 0.5103 - acc: 0.75 - ETA: 14s - loss: 0.5107 - acc: 0.75 - ETA: 9s - loss: 0.5106 - acc: 0.7526 - ETA: 4s - loss: 0.5112 - acc: 0.7520\n",
      "Epoch 00020: val_loss did not improve\n",
      "78/78 [==============================] - 465s 6s/step - loss: 0.5110 - acc: 0.7521 - val_loss: 0.6118 - val_acc: 0.6622\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.4917 - acc: 0.781 - ETA: 6:06 - loss: 0.5486 - acc: 0.739 - ETA: 6:01 - loss: 0.5375 - acc: 0.732 - ETA: 5:56 - loss: 0.5321 - acc: 0.744 - ETA: 5:52 - loss: 0.5370 - acc: 0.743 - ETA: 5:47 - loss: 0.5324 - acc: 0.746 - ETA: 5:42 - loss: 0.5278 - acc: 0.747 - ETA: 5:37 - loss: 0.5229 - acc: 0.750 - ETA: 5:32 - loss: 0.5282 - acc: 0.746 - ETA: 5:28 - loss: 0.5209 - acc: 0.753 - ETA: 5:23 - loss: 0.5178 - acc: 0.756 - ETA: 5:18 - loss: 0.5141 - acc: 0.758 - ETA: 5:13 - loss: 0.5141 - acc: 0.760 - ETA: 5:08 - loss: 0.5157 - acc: 0.761 - ETA: 5:03 - loss: 0.5151 - acc: 0.762 - ETA: 4:59 - loss: 0.5141 - acc: 0.761 - ETA: 4:54 - loss: 0.5126 - acc: 0.762 - ETA: 4:49 - loss: 0.5105 - acc: 0.764 - ETA: 4:31 - loss: 0.5153 - acc: 0.759 - ETA: 4:27 - loss: 0.5151 - acc: 0.757 - ETA: 4:23 - loss: 0.5145 - acc: 0.757 - ETA: 4:19 - loss: 0.5140 - acc: 0.759 - ETA: 4:15 - loss: 0.5150 - acc: 0.757 - ETA: 4:11 - loss: 0.5126 - acc: 0.760 - ETA: 4:06 - loss: 0.5127 - acc: 0.759 - ETA: 4:02 - loss: 0.5124 - acc: 0.758 - ETA: 3:58 - loss: 0.5123 - acc: 0.758 - ETA: 3:53 - loss: 0.5128 - acc: 0.758 - ETA: 3:49 - loss: 0.5133 - acc: 0.758 - ETA: 3:44 - loss: 0.5154 - acc: 0.756 - ETA: 3:40 - loss: 0.5131 - acc: 0.758 - ETA: 3:35 - loss: 0.5127 - acc: 0.758 - ETA: 3:31 - loss: 0.5116 - acc: 0.760 - ETA: 3:26 - loss: 0.5119 - acc: 0.758 - ETA: 3:22 - loss: 0.5105 - acc: 0.760 - ETA: 3:17 - loss: 0.5110 - acc: 0.759 - ETA: 3:13 - loss: 0.5115 - acc: 0.759 - ETA: 3:08 - loss: 0.5111 - acc: 0.759 - ETA: 3:03 - loss: 0.5091 - acc: 0.761 - ETA: 2:59 - loss: 0.5077 - acc: 0.762 - ETA: 2:54 - loss: 0.5054 - acc: 0.764 - ETA: 2:50 - loss: 0.5055 - acc: 0.764 - ETA: 2:45 - loss: 0.5055 - acc: 0.765 - ETA: 2:40 - loss: 0.5056 - acc: 0.764 - ETA: 2:36 - loss: 0.5056 - acc: 0.764 - ETA: 2:31 - loss: 0.5043 - acc: 0.764 - ETA: 2:26 - loss: 0.5040 - acc: 0.764 - ETA: 2:22 - loss: 0.5046 - acc: 0.763 - ETA: 2:17 - loss: 0.5035 - acc: 0.765 - ETA: 2:12 - loss: 0.5035 - acc: 0.764 - ETA: 2:08 - loss: 0.5036 - acc: 0.765 - ETA: 2:03 - loss: 0.5029 - acc: 0.765 - ETA: 1:58 - loss: 0.5031 - acc: 0.765 - ETA: 1:53 - loss: 0.5022 - acc: 0.766 - ETA: 1:49 - loss: 0.5015 - acc: 0.766 - ETA: 1:44 - loss: 0.5012 - acc: 0.766 - ETA: 1:39 - loss: 0.5016 - acc: 0.765 - ETA: 1:35 - loss: 0.5028 - acc: 0.763 - ETA: 1:30 - loss: 0.5029 - acc: 0.763 - ETA: 1:25 - loss: 0.5033 - acc: 0.762 - ETA: 1:20 - loss: 0.5025 - acc: 0.763 - ETA: 1:16 - loss: 0.5020 - acc: 0.763 - ETA: 1:11 - loss: 0.5018 - acc: 0.763 - ETA: 1:06 - loss: 0.5023 - acc: 0.762 - ETA: 1:01 - loss: 0.5024 - acc: 0.762 - ETA: 57s - loss: 0.5025 - acc: 0.762 - ETA: 52s - loss: 0.5026 - acc: 0.76 - ETA: 47s - loss: 0.5027 - acc: 0.76 - ETA: 42s - loss: 0.5026 - acc: 0.76 - ETA: 38s - loss: 0.5033 - acc: 0.76 - ETA: 33s - loss: 0.5032 - acc: 0.76 - ETA: 28s - loss: 0.5031 - acc: 0.76 - ETA: 23s - loss: 0.5034 - acc: 0.76 - ETA: 19s - loss: 0.5038 - acc: 0.76 - ETA: 14s - loss: 0.5036 - acc: 0.76 - ETA: 9s - loss: 0.5034 - acc: 0.7607 - ETA: 4s - loss: 0.5033 - acc: 0.7610\n",
      "Epoch 00021: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.5032 - acc: 0.7612 - val_loss: 0.6235 - val_acc: 0.6484\n",
      "Epoch 22/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.5164 - acc: 0.750 - ETA: 6:06 - loss: 0.5100 - acc: 0.744 - ETA: 4:17 - loss: 0.4948 - acc: 0.746 - ETA: 4:39 - loss: 0.5013 - acc: 0.747 - ETA: 4:51 - loss: 0.4966 - acc: 0.747 - ETA: 4:57 - loss: 0.4978 - acc: 0.751 - ETA: 5:00 - loss: 0.5020 - acc: 0.748 - ETA: 5:01 - loss: 0.4946 - acc: 0.753 - ETA: 5:00 - loss: 0.4947 - acc: 0.753 - ETA: 4:59 - loss: 0.4971 - acc: 0.754 - ETA: 4:57 - loss: 0.4988 - acc: 0.750 - ETA: 4:55 - loss: 0.4927 - acc: 0.755 - ETA: 4:52 - loss: 0.4942 - acc: 0.753 - ETA: 4:49 - loss: 0.4913 - acc: 0.755 - ETA: 4:46 - loss: 0.4943 - acc: 0.754 - ETA: 4:42 - loss: 0.4945 - acc: 0.752 - ETA: 4:39 - loss: 0.4959 - acc: 0.752 - ETA: 4:35 - loss: 0.4983 - acc: 0.751 - ETA: 4:31 - loss: 0.4987 - acc: 0.751 - ETA: 4:27 - loss: 0.5004 - acc: 0.750 - ETA: 4:23 - loss: 0.5007 - acc: 0.752 - ETA: 4:19 - loss: 0.4999 - acc: 0.754 - ETA: 4:15 - loss: 0.5013 - acc: 0.753 - ETA: 4:11 - loss: 0.5004 - acc: 0.752 - ETA: 4:06 - loss: 0.4997 - acc: 0.753 - ETA: 4:02 - loss: 0.4994 - acc: 0.755 - ETA: 3:58 - loss: 0.4991 - acc: 0.754 - ETA: 3:53 - loss: 0.4984 - acc: 0.755 - ETA: 3:49 - loss: 0.4996 - acc: 0.754 - ETA: 3:44 - loss: 0.5009 - acc: 0.753 - ETA: 3:40 - loss: 0.5035 - acc: 0.752 - ETA: 3:35 - loss: 0.5034 - acc: 0.751 - ETA: 3:31 - loss: 0.5033 - acc: 0.751 - ETA: 3:26 - loss: 0.5022 - acc: 0.753 - ETA: 3:22 - loss: 0.5015 - acc: 0.754 - ETA: 3:17 - loss: 0.5000 - acc: 0.756 - ETA: 3:13 - loss: 0.4995 - acc: 0.758 - ETA: 3:08 - loss: 0.5004 - acc: 0.757 - ETA: 3:03 - loss: 0.5011 - acc: 0.757 - ETA: 2:59 - loss: 0.4998 - acc: 0.758 - ETA: 2:54 - loss: 0.4999 - acc: 0.759 - ETA: 2:50 - loss: 0.4994 - acc: 0.760 - ETA: 2:42 - loss: 0.5022 - acc: 0.756 - ETA: 2:37 - loss: 0.5027 - acc: 0.755 - ETA: 2:33 - loss: 0.5012 - acc: 0.757 - ETA: 2:28 - loss: 0.5006 - acc: 0.757 - ETA: 2:24 - loss: 0.5007 - acc: 0.756 - ETA: 2:19 - loss: 0.5005 - acc: 0.757 - ETA: 2:14 - loss: 0.5004 - acc: 0.757 - ETA: 2:10 - loss: 0.5005 - acc: 0.757 - ETA: 2:05 - loss: 0.4998 - acc: 0.758 - ETA: 2:01 - loss: 0.5006 - acc: 0.758 - ETA: 1:56 - loss: 0.5001 - acc: 0.758 - ETA: 1:52 - loss: 0.5001 - acc: 0.758 - ETA: 1:47 - loss: 0.4999 - acc: 0.758 - ETA: 1:42 - loss: 0.4990 - acc: 0.759 - ETA: 1:38 - loss: 0.4982 - acc: 0.760 - ETA: 1:33 - loss: 0.4987 - acc: 0.760 - ETA: 1:28 - loss: 0.4992 - acc: 0.759 - ETA: 1:24 - loss: 0.4993 - acc: 0.758 - ETA: 1:19 - loss: 0.5016 - acc: 0.756 - ETA: 1:15 - loss: 0.5011 - acc: 0.757 - ETA: 1:10 - loss: 0.5005 - acc: 0.757 - ETA: 1:05 - loss: 0.5011 - acc: 0.757 - ETA: 1:01 - loss: 0.5006 - acc: 0.757 - ETA: 56s - loss: 0.5007 - acc: 0.757 - ETA: 51s - loss: 0.5006 - acc: 0.75 - ETA: 47s - loss: 0.5002 - acc: 0.75 - ETA: 42s - loss: 0.5004 - acc: 0.75 - ETA: 37s - loss: 0.5010 - acc: 0.75 - ETA: 32s - loss: 0.5009 - acc: 0.75 - ETA: 28s - loss: 0.5008 - acc: 0.75 - ETA: 23s - loss: 0.5007 - acc: 0.75 - ETA: 18s - loss: 0.5012 - acc: 0.75 - ETA: 14s - loss: 0.5009 - acc: 0.75 - ETA: 9s - loss: 0.5002 - acc: 0.7590 - ETA: 4s - loss: 0.5000 - acc: 0.7592\n",
      "Epoch 00022: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.4999 - acc: 0.7591 - val_loss: 0.6164 - val_acc: 0.6572\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4777 - acc: 0.781 - ETA: 6:06 - loss: 0.5186 - acc: 0.739 - ETA: 6:01 - loss: 0.5174 - acc: 0.736 - ETA: 5:56 - loss: 0.5139 - acc: 0.744 - ETA: 5:51 - loss: 0.4995 - acc: 0.758 - ETA: 5:47 - loss: 0.5039 - acc: 0.756 - ETA: 5:42 - loss: 0.4971 - acc: 0.767 - ETA: 5:37 - loss: 0.5007 - acc: 0.761 - ETA: 5:32 - loss: 0.4964 - acc: 0.763 - ETA: 5:27 - loss: 0.4936 - acc: 0.768 - ETA: 5:23 - loss: 0.4907 - acc: 0.768 - ETA: 5:18 - loss: 0.4914 - acc: 0.769 - ETA: 5:13 - loss: 0.4913 - acc: 0.768 - ETA: 5:08 - loss: 0.4929 - acc: 0.766 - ETA: 5:03 - loss: 0.4947 - acc: 0.765 - ETA: 4:58 - loss: 0.4969 - acc: 0.765 - ETA: 4:54 - loss: 0.4995 - acc: 0.764 - ETA: 4:49 - loss: 0.4997 - acc: 0.763 - ETA: 4:44 - loss: 0.4978 - acc: 0.764 - ETA: 4:39 - loss: 0.4960 - acc: 0.766 - ETA: 4:34 - loss: 0.4944 - acc: 0.768 - ETA: 4:30 - loss: 0.4940 - acc: 0.768 - ETA: 4:25 - loss: 0.4966 - acc: 0.764 - ETA: 4:20 - loss: 0.4980 - acc: 0.763 - ETA: 4:15 - loss: 0.4979 - acc: 0.765 - ETA: 4:10 - loss: 0.4993 - acc: 0.763 - ETA: 4:05 - loss: 0.4998 - acc: 0.763 - ETA: 4:01 - loss: 0.5006 - acc: 0.762 - ETA: 3:56 - loss: 0.4997 - acc: 0.764 - ETA: 3:51 - loss: 0.5009 - acc: 0.763 - ETA: 3:46 - loss: 0.5010 - acc: 0.763 - ETA: 3:41 - loss: 0.5005 - acc: 0.763 - ETA: 3:37 - loss: 0.4995 - acc: 0.764 - ETA: 3:32 - loss: 0.4996 - acc: 0.763 - ETA: 3:27 - loss: 0.5009 - acc: 0.761 - ETA: 3:22 - loss: 0.5004 - acc: 0.761 - ETA: 3:17 - loss: 0.5010 - acc: 0.761 - ETA: 3:12 - loss: 0.5016 - acc: 0.761 - ETA: 3:08 - loss: 0.5014 - acc: 0.762 - ETA: 3:03 - loss: 0.5002 - acc: 0.763 - ETA: 2:58 - loss: 0.4999 - acc: 0.763 - ETA: 2:53 - loss: 0.4993 - acc: 0.763 - ETA: 2:48 - loss: 0.4989 - acc: 0.765 - ETA: 2:43 - loss: 0.4987 - acc: 0.765 - ETA: 2:39 - loss: 0.4991 - acc: 0.764 - ETA: 2:34 - loss: 0.4990 - acc: 0.764 - ETA: 2:29 - loss: 0.4995 - acc: 0.764 - ETA: 2:24 - loss: 0.4991 - acc: 0.764 - ETA: 2:19 - loss: 0.4983 - acc: 0.765 - ETA: 2:15 - loss: 0.4989 - acc: 0.765 - ETA: 2:10 - loss: 0.4988 - acc: 0.765 - ETA: 2:05 - loss: 0.4974 - acc: 0.767 - ETA: 2:00 - loss: 0.4967 - acc: 0.768 - ETA: 1:55 - loss: 0.4976 - acc: 0.767 - ETA: 1:50 - loss: 0.4971 - acc: 0.768 - ETA: 1:46 - loss: 0.4970 - acc: 0.768 - ETA: 1:41 - loss: 0.4967 - acc: 0.768 - ETA: 1:36 - loss: 0.4965 - acc: 0.769 - ETA: 1:31 - loss: 0.4967 - acc: 0.768 - ETA: 1:26 - loss: 0.4961 - acc: 0.769 - ETA: 1:21 - loss: 0.4964 - acc: 0.768 - ETA: 1:17 - loss: 0.4968 - acc: 0.768 - ETA: 1:12 - loss: 0.4968 - acc: 0.767 - ETA: 1:07 - loss: 0.4973 - acc: 0.766 - ETA: 1:02 - loss: 0.4971 - acc: 0.767 - ETA: 57s - loss: 0.4972 - acc: 0.766 - ETA: 53s - loss: 0.4969 - acc: 0.76 - ETA: 48s - loss: 0.4961 - acc: 0.76 - ETA: 43s - loss: 0.4950 - acc: 0.76 - ETA: 38s - loss: 0.4943 - acc: 0.76 - ETA: 33s - loss: 0.4943 - acc: 0.76 - ETA: 28s - loss: 0.4946 - acc: 0.76 - ETA: 24s - loss: 0.4949 - acc: 0.76 - ETA: 19s - loss: 0.4955 - acc: 0.76 - ETA: 14s - loss: 0.4953 - acc: 0.76 - ETA: 9s - loss: 0.4949 - acc: 0.7688 - ETA: 4s - loss: 0.4938 - acc: 0.7696\n",
      "Epoch 00023: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.4939 - acc: 0.7695 - val_loss: 0.6124 - val_acc: 0.6629\n",
      "Epoch 24/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.5018 - acc: 0.750 - ETA: 6:06 - loss: 0.4985 - acc: 0.739 - ETA: 6:01 - loss: 0.4866 - acc: 0.753 - ETA: 5:56 - loss: 0.4957 - acc: 0.747 - ETA: 5:51 - loss: 0.5006 - acc: 0.750 - ETA: 5:47 - loss: 0.4953 - acc: 0.755 - ETA: 5:42 - loss: 0.4860 - acc: 0.766 - ETA: 5:37 - loss: 0.4913 - acc: 0.763 - ETA: 5:32 - loss: 0.4884 - acc: 0.769 - ETA: 5:27 - loss: 0.4887 - acc: 0.767 - ETA: 5:23 - loss: 0.4894 - acc: 0.768 - ETA: 5:18 - loss: 0.4902 - acc: 0.767 - ETA: 5:13 - loss: 0.4902 - acc: 0.766 - ETA: 5:08 - loss: 0.4880 - acc: 0.768 - ETA: 5:03 - loss: 0.4872 - acc: 0.771 - ETA: 4:59 - loss: 0.4868 - acc: 0.770 - ETA: 4:54 - loss: 0.4867 - acc: 0.772 - ETA: 4:49 - loss: 0.4851 - acc: 0.774 - ETA: 4:44 - loss: 0.4861 - acc: 0.775 - ETA: 4:27 - loss: 0.4814 - acc: 0.782 - ETA: 4:23 - loss: 0.4796 - acc: 0.784 - ETA: 4:19 - loss: 0.4818 - acc: 0.782 - ETA: 4:15 - loss: 0.4795 - acc: 0.784 - ETA: 4:11 - loss: 0.4773 - acc: 0.786 - ETA: 4:06 - loss: 0.4798 - acc: 0.784 - ETA: 4:02 - loss: 0.4795 - acc: 0.785 - ETA: 3:58 - loss: 0.4786 - acc: 0.785 - ETA: 3:53 - loss: 0.4781 - acc: 0.786 - ETA: 3:49 - loss: 0.4786 - acc: 0.787 - ETA: 3:44 - loss: 0.4785 - acc: 0.787 - ETA: 3:40 - loss: 0.4783 - acc: 0.787 - ETA: 3:35 - loss: 0.4793 - acc: 0.785 - ETA: 3:31 - loss: 0.4781 - acc: 0.786 - ETA: 3:26 - loss: 0.4786 - acc: 0.785 - ETA: 3:22 - loss: 0.4783 - acc: 0.786 - ETA: 3:17 - loss: 0.4797 - acc: 0.785 - ETA: 3:13 - loss: 0.4790 - acc: 0.785 - ETA: 3:08 - loss: 0.4784 - acc: 0.785 - ETA: 3:03 - loss: 0.4788 - acc: 0.785 - ETA: 2:59 - loss: 0.4783 - acc: 0.785 - ETA: 2:54 - loss: 0.4776 - acc: 0.786 - ETA: 2:50 - loss: 0.4774 - acc: 0.786 - ETA: 2:45 - loss: 0.4770 - acc: 0.786 - ETA: 2:40 - loss: 0.4785 - acc: 0.786 - ETA: 2:36 - loss: 0.4807 - acc: 0.783 - ETA: 2:31 - loss: 0.4793 - acc: 0.784 - ETA: 2:26 - loss: 0.4789 - acc: 0.784 - ETA: 2:22 - loss: 0.4790 - acc: 0.784 - ETA: 2:17 - loss: 0.4799 - acc: 0.782 - ETA: 2:12 - loss: 0.4801 - acc: 0.781 - ETA: 2:08 - loss: 0.4805 - acc: 0.781 - ETA: 2:03 - loss: 0.4805 - acc: 0.781 - ETA: 1:58 - loss: 0.4804 - acc: 0.781 - ETA: 1:53 - loss: 0.4807 - acc: 0.782 - ETA: 1:49 - loss: 0.4809 - acc: 0.782 - ETA: 1:44 - loss: 0.4815 - acc: 0.781 - ETA: 1:39 - loss: 0.4806 - acc: 0.781 - ETA: 1:35 - loss: 0.4813 - acc: 0.781 - ETA: 1:30 - loss: 0.4819 - acc: 0.780 - ETA: 1:25 - loss: 0.4814 - acc: 0.781 - ETA: 1:20 - loss: 0.4812 - acc: 0.781 - ETA: 1:16 - loss: 0.4808 - acc: 0.781 - ETA: 1:11 - loss: 0.4802 - acc: 0.782 - ETA: 1:06 - loss: 0.4810 - acc: 0.781 - ETA: 1:01 - loss: 0.4825 - acc: 0.780 - ETA: 56s - loss: 0.4814 - acc: 0.782 - ETA: 51s - loss: 0.4814 - acc: 0.78 - ETA: 47s - loss: 0.4811 - acc: 0.78 - ETA: 42s - loss: 0.4812 - acc: 0.78 - ETA: 37s - loss: 0.4811 - acc: 0.78 - ETA: 32s - loss: 0.4814 - acc: 0.78 - ETA: 28s - loss: 0.4816 - acc: 0.78 - ETA: 23s - loss: 0.4821 - acc: 0.78 - ETA: 18s - loss: 0.4820 - acc: 0.78 - ETA: 14s - loss: 0.4823 - acc: 0.78 - ETA: 9s - loss: 0.4835 - acc: 0.7804 - ETA: 4s - loss: 0.4833 - acc: 0.7806\n",
      "Epoch 00024: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.4838 - acc: 0.7802 - val_loss: 0.6157 - val_acc: 0.6604\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4788 - acc: 0.802 - ETA: 6:06 - loss: 0.4749 - acc: 0.807 - ETA: 6:01 - loss: 0.4545 - acc: 0.809 - ETA: 5:57 - loss: 0.4671 - acc: 0.796 - ETA: 5:52 - loss: 0.4722 - acc: 0.789 - ETA: 5:47 - loss: 0.4744 - acc: 0.789 - ETA: 5:42 - loss: 0.4797 - acc: 0.784 - ETA: 5:37 - loss: 0.4785 - acc: 0.783 - ETA: 5:33 - loss: 0.4804 - acc: 0.783 - ETA: 5:28 - loss: 0.4799 - acc: 0.781 - ETA: 5:23 - loss: 0.4836 - acc: 0.779 - ETA: 5:18 - loss: 0.4832 - acc: 0.778 - ETA: 5:13 - loss: 0.4807 - acc: 0.781 - ETA: 5:08 - loss: 0.4822 - acc: 0.779 - ETA: 5:03 - loss: 0.4810 - acc: 0.780 - ETA: 4:59 - loss: 0.4814 - acc: 0.778 - ETA: 4:54 - loss: 0.4808 - acc: 0.780 - ETA: 4:49 - loss: 0.4790 - acc: 0.782 - ETA: 4:44 - loss: 0.4829 - acc: 0.779 - ETA: 4:39 - loss: 0.4832 - acc: 0.780 - ETA: 4:34 - loss: 0.4826 - acc: 0.781 - ETA: 4:30 - loss: 0.4813 - acc: 0.783 - ETA: 4:25 - loss: 0.4835 - acc: 0.780 - ETA: 4:20 - loss: 0.4834 - acc: 0.782 - ETA: 4:15 - loss: 0.4860 - acc: 0.779 - ETA: 4:10 - loss: 0.4849 - acc: 0.780 - ETA: 4:05 - loss: 0.4837 - acc: 0.782 - ETA: 4:01 - loss: 0.4827 - acc: 0.783 - ETA: 3:56 - loss: 0.4841 - acc: 0.782 - ETA: 3:51 - loss: 0.4861 - acc: 0.780 - ETA: 3:46 - loss: 0.4843 - acc: 0.782 - ETA: 3:41 - loss: 0.4866 - acc: 0.780 - ETA: 3:37 - loss: 0.4847 - acc: 0.782 - ETA: 3:32 - loss: 0.4867 - acc: 0.779 - ETA: 3:27 - loss: 0.4872 - acc: 0.779 - ETA: 3:22 - loss: 0.4859 - acc: 0.781 - ETA: 3:17 - loss: 0.4861 - acc: 0.780 - ETA: 3:12 - loss: 0.4864 - acc: 0.779 - ETA: 3:08 - loss: 0.4884 - acc: 0.777 - ETA: 3:03 - loss: 0.4888 - acc: 0.775 - ETA: 2:58 - loss: 0.4882 - acc: 0.775 - ETA: 2:53 - loss: 0.4876 - acc: 0.775 - ETA: 2:48 - loss: 0.4882 - acc: 0.775 - ETA: 2:44 - loss: 0.4878 - acc: 0.775 - ETA: 2:39 - loss: 0.4878 - acc: 0.775 - ETA: 2:34 - loss: 0.4872 - acc: 0.776 - ETA: 2:29 - loss: 0.4868 - acc: 0.777 - ETA: 2:24 - loss: 0.4875 - acc: 0.777 - ETA: 2:19 - loss: 0.4874 - acc: 0.776 - ETA: 2:15 - loss: 0.4865 - acc: 0.776 - ETA: 2:10 - loss: 0.4856 - acc: 0.777 - ETA: 2:05 - loss: 0.4857 - acc: 0.777 - ETA: 2:00 - loss: 0.4851 - acc: 0.776 - ETA: 1:55 - loss: 0.4847 - acc: 0.776 - ETA: 1:50 - loss: 0.4840 - acc: 0.777 - ETA: 1:46 - loss: 0.4847 - acc: 0.776 - ETA: 1:41 - loss: 0.4846 - acc: 0.777 - ETA: 1:36 - loss: 0.4848 - acc: 0.777 - ETA: 1:31 - loss: 0.4843 - acc: 0.777 - ETA: 1:26 - loss: 0.4836 - acc: 0.778 - ETA: 1:22 - loss: 0.4830 - acc: 0.778 - ETA: 1:17 - loss: 0.4840 - acc: 0.777 - ETA: 1:12 - loss: 0.4830 - acc: 0.777 - ETA: 1:07 - loss: 0.4827 - acc: 0.777 - ETA: 1:02 - loss: 0.4822 - acc: 0.778 - ETA: 57s - loss: 0.4813 - acc: 0.779 - ETA: 53s - loss: 0.4812 - acc: 0.77 - ETA: 48s - loss: 0.4816 - acc: 0.77 - ETA: 43s - loss: 0.4814 - acc: 0.77 - ETA: 38s - loss: 0.4809 - acc: 0.77 - ETA: 33s - loss: 0.4814 - acc: 0.77 - ETA: 28s - loss: 0.4815 - acc: 0.77 - ETA: 24s - loss: 0.4805 - acc: 0.77 - ETA: 19s - loss: 0.4799 - acc: 0.78 - ETA: 14s - loss: 0.4805 - acc: 0.77 - ETA: 9s - loss: 0.4810 - acc: 0.7788 - ETA: 4s - loss: 0.4810 - acc: 0.7790\n",
      "Epoch 00025: val_loss did not improve\n",
      "78/78 [==============================] - 465s 6s/step - loss: 0.4805 - acc: 0.7796 - val_loss: 0.6185 - val_acc: 0.6490\n",
      "Epoch 26/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4772 - acc: 0.791 - ETA: 6:05 - loss: 0.4859 - acc: 0.781 - ETA: 6:01 - loss: 0.4759 - acc: 0.784 - ETA: 5:56 - loss: 0.4884 - acc: 0.773 - ETA: 5:51 - loss: 0.4981 - acc: 0.766 - ETA: 5:47 - loss: 0.4987 - acc: 0.767 - ETA: 5:42 - loss: 0.5005 - acc: 0.764 - ETA: 5:37 - loss: 0.4940 - acc: 0.774 - ETA: 5:32 - loss: 0.4918 - acc: 0.775 - ETA: 5:27 - loss: 0.4937 - acc: 0.769 - ETA: 5:23 - loss: 0.4914 - acc: 0.769 - ETA: 5:18 - loss: 0.4893 - acc: 0.770 - ETA: 5:13 - loss: 0.4883 - acc: 0.771 - ETA: 5:08 - loss: 0.4853 - acc: 0.773 - ETA: 5:03 - loss: 0.4870 - acc: 0.772 - ETA: 4:59 - loss: 0.4860 - acc: 0.773 - ETA: 4:54 - loss: 0.4861 - acc: 0.774 - ETA: 4:49 - loss: 0.4873 - acc: 0.774 - ETA: 4:44 - loss: 0.4883 - acc: 0.773 - ETA: 4:39 - loss: 0.4872 - acc: 0.774 - ETA: 4:34 - loss: 0.4900 - acc: 0.771 - ETA: 4:19 - loss: 0.4859 - acc: 0.777 - ETA: 4:15 - loss: 0.4851 - acc: 0.779 - ETA: 4:11 - loss: 0.4866 - acc: 0.778 - ETA: 4:06 - loss: 0.4855 - acc: 0.778 - ETA: 4:02 - loss: 0.4851 - acc: 0.778 - ETA: 3:58 - loss: 0.4829 - acc: 0.780 - ETA: 3:53 - loss: 0.4815 - acc: 0.781 - ETA: 3:49 - loss: 0.4804 - acc: 0.783 - ETA: 3:44 - loss: 0.4789 - acc: 0.785 - ETA: 3:40 - loss: 0.4787 - acc: 0.783 - ETA: 3:35 - loss: 0.4809 - acc: 0.782 - ETA: 3:31 - loss: 0.4804 - acc: 0.782 - ETA: 3:26 - loss: 0.4798 - acc: 0.782 - ETA: 3:22 - loss: 0.4791 - acc: 0.783 - ETA: 3:17 - loss: 0.4792 - acc: 0.784 - ETA: 3:13 - loss: 0.4788 - acc: 0.784 - ETA: 3:08 - loss: 0.4786 - acc: 0.783 - ETA: 3:03 - loss: 0.4776 - acc: 0.784 - ETA: 2:59 - loss: 0.4785 - acc: 0.784 - ETA: 2:54 - loss: 0.4795 - acc: 0.783 - ETA: 2:50 - loss: 0.4792 - acc: 0.783 - ETA: 2:45 - loss: 0.4786 - acc: 0.783 - ETA: 2:40 - loss: 0.4773 - acc: 0.784 - ETA: 2:36 - loss: 0.4778 - acc: 0.783 - ETA: 2:31 - loss: 0.4776 - acc: 0.784 - ETA: 2:26 - loss: 0.4767 - acc: 0.785 - ETA: 2:22 - loss: 0.4773 - acc: 0.784 - ETA: 2:17 - loss: 0.4782 - acc: 0.784 - ETA: 2:12 - loss: 0.4783 - acc: 0.784 - ETA: 2:08 - loss: 0.4780 - acc: 0.784 - ETA: 2:03 - loss: 0.4782 - acc: 0.783 - ETA: 1:58 - loss: 0.4781 - acc: 0.783 - ETA: 1:53 - loss: 0.4777 - acc: 0.784 - ETA: 1:49 - loss: 0.4770 - acc: 0.785 - ETA: 1:44 - loss: 0.4768 - acc: 0.785 - ETA: 1:39 - loss: 0.4763 - acc: 0.785 - ETA: 1:35 - loss: 0.4763 - acc: 0.785 - ETA: 1:28 - loss: 0.4748 - acc: 0.788 - ETA: 1:24 - loss: 0.4760 - acc: 0.787 - ETA: 1:19 - loss: 0.4758 - acc: 0.787 - ETA: 1:15 - loss: 0.4749 - acc: 0.788 - ETA: 1:10 - loss: 0.4749 - acc: 0.788 - ETA: 1:05 - loss: 0.4752 - acc: 0.788 - ETA: 1:01 - loss: 0.4749 - acc: 0.788 - ETA: 56s - loss: 0.4746 - acc: 0.788 - ETA: 51s - loss: 0.4745 - acc: 0.78 - ETA: 47s - loss: 0.4742 - acc: 0.78 - ETA: 42s - loss: 0.4743 - acc: 0.78 - ETA: 37s - loss: 0.4757 - acc: 0.78 - ETA: 32s - loss: 0.4751 - acc: 0.78 - ETA: 28s - loss: 0.4749 - acc: 0.78 - ETA: 23s - loss: 0.4753 - acc: 0.78 - ETA: 18s - loss: 0.4758 - acc: 0.78 - ETA: 14s - loss: 0.4757 - acc: 0.78 - ETA: 9s - loss: 0.4759 - acc: 0.7863 - ETA: 4s - loss: 0.4759 - acc: 0.7861\n",
      "Epoch 00026: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.4754 - acc: 0.7865 - val_loss: 0.6313 - val_acc: 0.6553\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.5047 - acc: 0.750 - ETA: 6:06 - loss: 0.4905 - acc: 0.755 - ETA: 6:01 - loss: 0.4856 - acc: 0.767 - ETA: 5:56 - loss: 0.4692 - acc: 0.781 - ETA: 5:52 - loss: 0.4726 - acc: 0.783 - ETA: 5:47 - loss: 0.4689 - acc: 0.783 - ETA: 5:42 - loss: 0.4646 - acc: 0.787 - ETA: 5:37 - loss: 0.4608 - acc: 0.795 - ETA: 5:32 - loss: 0.4582 - acc: 0.798 - ETA: 5:28 - loss: 0.4591 - acc: 0.799 - ETA: 5:23 - loss: 0.4602 - acc: 0.798 - ETA: 5:18 - loss: 0.4607 - acc: 0.800 - ETA: 5:13 - loss: 0.4623 - acc: 0.794 - ETA: 5:08 - loss: 0.4629 - acc: 0.793 - ETA: 5:03 - loss: 0.4644 - acc: 0.793 - ETA: 4:59 - loss: 0.4667 - acc: 0.789 - ETA: 4:54 - loss: 0.4700 - acc: 0.787 - ETA: 4:49 - loss: 0.4676 - acc: 0.788 - ETA: 4:44 - loss: 0.4684 - acc: 0.788 - ETA: 4:39 - loss: 0.4706 - acc: 0.787 - ETA: 4:34 - loss: 0.4704 - acc: 0.787 - ETA: 4:30 - loss: 0.4733 - acc: 0.784 - ETA: 4:25 - loss: 0.4725 - acc: 0.785 - ETA: 4:20 - loss: 0.4708 - acc: 0.786 - ETA: 4:15 - loss: 0.4717 - acc: 0.785 - ETA: 4:10 - loss: 0.4708 - acc: 0.786 - ETA: 4:05 - loss: 0.4712 - acc: 0.784 - ETA: 4:01 - loss: 0.4703 - acc: 0.784 - ETA: 3:56 - loss: 0.4718 - acc: 0.784 - ETA: 3:51 - loss: 0.4726 - acc: 0.783 - ETA: 3:46 - loss: 0.4726 - acc: 0.781 - ETA: 3:35 - loss: 0.4741 - acc: 0.780 - ETA: 3:31 - loss: 0.4746 - acc: 0.780 - ETA: 3:26 - loss: 0.4748 - acc: 0.781 - ETA: 3:22 - loss: 0.4734 - acc: 0.782 - ETA: 3:17 - loss: 0.4728 - acc: 0.783 - ETA: 3:13 - loss: 0.4719 - acc: 0.783 - ETA: 3:08 - loss: 0.4743 - acc: 0.781 - ETA: 3:03 - loss: 0.4738 - acc: 0.781 - ETA: 2:59 - loss: 0.4725 - acc: 0.782 - ETA: 2:54 - loss: 0.4735 - acc: 0.781 - ETA: 2:50 - loss: 0.4725 - acc: 0.782 - ETA: 2:45 - loss: 0.4736 - acc: 0.779 - ETA: 2:40 - loss: 0.4736 - acc: 0.779 - ETA: 2:36 - loss: 0.4750 - acc: 0.778 - ETA: 2:31 - loss: 0.4751 - acc: 0.779 - ETA: 2:26 - loss: 0.4758 - acc: 0.778 - ETA: 2:22 - loss: 0.4759 - acc: 0.778 - ETA: 2:17 - loss: 0.4762 - acc: 0.778 - ETA: 2:12 - loss: 0.4764 - acc: 0.778 - ETA: 2:08 - loss: 0.4763 - acc: 0.778 - ETA: 2:03 - loss: 0.4764 - acc: 0.778 - ETA: 1:58 - loss: 0.4756 - acc: 0.779 - ETA: 1:53 - loss: 0.4764 - acc: 0.778 - ETA: 1:49 - loss: 0.4760 - acc: 0.778 - ETA: 1:44 - loss: 0.4769 - acc: 0.777 - ETA: 1:39 - loss: 0.4774 - acc: 0.776 - ETA: 1:35 - loss: 0.4777 - acc: 0.776 - ETA: 1:30 - loss: 0.4779 - acc: 0.776 - ETA: 1:25 - loss: 0.4777 - acc: 0.776 - ETA: 1:20 - loss: 0.4782 - acc: 0.775 - ETA: 1:16 - loss: 0.4780 - acc: 0.775 - ETA: 1:11 - loss: 0.4785 - acc: 0.775 - ETA: 1:06 - loss: 0.4783 - acc: 0.775 - ETA: 1:01 - loss: 0.4773 - acc: 0.776 - ETA: 57s - loss: 0.4772 - acc: 0.776 - ETA: 52s - loss: 0.4762 - acc: 0.77 - ETA: 47s - loss: 0.4762 - acc: 0.77 - ETA: 42s - loss: 0.4760 - acc: 0.77 - ETA: 38s - loss: 0.4757 - acc: 0.77 - ETA: 33s - loss: 0.4755 - acc: 0.77 - ETA: 28s - loss: 0.4754 - acc: 0.77 - ETA: 23s - loss: 0.4754 - acc: 0.78 - ETA: 19s - loss: 0.4752 - acc: 0.78 - ETA: 14s - loss: 0.4758 - acc: 0.77 - ETA: 9s - loss: 0.4762 - acc: 0.7792 - ETA: 4s - loss: 0.4759 - acc: 0.7796\n",
      "Epoch 00027: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4757 - acc: 0.7799 - val_loss: 0.6111 - val_acc: 0.6667\n",
      "Epoch 28/100\n",
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.4177 - acc: 0.864 - ETA: 6:05 - loss: 0.4351 - acc: 0.838 - ETA: 6:01 - loss: 0.4487 - acc: 0.816 - ETA: 5:56 - loss: 0.4677 - acc: 0.799 - ETA: 5:51 - loss: 0.4690 - acc: 0.797 - ETA: 5:46 - loss: 0.4682 - acc: 0.798 - ETA: 5:42 - loss: 0.4665 - acc: 0.793 - ETA: 5:37 - loss: 0.4618 - acc: 0.800 - ETA: 5:32 - loss: 0.4659 - acc: 0.795 - ETA: 5:27 - loss: 0.4647 - acc: 0.800 - ETA: 5:22 - loss: 0.4705 - acc: 0.796 - ETA: 5:18 - loss: 0.4748 - acc: 0.790 - ETA: 5:13 - loss: 0.4776 - acc: 0.786 - ETA: 5:08 - loss: 0.4785 - acc: 0.786 - ETA: 5:03 - loss: 0.4771 - acc: 0.789 - ETA: 4:58 - loss: 0.4751 - acc: 0.789 - ETA: 4:54 - loss: 0.4759 - acc: 0.789 - ETA: 4:49 - loss: 0.4754 - acc: 0.789 - ETA: 4:44 - loss: 0.4784 - acc: 0.787 - ETA: 4:39 - loss: 0.4789 - acc: 0.788 - ETA: 4:34 - loss: 0.4784 - acc: 0.788 - ETA: 4:30 - loss: 0.4789 - acc: 0.786 - ETA: 4:25 - loss: 0.4762 - acc: 0.788 - ETA: 4:20 - loss: 0.4745 - acc: 0.790 - ETA: 4:15 - loss: 0.4740 - acc: 0.789 - ETA: 4:10 - loss: 0.4747 - acc: 0.788 - ETA: 4:05 - loss: 0.4749 - acc: 0.787 - ETA: 4:01 - loss: 0.4757 - acc: 0.786 - ETA: 3:56 - loss: 0.4757 - acc: 0.787 - ETA: 3:51 - loss: 0.4751 - acc: 0.787 - ETA: 3:46 - loss: 0.4764 - acc: 0.786 - ETA: 3:41 - loss: 0.4770 - acc: 0.785 - ETA: 3:36 - loss: 0.4752 - acc: 0.786 - ETA: 3:32 - loss: 0.4761 - acc: 0.786 - ETA: 3:27 - loss: 0.4760 - acc: 0.786 - ETA: 3:22 - loss: 0.4756 - acc: 0.786 - ETA: 3:17 - loss: 0.4748 - acc: 0.787 - ETA: 3:12 - loss: 0.4755 - acc: 0.785 - ETA: 3:08 - loss: 0.4763 - acc: 0.785 - ETA: 3:03 - loss: 0.4764 - acc: 0.785 - ETA: 2:58 - loss: 0.4772 - acc: 0.783 - ETA: 2:53 - loss: 0.4765 - acc: 0.784 - ETA: 2:48 - loss: 0.4751 - acc: 0.785 - ETA: 2:43 - loss: 0.4744 - acc: 0.786 - ETA: 2:39 - loss: 0.4746 - acc: 0.786 - ETA: 2:34 - loss: 0.4738 - acc: 0.787 - ETA: 2:29 - loss: 0.4728 - acc: 0.788 - ETA: 2:24 - loss: 0.4742 - acc: 0.787 - ETA: 2:19 - loss: 0.4745 - acc: 0.786 - ETA: 2:15 - loss: 0.4736 - acc: 0.787 - ETA: 2:10 - loss: 0.4745 - acc: 0.786 - ETA: 2:05 - loss: 0.4743 - acc: 0.786 - ETA: 2:00 - loss: 0.4746 - acc: 0.786 - ETA: 1:55 - loss: 0.4740 - acc: 0.786 - ETA: 1:50 - loss: 0.4732 - acc: 0.788 - ETA: 1:46 - loss: 0.4729 - acc: 0.788 - ETA: 1:41 - loss: 0.4722 - acc: 0.789 - ETA: 1:36 - loss: 0.4717 - acc: 0.789 - ETA: 1:31 - loss: 0.4717 - acc: 0.789 - ETA: 1:26 - loss: 0.4720 - acc: 0.789 - ETA: 1:21 - loss: 0.4718 - acc: 0.789 - ETA: 1:17 - loss: 0.4716 - acc: 0.788 - ETA: 1:12 - loss: 0.4724 - acc: 0.787 - ETA: 1:07 - loss: 0.4730 - acc: 0.787 - ETA: 1:02 - loss: 0.4726 - acc: 0.788 - ETA: 57s - loss: 0.4718 - acc: 0.789 - ETA: 53s - loss: 0.4722 - acc: 0.78 - ETA: 48s - loss: 0.4722 - acc: 0.78 - ETA: 43s - loss: 0.4728 - acc: 0.78 - ETA: 38s - loss: 0.4723 - acc: 0.78 - ETA: 33s - loss: 0.4717 - acc: 0.78 - ETA: 28s - loss: 0.4721 - acc: 0.78 - ETA: 24s - loss: 0.4720 - acc: 0.78 - ETA: 19s - loss: 0.4719 - acc: 0.78 - ETA: 14s - loss: 0.4722 - acc: 0.78 - ETA: 9s - loss: 0.4730 - acc: 0.7882 - ETA: 4s - loss: 0.4721 - acc: 0.7888\n",
      "Epoch 00028: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4723 - acc: 0.7886 - val_loss: 0.6423 - val_acc: 0.6458\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4743 - acc: 0.812 - ETA: 6:05 - loss: 0.4461 - acc: 0.817 - ETA: 6:01 - loss: 0.4628 - acc: 0.802 - ETA: 5:56 - loss: 0.4660 - acc: 0.802 - ETA: 5:51 - loss: 0.4684 - acc: 0.793 - ETA: 5:46 - loss: 0.4663 - acc: 0.795 - ETA: 5:42 - loss: 0.4672 - acc: 0.791 - ETA: 5:37 - loss: 0.4619 - acc: 0.798 - ETA: 5:32 - loss: 0.4593 - acc: 0.797 - ETA: 5:27 - loss: 0.4571 - acc: 0.800 - ETA: 5:22 - loss: 0.4613 - acc: 0.796 - ETA: 5:18 - loss: 0.4615 - acc: 0.797 - ETA: 5:13 - loss: 0.4606 - acc: 0.796 - ETA: 5:08 - loss: 0.4591 - acc: 0.797 - ETA: 5:03 - loss: 0.4586 - acc: 0.798 - ETA: 4:58 - loss: 0.4585 - acc: 0.798 - ETA: 4:54 - loss: 0.4564 - acc: 0.799 - ETA: 4:49 - loss: 0.4556 - acc: 0.799 - ETA: 4:44 - loss: 0.4560 - acc: 0.799 - ETA: 4:39 - loss: 0.4579 - acc: 0.797 - ETA: 4:34 - loss: 0.4625 - acc: 0.794 - ETA: 4:29 - loss: 0.4620 - acc: 0.795 - ETA: 4:25 - loss: 0.4642 - acc: 0.793 - ETA: 4:20 - loss: 0.4636 - acc: 0.792 - ETA: 4:15 - loss: 0.4642 - acc: 0.789 - ETA: 4:10 - loss: 0.4634 - acc: 0.789 - ETA: 4:05 - loss: 0.4633 - acc: 0.789 - ETA: 4:01 - loss: 0.4648 - acc: 0.787 - ETA: 3:56 - loss: 0.4656 - acc: 0.787 - ETA: 3:51 - loss: 0.4634 - acc: 0.789 - ETA: 3:46 - loss: 0.4645 - acc: 0.787 - ETA: 3:41 - loss: 0.4647 - acc: 0.787 - ETA: 3:36 - loss: 0.4644 - acc: 0.788 - ETA: 3:32 - loss: 0.4641 - acc: 0.787 - ETA: 3:27 - loss: 0.4638 - acc: 0.788 - ETA: 3:22 - loss: 0.4640 - acc: 0.789 - ETA: 3:17 - loss: 0.4648 - acc: 0.788 - ETA: 3:12 - loss: 0.4650 - acc: 0.788 - ETA: 3:08 - loss: 0.4636 - acc: 0.790 - ETA: 2:59 - loss: 0.4644 - acc: 0.791 - ETA: 2:54 - loss: 0.4663 - acc: 0.789 - ETA: 2:49 - loss: 0.4652 - acc: 0.789 - ETA: 2:45 - loss: 0.4662 - acc: 0.789 - ETA: 2:40 - loss: 0.4660 - acc: 0.789 - ETA: 2:36 - loss: 0.4658 - acc: 0.790 - ETA: 2:31 - loss: 0.4675 - acc: 0.789 - ETA: 2:26 - loss: 0.4666 - acc: 0.789 - ETA: 2:22 - loss: 0.4670 - acc: 0.789 - ETA: 2:17 - loss: 0.4676 - acc: 0.789 - ETA: 2:12 - loss: 0.4669 - acc: 0.790 - ETA: 2:07 - loss: 0.4664 - acc: 0.790 - ETA: 2:03 - loss: 0.4658 - acc: 0.791 - ETA: 1:58 - loss: 0.4663 - acc: 0.790 - ETA: 1:53 - loss: 0.4662 - acc: 0.790 - ETA: 1:49 - loss: 0.4665 - acc: 0.790 - ETA: 1:44 - loss: 0.4663 - acc: 0.789 - ETA: 1:39 - loss: 0.4664 - acc: 0.789 - ETA: 1:35 - loss: 0.4660 - acc: 0.790 - ETA: 1:30 - loss: 0.4661 - acc: 0.790 - ETA: 1:25 - loss: 0.4664 - acc: 0.789 - ETA: 1:20 - loss: 0.4657 - acc: 0.790 - ETA: 1:16 - loss: 0.4658 - acc: 0.790 - ETA: 1:11 - loss: 0.4664 - acc: 0.789 - ETA: 1:06 - loss: 0.4669 - acc: 0.789 - ETA: 1:01 - loss: 0.4671 - acc: 0.788 - ETA: 57s - loss: 0.4669 - acc: 0.789 - ETA: 52s - loss: 0.4666 - acc: 0.78 - ETA: 47s - loss: 0.4664 - acc: 0.78 - ETA: 42s - loss: 0.4654 - acc: 0.79 - ETA: 38s - loss: 0.4653 - acc: 0.78 - ETA: 33s - loss: 0.4650 - acc: 0.79 - ETA: 28s - loss: 0.4656 - acc: 0.79 - ETA: 23s - loss: 0.4664 - acc: 0.79 - ETA: 19s - loss: 0.4666 - acc: 0.79 - ETA: 14s - loss: 0.4669 - acc: 0.79 - ETA: 9s - loss: 0.4663 - acc: 0.7907 - ETA: 4s - loss: 0.4660 - acc: 0.7909\n",
      "Epoch 00029: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4660 - acc: 0.7910 - val_loss: 0.6311 - val_acc: 0.6566\n",
      "Epoch 30/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.4912 - acc: 0.750 - ETA: 6:06 - loss: 0.5026 - acc: 0.734 - ETA: 6:01 - loss: 0.4876 - acc: 0.753 - ETA: 5:56 - loss: 0.4668 - acc: 0.783 - ETA: 5:51 - loss: 0.4703 - acc: 0.777 - ETA: 5:47 - loss: 0.4727 - acc: 0.774 - ETA: 5:42 - loss: 0.4691 - acc: 0.778 - ETA: 5:37 - loss: 0.4620 - acc: 0.781 - ETA: 5:32 - loss: 0.4572 - acc: 0.789 - ETA: 5:27 - loss: 0.4613 - acc: 0.786 - ETA: 5:22 - loss: 0.4594 - acc: 0.786 - ETA: 5:18 - loss: 0.4583 - acc: 0.786 - ETA: 5:13 - loss: 0.4604 - acc: 0.785 - ETA: 5:08 - loss: 0.4622 - acc: 0.784 - ETA: 5:03 - loss: 0.4638 - acc: 0.783 - ETA: 4:58 - loss: 0.4659 - acc: 0.784 - ETA: 4:54 - loss: 0.4686 - acc: 0.783 - ETA: 4:49 - loss: 0.4704 - acc: 0.781 - ETA: 4:44 - loss: 0.4704 - acc: 0.782 - ETA: 4:39 - loss: 0.4666 - acc: 0.785 - ETA: 4:34 - loss: 0.4662 - acc: 0.785 - ETA: 4:29 - loss: 0.4678 - acc: 0.785 - ETA: 4:25 - loss: 0.4657 - acc: 0.788 - ETA: 4:20 - loss: 0.4647 - acc: 0.789 - ETA: 4:15 - loss: 0.4648 - acc: 0.789 - ETA: 4:10 - loss: 0.4668 - acc: 0.787 - ETA: 4:05 - loss: 0.4667 - acc: 0.787 - ETA: 4:01 - loss: 0.4647 - acc: 0.789 - ETA: 3:56 - loss: 0.4640 - acc: 0.789 - ETA: 3:51 - loss: 0.4610 - acc: 0.792 - ETA: 3:46 - loss: 0.4597 - acc: 0.792 - ETA: 3:41 - loss: 0.4611 - acc: 0.791 - ETA: 3:36 - loss: 0.4611 - acc: 0.791 - ETA: 3:32 - loss: 0.4617 - acc: 0.791 - ETA: 3:27 - loss: 0.4611 - acc: 0.791 - ETA: 3:22 - loss: 0.4618 - acc: 0.789 - ETA: 3:17 - loss: 0.4621 - acc: 0.789 - ETA: 3:12 - loss: 0.4612 - acc: 0.790 - ETA: 3:08 - loss: 0.4617 - acc: 0.790 - ETA: 3:03 - loss: 0.4623 - acc: 0.790 - ETA: 2:58 - loss: 0.4642 - acc: 0.788 - ETA: 2:53 - loss: 0.4656 - acc: 0.788 - ETA: 2:48 - loss: 0.4655 - acc: 0.788 - ETA: 2:43 - loss: 0.4670 - acc: 0.787 - ETA: 2:39 - loss: 0.4673 - acc: 0.786 - ETA: 2:34 - loss: 0.4686 - acc: 0.785 - ETA: 2:29 - loss: 0.4683 - acc: 0.785 - ETA: 2:24 - loss: 0.4680 - acc: 0.785 - ETA: 2:19 - loss: 0.4677 - acc: 0.786 - ETA: 2:15 - loss: 0.4670 - acc: 0.786 - ETA: 2:10 - loss: 0.4676 - acc: 0.785 - ETA: 2:05 - loss: 0.4673 - acc: 0.785 - ETA: 2:00 - loss: 0.4666 - acc: 0.786 - ETA: 1:55 - loss: 0.4674 - acc: 0.785 - ETA: 1:50 - loss: 0.4691 - acc: 0.784 - ETA: 1:46 - loss: 0.4692 - acc: 0.785 - ETA: 1:41 - loss: 0.4681 - acc: 0.786 - ETA: 1:36 - loss: 0.4688 - acc: 0.785 - ETA: 1:31 - loss: 0.4686 - acc: 0.785 - ETA: 1:26 - loss: 0.4688 - acc: 0.784 - ETA: 1:21 - loss: 0.4678 - acc: 0.785 - ETA: 1:17 - loss: 0.4681 - acc: 0.785 - ETA: 1:12 - loss: 0.4681 - acc: 0.785 - ETA: 1:07 - loss: 0.4676 - acc: 0.786 - ETA: 1:02 - loss: 0.4679 - acc: 0.785 - ETA: 57s - loss: 0.4682 - acc: 0.785 - ETA: 53s - loss: 0.4680 - acc: 0.78 - ETA: 48s - loss: 0.4685 - acc: 0.78 - ETA: 43s - loss: 0.4678 - acc: 0.78 - ETA: 38s - loss: 0.4670 - acc: 0.78 - ETA: 33s - loss: 0.4659 - acc: 0.78 - ETA: 28s - loss: 0.4668 - acc: 0.78 - ETA: 23s - loss: 0.4670 - acc: 0.78 - ETA: 19s - loss: 0.4665 - acc: 0.78 - ETA: 14s - loss: 0.4663 - acc: 0.78 - ETA: 9s - loss: 0.4665 - acc: 0.7882 - ETA: 4s - loss: 0.4666 - acc: 0.7883\n",
      "Epoch 00030: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4671 - acc: 0.7878 - val_loss: 0.6168 - val_acc: 0.6641\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4438 - acc: 0.822 - ETA: 6:06 - loss: 0.4483 - acc: 0.828 - ETA: 6:01 - loss: 0.4430 - acc: 0.826 - ETA: 5:56 - loss: 0.4520 - acc: 0.809 - ETA: 5:52 - loss: 0.4416 - acc: 0.822 - ETA: 5:47 - loss: 0.4460 - acc: 0.812 - ETA: 5:42 - loss: 0.4461 - acc: 0.806 - ETA: 5:37 - loss: 0.4463 - acc: 0.808 - ETA: 5:32 - loss: 0.4421 - acc: 0.812 - ETA: 5:28 - loss: 0.4483 - acc: 0.807 - ETA: 5:23 - loss: 0.4460 - acc: 0.807 - ETA: 5:18 - loss: 0.4476 - acc: 0.805 - ETA: 5:13 - loss: 0.4483 - acc: 0.806 - ETA: 5:08 - loss: 0.4502 - acc: 0.805 - ETA: 5:03 - loss: 0.4488 - acc: 0.805 - ETA: 4:59 - loss: 0.4483 - acc: 0.804 - ETA: 4:54 - loss: 0.4470 - acc: 0.807 - ETA: 4:49 - loss: 0.4466 - acc: 0.809 - ETA: 4:44 - loss: 0.4531 - acc: 0.804 - ETA: 4:39 - loss: 0.4512 - acc: 0.804 - ETA: 4:34 - loss: 0.4515 - acc: 0.803 - ETA: 4:30 - loss: 0.4518 - acc: 0.804 - ETA: 4:25 - loss: 0.4509 - acc: 0.806 - ETA: 4:20 - loss: 0.4517 - acc: 0.804 - ETA: 4:15 - loss: 0.4511 - acc: 0.803 - ETA: 4:10 - loss: 0.4524 - acc: 0.802 - ETA: 4:05 - loss: 0.4550 - acc: 0.799 - ETA: 4:01 - loss: 0.4575 - acc: 0.796 - ETA: 3:56 - loss: 0.4573 - acc: 0.796 - ETA: 3:51 - loss: 0.4569 - acc: 0.796 - ETA: 3:46 - loss: 0.4568 - acc: 0.797 - ETA: 3:41 - loss: 0.4582 - acc: 0.795 - ETA: 3:37 - loss: 0.4565 - acc: 0.797 - ETA: 3:32 - loss: 0.4578 - acc: 0.796 - ETA: 3:27 - loss: 0.4575 - acc: 0.796 - ETA: 3:22 - loss: 0.4577 - acc: 0.795 - ETA: 3:17 - loss: 0.4594 - acc: 0.795 - ETA: 3:08 - loss: 0.4626 - acc: 0.791 - ETA: 3:03 - loss: 0.4632 - acc: 0.791 - ETA: 2:59 - loss: 0.4636 - acc: 0.790 - ETA: 2:54 - loss: 0.4639 - acc: 0.790 - ETA: 2:50 - loss: 0.4628 - acc: 0.791 - ETA: 2:45 - loss: 0.4620 - acc: 0.791 - ETA: 2:40 - loss: 0.4616 - acc: 0.792 - ETA: 2:36 - loss: 0.4611 - acc: 0.792 - ETA: 2:31 - loss: 0.4609 - acc: 0.793 - ETA: 2:26 - loss: 0.4607 - acc: 0.793 - ETA: 2:22 - loss: 0.4616 - acc: 0.793 - ETA: 2:17 - loss: 0.4618 - acc: 0.793 - ETA: 2:12 - loss: 0.4617 - acc: 0.793 - ETA: 2:08 - loss: 0.4627 - acc: 0.792 - ETA: 2:03 - loss: 0.4620 - acc: 0.793 - ETA: 1:58 - loss: 0.4622 - acc: 0.793 - ETA: 1:53 - loss: 0.4620 - acc: 0.793 - ETA: 1:49 - loss: 0.4611 - acc: 0.794 - ETA: 1:44 - loss: 0.4604 - acc: 0.794 - ETA: 1:39 - loss: 0.4603 - acc: 0.794 - ETA: 1:35 - loss: 0.4609 - acc: 0.793 - ETA: 1:30 - loss: 0.4607 - acc: 0.794 - ETA: 1:25 - loss: 0.4599 - acc: 0.794 - ETA: 1:20 - loss: 0.4582 - acc: 0.795 - ETA: 1:16 - loss: 0.4577 - acc: 0.795 - ETA: 1:11 - loss: 0.4577 - acc: 0.795 - ETA: 1:06 - loss: 0.4572 - acc: 0.795 - ETA: 1:01 - loss: 0.4564 - acc: 0.796 - ETA: 57s - loss: 0.4565 - acc: 0.795 - ETA: 52s - loss: 0.4567 - acc: 0.79 - ETA: 47s - loss: 0.4573 - acc: 0.79 - ETA: 42s - loss: 0.4574 - acc: 0.79 - ETA: 38s - loss: 0.4576 - acc: 0.79 - ETA: 33s - loss: 0.4577 - acc: 0.79 - ETA: 28s - loss: 0.4583 - acc: 0.79 - ETA: 23s - loss: 0.4587 - acc: 0.79 - ETA: 19s - loss: 0.4590 - acc: 0.79 - ETA: 14s - loss: 0.4588 - acc: 0.79 - ETA: 9s - loss: 0.4591 - acc: 0.7928 - ETA: 4s - loss: 0.4594 - acc: 0.7921\n",
      "Epoch 00031: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4596 - acc: 0.7917 - val_loss: 0.6315 - val_acc: 0.6515\n",
      "Epoch 32/100\n",
      "77/78 [============================>.] - ETA: 6:12 - loss: 0.4688 - acc: 0.802 - ETA: 6:06 - loss: 0.4293 - acc: 0.822 - ETA: 6:01 - loss: 0.4384 - acc: 0.812 - ETA: 5:56 - loss: 0.4519 - acc: 0.796 - ETA: 5:52 - loss: 0.4561 - acc: 0.795 - ETA: 5:47 - loss: 0.4565 - acc: 0.796 - ETA: 5:42 - loss: 0.4563 - acc: 0.796 - ETA: 5:37 - loss: 0.4453 - acc: 0.808 - ETA: 5:32 - loss: 0.4472 - acc: 0.805 - ETA: 5:27 - loss: 0.4531 - acc: 0.801 - ETA: 5:22 - loss: 0.4549 - acc: 0.796 - ETA: 5:18 - loss: 0.4553 - acc: 0.796 - ETA: 5:13 - loss: 0.4493 - acc: 0.801 - ETA: 5:08 - loss: 0.4534 - acc: 0.798 - ETA: 5:03 - loss: 0.4518 - acc: 0.798 - ETA: 4:58 - loss: 0.4536 - acc: 0.798 - ETA: 4:53 - loss: 0.4529 - acc: 0.797 - ETA: 4:49 - loss: 0.4547 - acc: 0.794 - ETA: 4:44 - loss: 0.4540 - acc: 0.793 - ETA: 4:39 - loss: 0.4543 - acc: 0.791 - ETA: 4:34 - loss: 0.4551 - acc: 0.792 - ETA: 4:29 - loss: 0.4537 - acc: 0.792 - ETA: 4:25 - loss: 0.4552 - acc: 0.791 - ETA: 4:20 - loss: 0.4569 - acc: 0.791 - ETA: 4:15 - loss: 0.4585 - acc: 0.789 - ETA: 4:10 - loss: 0.4585 - acc: 0.790 - ETA: 4:05 - loss: 0.4556 - acc: 0.792 - ETA: 4:01 - loss: 0.4571 - acc: 0.792 - ETA: 3:56 - loss: 0.4555 - acc: 0.793 - ETA: 3:51 - loss: 0.4541 - acc: 0.794 - ETA: 3:46 - loss: 0.4552 - acc: 0.793 - ETA: 3:41 - loss: 0.4555 - acc: 0.794 - ETA: 3:36 - loss: 0.4545 - acc: 0.795 - ETA: 3:32 - loss: 0.4551 - acc: 0.795 - ETA: 3:27 - loss: 0.4553 - acc: 0.794 - ETA: 3:22 - loss: 0.4551 - acc: 0.794 - ETA: 3:17 - loss: 0.4562 - acc: 0.793 - ETA: 3:12 - loss: 0.4565 - acc: 0.793 - ETA: 3:08 - loss: 0.4563 - acc: 0.793 - ETA: 3:03 - loss: 0.4559 - acc: 0.793 - ETA: 2:58 - loss: 0.4544 - acc: 0.795 - ETA: 2:53 - loss: 0.4534 - acc: 0.796 - ETA: 2:48 - loss: 0.4530 - acc: 0.796 - ETA: 2:43 - loss: 0.4512 - acc: 0.798 - ETA: 2:39 - loss: 0.4502 - acc: 0.799 - ETA: 2:34 - loss: 0.4501 - acc: 0.799 - ETA: 2:29 - loss: 0.4512 - acc: 0.799 - ETA: 2:24 - loss: 0.4510 - acc: 0.799 - ETA: 2:19 - loss: 0.4513 - acc: 0.799 - ETA: 2:14 - loss: 0.4520 - acc: 0.799 - ETA: 2:10 - loss: 0.4512 - acc: 0.799 - ETA: 2:05 - loss: 0.4510 - acc: 0.799 - ETA: 2:00 - loss: 0.4507 - acc: 0.800 - ETA: 1:55 - loss: 0.4506 - acc: 0.800 - ETA: 1:50 - loss: 0.4510 - acc: 0.800 - ETA: 1:46 - loss: 0.4509 - acc: 0.800 - ETA: 1:41 - loss: 0.4504 - acc: 0.800 - ETA: 1:36 - loss: 0.4506 - acc: 0.800 - ETA: 1:31 - loss: 0.4501 - acc: 0.801 - ETA: 1:26 - loss: 0.4495 - acc: 0.801 - ETA: 1:21 - loss: 0.4505 - acc: 0.800 - ETA: 1:17 - loss: 0.4500 - acc: 0.800 - ETA: 1:11 - loss: 0.4521 - acc: 0.798 - ETA: 1:06 - loss: 0.4528 - acc: 0.796 - ETA: 1:01 - loss: 0.4531 - acc: 0.796 - ETA: 57s - loss: 0.4524 - acc: 0.797 - ETA: 52s - loss: 0.4527 - acc: 0.79 - ETA: 47s - loss: 0.4525 - acc: 0.79 - ETA: 42s - loss: 0.4521 - acc: 0.79 - ETA: 38s - loss: 0.4522 - acc: 0.79 - ETA: 33s - loss: 0.4526 - acc: 0.79 - ETA: 28s - loss: 0.4520 - acc: 0.79 - ETA: 23s - loss: 0.4530 - acc: 0.79 - ETA: 19s - loss: 0.4528 - acc: 0.79 - ETA: 14s - loss: 0.4525 - acc: 0.79 - ETA: 9s - loss: 0.4529 - acc: 0.7982 - ETA: 4s - loss: 0.4529 - acc: 0.7983\n",
      "Epoch 00032: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4531 - acc: 0.7986 - val_loss: 0.6225 - val_acc: 0.6648\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4787 - acc: 0.760 - ETA: 6:05 - loss: 0.4645 - acc: 0.765 - ETA: 6:01 - loss: 0.4582 - acc: 0.774 - ETA: 5:56 - loss: 0.4513 - acc: 0.786 - ETA: 5:52 - loss: 0.4484 - acc: 0.791 - ETA: 5:47 - loss: 0.4395 - acc: 0.803 - ETA: 5:42 - loss: 0.4389 - acc: 0.800 - ETA: 5:37 - loss: 0.4433 - acc: 0.800 - ETA: 5:32 - loss: 0.4487 - acc: 0.796 - ETA: 5:27 - loss: 0.4519 - acc: 0.794 - ETA: 5:23 - loss: 0.4501 - acc: 0.797 - ETA: 5:18 - loss: 0.4514 - acc: 0.797 - ETA: 5:13 - loss: 0.4534 - acc: 0.794 - ETA: 5:08 - loss: 0.4515 - acc: 0.796 - ETA: 5:03 - loss: 0.4501 - acc: 0.797 - ETA: 4:58 - loss: 0.4514 - acc: 0.798 - ETA: 4:54 - loss: 0.4528 - acc: 0.797 - ETA: 4:49 - loss: 0.4488 - acc: 0.801 - ETA: 4:44 - loss: 0.4475 - acc: 0.803 - ETA: 4:39 - loss: 0.4472 - acc: 0.803 - ETA: 4:34 - loss: 0.4469 - acc: 0.804 - ETA: 4:30 - loss: 0.4473 - acc: 0.804 - ETA: 4:25 - loss: 0.4468 - acc: 0.805 - ETA: 4:20 - loss: 0.4441 - acc: 0.808 - ETA: 4:15 - loss: 0.4438 - acc: 0.808 - ETA: 4:10 - loss: 0.4453 - acc: 0.806 - ETA: 4:05 - loss: 0.4440 - acc: 0.807 - ETA: 4:01 - loss: 0.4429 - acc: 0.808 - ETA: 3:56 - loss: 0.4439 - acc: 0.806 - ETA: 3:51 - loss: 0.4444 - acc: 0.806 - ETA: 3:46 - loss: 0.4437 - acc: 0.807 - ETA: 3:41 - loss: 0.4425 - acc: 0.808 - ETA: 3:36 - loss: 0.4426 - acc: 0.808 - ETA: 3:32 - loss: 0.4411 - acc: 0.808 - ETA: 3:27 - loss: 0.4408 - acc: 0.808 - ETA: 3:22 - loss: 0.4413 - acc: 0.808 - ETA: 3:13 - loss: 0.4387 - acc: 0.811 - ETA: 3:08 - loss: 0.4392 - acc: 0.812 - ETA: 3:03 - loss: 0.4392 - acc: 0.811 - ETA: 2:59 - loss: 0.4397 - acc: 0.810 - ETA: 2:54 - loss: 0.4397 - acc: 0.811 - ETA: 2:50 - loss: 0.4392 - acc: 0.811 - ETA: 2:45 - loss: 0.4392 - acc: 0.811 - ETA: 2:40 - loss: 0.4394 - acc: 0.810 - ETA: 2:36 - loss: 0.4390 - acc: 0.810 - ETA: 2:31 - loss: 0.4395 - acc: 0.809 - ETA: 2:26 - loss: 0.4392 - acc: 0.809 - ETA: 2:22 - loss: 0.4392 - acc: 0.809 - ETA: 2:17 - loss: 0.4391 - acc: 0.810 - ETA: 2:12 - loss: 0.4402 - acc: 0.809 - ETA: 2:08 - loss: 0.4400 - acc: 0.810 - ETA: 2:03 - loss: 0.4395 - acc: 0.810 - ETA: 1:58 - loss: 0.4405 - acc: 0.808 - ETA: 1:53 - loss: 0.4407 - acc: 0.809 - ETA: 1:49 - loss: 0.4402 - acc: 0.808 - ETA: 1:44 - loss: 0.4409 - acc: 0.807 - ETA: 1:39 - loss: 0.4402 - acc: 0.808 - ETA: 1:35 - loss: 0.4409 - acc: 0.808 - ETA: 1:30 - loss: 0.4410 - acc: 0.808 - ETA: 1:25 - loss: 0.4409 - acc: 0.808 - ETA: 1:20 - loss: 0.4407 - acc: 0.808 - ETA: 1:16 - loss: 0.4407 - acc: 0.808 - ETA: 1:11 - loss: 0.4403 - acc: 0.808 - ETA: 1:06 - loss: 0.4404 - acc: 0.808 - ETA: 1:01 - loss: 0.4419 - acc: 0.807 - ETA: 57s - loss: 0.4422 - acc: 0.807 - ETA: 52s - loss: 0.4421 - acc: 0.80 - ETA: 47s - loss: 0.4431 - acc: 0.80 - ETA: 42s - loss: 0.4439 - acc: 0.80 - ETA: 38s - loss: 0.4441 - acc: 0.80 - ETA: 33s - loss: 0.4441 - acc: 0.80 - ETA: 28s - loss: 0.4434 - acc: 0.80 - ETA: 23s - loss: 0.4444 - acc: 0.80 - ETA: 19s - loss: 0.4442 - acc: 0.80 - ETA: 14s - loss: 0.4446 - acc: 0.80 - ETA: 9s - loss: 0.4442 - acc: 0.8056 - ETA: 4s - loss: 0.4443 - acc: 0.8052\n",
      "Epoch 00033: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4440 - acc: 0.8057 - val_loss: 0.6310 - val_acc: 0.6648\n",
      "Epoch 34/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.4100 - acc: 0.843 - ETA: 6:06 - loss: 0.4156 - acc: 0.828 - ETA: 6:01 - loss: 0.4283 - acc: 0.819 - ETA: 5:56 - loss: 0.4276 - acc: 0.812 - ETA: 5:51 - loss: 0.4310 - acc: 0.816 - ETA: 5:46 - loss: 0.4284 - acc: 0.816 - ETA: 5:42 - loss: 0.4347 - acc: 0.814 - ETA: 5:37 - loss: 0.4296 - acc: 0.820 - ETA: 5:32 - loss: 0.4421 - acc: 0.809 - ETA: 5:27 - loss: 0.4409 - acc: 0.811 - ETA: 5:22 - loss: 0.4443 - acc: 0.811 - ETA: 5:18 - loss: 0.4465 - acc: 0.808 - ETA: 5:13 - loss: 0.4451 - acc: 0.809 - ETA: 5:08 - loss: 0.4453 - acc: 0.808 - ETA: 5:03 - loss: 0.4491 - acc: 0.804 - ETA: 4:58 - loss: 0.4519 - acc: 0.800 - ETA: 4:54 - loss: 0.4518 - acc: 0.799 - ETA: 4:49 - loss: 0.4536 - acc: 0.800 - ETA: 4:44 - loss: 0.4526 - acc: 0.799 - ETA: 4:39 - loss: 0.4549 - acc: 0.800 - ETA: 4:34 - loss: 0.4556 - acc: 0.799 - ETA: 4:29 - loss: 0.4548 - acc: 0.799 - ETA: 4:25 - loss: 0.4551 - acc: 0.798 - ETA: 4:20 - loss: 0.4550 - acc: 0.796 - ETA: 4:15 - loss: 0.4552 - acc: 0.795 - ETA: 4:10 - loss: 0.4550 - acc: 0.794 - ETA: 4:05 - loss: 0.4543 - acc: 0.795 - ETA: 4:00 - loss: 0.4553 - acc: 0.795 - ETA: 3:56 - loss: 0.4556 - acc: 0.795 - ETA: 3:51 - loss: 0.4535 - acc: 0.797 - ETA: 3:46 - loss: 0.4535 - acc: 0.798 - ETA: 3:41 - loss: 0.4522 - acc: 0.799 - ETA: 3:36 - loss: 0.4512 - acc: 0.800 - ETA: 3:32 - loss: 0.4513 - acc: 0.800 - ETA: 3:27 - loss: 0.4506 - acc: 0.801 - ETA: 3:22 - loss: 0.4497 - acc: 0.802 - ETA: 3:17 - loss: 0.4520 - acc: 0.799 - ETA: 3:12 - loss: 0.4520 - acc: 0.799 - ETA: 3:07 - loss: 0.4501 - acc: 0.800 - ETA: 3:03 - loss: 0.4514 - acc: 0.799 - ETA: 2:58 - loss: 0.4518 - acc: 0.799 - ETA: 2:53 - loss: 0.4513 - acc: 0.799 - ETA: 2:48 - loss: 0.4515 - acc: 0.799 - ETA: 2:43 - loss: 0.4511 - acc: 0.799 - ETA: 2:39 - loss: 0.4513 - acc: 0.799 - ETA: 2:34 - loss: 0.4518 - acc: 0.799 - ETA: 2:29 - loss: 0.4517 - acc: 0.798 - ETA: 2:24 - loss: 0.4517 - acc: 0.798 - ETA: 2:19 - loss: 0.4516 - acc: 0.798 - ETA: 2:14 - loss: 0.4516 - acc: 0.798 - ETA: 2:10 - loss: 0.4508 - acc: 0.799 - ETA: 2:05 - loss: 0.4511 - acc: 0.800 - ETA: 2:00 - loss: 0.4508 - acc: 0.800 - ETA: 1:55 - loss: 0.4505 - acc: 0.800 - ETA: 1:50 - loss: 0.4495 - acc: 0.801 - ETA: 1:46 - loss: 0.4496 - acc: 0.800 - ETA: 1:41 - loss: 0.4500 - acc: 0.800 - ETA: 1:36 - loss: 0.4503 - acc: 0.799 - ETA: 1:31 - loss: 0.4504 - acc: 0.799 - ETA: 1:26 - loss: 0.4510 - acc: 0.798 - ETA: 1:20 - loss: 0.4505 - acc: 0.797 - ETA: 1:16 - loss: 0.4507 - acc: 0.797 - ETA: 1:11 - loss: 0.4511 - acc: 0.797 - ETA: 1:06 - loss: 0.4507 - acc: 0.798 - ETA: 1:01 - loss: 0.4505 - acc: 0.798 - ETA: 57s - loss: 0.4504 - acc: 0.798 - ETA: 52s - loss: 0.4493 - acc: 0.80 - ETA: 47s - loss: 0.4499 - acc: 0.79 - ETA: 42s - loss: 0.4497 - acc: 0.80 - ETA: 38s - loss: 0.4495 - acc: 0.80 - ETA: 33s - loss: 0.4497 - acc: 0.80 - ETA: 28s - loss: 0.4490 - acc: 0.80 - ETA: 23s - loss: 0.4501 - acc: 0.80 - ETA: 19s - loss: 0.4497 - acc: 0.80 - ETA: 14s - loss: 0.4492 - acc: 0.80 - ETA: 9s - loss: 0.4491 - acc: 0.8006 - ETA: 4s - loss: 0.4482 - acc: 0.8011\n",
      "Epoch 00034: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4481 - acc: 0.8013 - val_loss: 0.6332 - val_acc: 0.6660\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4289 - acc: 0.833 - ETA: 6:05 - loss: 0.4233 - acc: 0.822 - ETA: 6:01 - loss: 0.4291 - acc: 0.809 - ETA: 5:55 - loss: 0.4332 - acc: 0.815 - ETA: 5:51 - loss: 0.4441 - acc: 0.804 - ETA: 5:46 - loss: 0.4518 - acc: 0.800 - ETA: 5:42 - loss: 0.4430 - acc: 0.811 - ETA: 5:37 - loss: 0.4472 - acc: 0.806 - ETA: 5:32 - loss: 0.4452 - acc: 0.809 - ETA: 5:27 - loss: 0.4436 - acc: 0.807 - ETA: 5:22 - loss: 0.4428 - acc: 0.807 - ETA: 5:17 - loss: 0.4410 - acc: 0.806 - ETA: 5:13 - loss: 0.4380 - acc: 0.807 - ETA: 5:08 - loss: 0.4403 - acc: 0.805 - ETA: 5:03 - loss: 0.4406 - acc: 0.805 - ETA: 4:58 - loss: 0.4404 - acc: 0.806 - ETA: 4:53 - loss: 0.4435 - acc: 0.803 - ETA: 4:49 - loss: 0.4427 - acc: 0.803 - ETA: 4:44 - loss: 0.4406 - acc: 0.805 - ETA: 4:39 - loss: 0.4424 - acc: 0.804 - ETA: 4:34 - loss: 0.4412 - acc: 0.804 - ETA: 4:29 - loss: 0.4404 - acc: 0.805 - ETA: 4:25 - loss: 0.4369 - acc: 0.808 - ETA: 4:20 - loss: 0.4363 - acc: 0.808 - ETA: 4:15 - loss: 0.4384 - acc: 0.806 - ETA: 4:10 - loss: 0.4400 - acc: 0.805 - ETA: 4:05 - loss: 0.4418 - acc: 0.804 - ETA: 4:00 - loss: 0.4399 - acc: 0.805 - ETA: 3:56 - loss: 0.4383 - acc: 0.806 - ETA: 3:51 - loss: 0.4375 - acc: 0.808 - ETA: 3:46 - loss: 0.4375 - acc: 0.809 - ETA: 3:41 - loss: 0.4370 - acc: 0.809 - ETA: 3:36 - loss: 0.4374 - acc: 0.809 - ETA: 3:32 - loss: 0.4377 - acc: 0.808 - ETA: 3:27 - loss: 0.4383 - acc: 0.807 - ETA: 3:22 - loss: 0.4400 - acc: 0.805 - ETA: 3:17 - loss: 0.4408 - acc: 0.804 - ETA: 3:12 - loss: 0.4418 - acc: 0.804 - ETA: 3:07 - loss: 0.4422 - acc: 0.803 - ETA: 3:03 - loss: 0.4424 - acc: 0.802 - ETA: 2:58 - loss: 0.4415 - acc: 0.803 - ETA: 2:53 - loss: 0.4424 - acc: 0.803 - ETA: 2:48 - loss: 0.4423 - acc: 0.803 - ETA: 2:43 - loss: 0.4417 - acc: 0.804 - ETA: 2:39 - loss: 0.4409 - acc: 0.804 - ETA: 2:34 - loss: 0.4406 - acc: 0.804 - ETA: 2:29 - loss: 0.4403 - acc: 0.804 - ETA: 2:24 - loss: 0.4404 - acc: 0.805 - ETA: 2:19 - loss: 0.4404 - acc: 0.805 - ETA: 2:14 - loss: 0.4406 - acc: 0.805 - ETA: 2:10 - loss: 0.4396 - acc: 0.805 - ETA: 2:05 - loss: 0.4407 - acc: 0.804 - ETA: 2:00 - loss: 0.4392 - acc: 0.805 - ETA: 1:55 - loss: 0.4398 - acc: 0.805 - ETA: 1:50 - loss: 0.4399 - acc: 0.805 - ETA: 1:46 - loss: 0.4402 - acc: 0.805 - ETA: 1:41 - loss: 0.4405 - acc: 0.804 - ETA: 1:36 - loss: 0.4397 - acc: 0.805 - ETA: 1:31 - loss: 0.4405 - acc: 0.804 - ETA: 1:26 - loss: 0.4401 - acc: 0.804 - ETA: 1:21 - loss: 0.4393 - acc: 0.805 - ETA: 1:17 - loss: 0.4393 - acc: 0.805 - ETA: 1:12 - loss: 0.4399 - acc: 0.805 - ETA: 1:07 - loss: 0.4392 - acc: 0.806 - ETA: 1:02 - loss: 0.4393 - acc: 0.806 - ETA: 57s - loss: 0.4398 - acc: 0.806 - ETA: 53s - loss: 0.4407 - acc: 0.80 - ETA: 48s - loss: 0.4407 - acc: 0.80 - ETA: 43s - loss: 0.4402 - acc: 0.80 - ETA: 38s - loss: 0.4400 - acc: 0.80 - ETA: 33s - loss: 0.4408 - acc: 0.80 - ETA: 28s - loss: 0.4407 - acc: 0.80 - ETA: 24s - loss: 0.4404 - acc: 0.80 - ETA: 19s - loss: 0.4406 - acc: 0.80 - ETA: 14s - loss: 0.4404 - acc: 0.80 - ETA: 9s - loss: 0.4399 - acc: 0.8050 - ETA: 4s - loss: 0.4402 - acc: 0.8047\n",
      "Epoch 00035: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.4399 - acc: 0.8049 - val_loss: 0.6276 - val_acc: 0.6692\n",
      "Epoch 36/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4130 - acc: 0.812 - ETA: 6:06 - loss: 0.4217 - acc: 0.802 - ETA: 6:01 - loss: 0.4203 - acc: 0.812 - ETA: 5:56 - loss: 0.4235 - acc: 0.817 - ETA: 5:51 - loss: 0.4445 - acc: 0.806 - ETA: 5:46 - loss: 0.4518 - acc: 0.800 - ETA: 5:42 - loss: 0.4537 - acc: 0.805 - ETA: 5:37 - loss: 0.4448 - acc: 0.812 - ETA: 5:32 - loss: 0.4412 - acc: 0.818 - ETA: 5:27 - loss: 0.4439 - acc: 0.809 - ETA: 5:22 - loss: 0.4429 - acc: 0.807 - ETA: 5:18 - loss: 0.4382 - acc: 0.810 - ETA: 5:13 - loss: 0.4377 - acc: 0.812 - ETA: 5:08 - loss: 0.4397 - acc: 0.811 - ETA: 5:03 - loss: 0.4366 - acc: 0.812 - ETA: 4:58 - loss: 0.4357 - acc: 0.811 - ETA: 4:54 - loss: 0.4349 - acc: 0.811 - ETA: 4:49 - loss: 0.4360 - acc: 0.809 - ETA: 4:44 - loss: 0.4353 - acc: 0.810 - ETA: 4:39 - loss: 0.4377 - acc: 0.806 - ETA: 4:34 - loss: 0.4374 - acc: 0.808 - ETA: 4:29 - loss: 0.4345 - acc: 0.812 - ETA: 4:25 - loss: 0.4313 - acc: 0.815 - ETA: 4:20 - loss: 0.4325 - acc: 0.815 - ETA: 4:15 - loss: 0.4334 - acc: 0.813 - ETA: 4:02 - loss: 0.4360 - acc: 0.814 - ETA: 3:57 - loss: 0.4365 - acc: 0.814 - ETA: 3:53 - loss: 0.4387 - acc: 0.813 - ETA: 3:49 - loss: 0.4398 - acc: 0.811 - ETA: 3:44 - loss: 0.4388 - acc: 0.812 - ETA: 3:40 - loss: 0.4393 - acc: 0.812 - ETA: 3:35 - loss: 0.4399 - acc: 0.811 - ETA: 3:31 - loss: 0.4399 - acc: 0.810 - ETA: 3:26 - loss: 0.4415 - acc: 0.809 - ETA: 3:22 - loss: 0.4421 - acc: 0.809 - ETA: 3:17 - loss: 0.4416 - acc: 0.809 - ETA: 3:12 - loss: 0.4414 - acc: 0.810 - ETA: 3:08 - loss: 0.4397 - acc: 0.812 - ETA: 3:03 - loss: 0.4390 - acc: 0.813 - ETA: 2:59 - loss: 0.4392 - acc: 0.812 - ETA: 2:54 - loss: 0.4393 - acc: 0.813 - ETA: 2:49 - loss: 0.4398 - acc: 0.813 - ETA: 2:45 - loss: 0.4398 - acc: 0.812 - ETA: 2:40 - loss: 0.4400 - acc: 0.812 - ETA: 2:36 - loss: 0.4393 - acc: 0.813 - ETA: 2:31 - loss: 0.4396 - acc: 0.812 - ETA: 2:26 - loss: 0.4396 - acc: 0.811 - ETA: 2:22 - loss: 0.4401 - acc: 0.811 - ETA: 2:17 - loss: 0.4390 - acc: 0.811 - ETA: 2:12 - loss: 0.4380 - acc: 0.812 - ETA: 2:07 - loss: 0.4383 - acc: 0.812 - ETA: 2:03 - loss: 0.4375 - acc: 0.813 - ETA: 1:58 - loss: 0.4398 - acc: 0.811 - ETA: 1:53 - loss: 0.4394 - acc: 0.811 - ETA: 1:49 - loss: 0.4407 - acc: 0.810 - ETA: 1:44 - loss: 0.4404 - acc: 0.810 - ETA: 1:39 - loss: 0.4405 - acc: 0.811 - ETA: 1:34 - loss: 0.4397 - acc: 0.812 - ETA: 1:30 - loss: 0.4403 - acc: 0.811 - ETA: 1:25 - loss: 0.4419 - acc: 0.809 - ETA: 1:20 - loss: 0.4416 - acc: 0.809 - ETA: 1:16 - loss: 0.4423 - acc: 0.807 - ETA: 1:11 - loss: 0.4418 - acc: 0.808 - ETA: 1:06 - loss: 0.4417 - acc: 0.808 - ETA: 1:01 - loss: 0.4421 - acc: 0.809 - ETA: 57s - loss: 0.4414 - acc: 0.810 - ETA: 52s - loss: 0.4411 - acc: 0.81 - ETA: 47s - loss: 0.4412 - acc: 0.81 - ETA: 42s - loss: 0.4422 - acc: 0.80 - ETA: 38s - loss: 0.4426 - acc: 0.80 - ETA: 33s - loss: 0.4424 - acc: 0.80 - ETA: 28s - loss: 0.4418 - acc: 0.80 - ETA: 23s - loss: 0.4413 - acc: 0.80 - ETA: 19s - loss: 0.4425 - acc: 0.80 - ETA: 14s - loss: 0.4424 - acc: 0.80 - ETA: 9s - loss: 0.4418 - acc: 0.8083 - ETA: 4s - loss: 0.4423 - acc: 0.8079\n",
      "Epoch 00036: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4429 - acc: 0.8073 - val_loss: 0.6386 - val_acc: 0.6585\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4335 - acc: 0.802 - ETA: 6:06 - loss: 0.4341 - acc: 0.807 - ETA: 6:01 - loss: 0.4344 - acc: 0.809 - ETA: 5:56 - loss: 0.4258 - acc: 0.817 - ETA: 5:51 - loss: 0.4294 - acc: 0.810 - ETA: 5:46 - loss: 0.4370 - acc: 0.803 - ETA: 5:42 - loss: 0.4407 - acc: 0.796 - ETA: 5:37 - loss: 0.4404 - acc: 0.794 - ETA: 5:32 - loss: 0.4375 - acc: 0.796 - ETA: 5:27 - loss: 0.4386 - acc: 0.796 - ETA: 5:22 - loss: 0.4380 - acc: 0.798 - ETA: 5:18 - loss: 0.4384 - acc: 0.796 - ETA: 5:13 - loss: 0.4432 - acc: 0.794 - ETA: 5:08 - loss: 0.4406 - acc: 0.796 - ETA: 4:46 - loss: 0.4504 - acc: 0.787 - ETA: 4:42 - loss: 0.4536 - acc: 0.786 - ETA: 4:39 - loss: 0.4504 - acc: 0.789 - ETA: 4:35 - loss: 0.4507 - acc: 0.791 - ETA: 4:31 - loss: 0.4491 - acc: 0.792 - ETA: 4:27 - loss: 0.4473 - acc: 0.795 - ETA: 4:23 - loss: 0.4492 - acc: 0.794 - ETA: 4:19 - loss: 0.4492 - acc: 0.795 - ETA: 4:15 - loss: 0.4481 - acc: 0.798 - ETA: 4:10 - loss: 0.4478 - acc: 0.798 - ETA: 4:06 - loss: 0.4458 - acc: 0.799 - ETA: 4:02 - loss: 0.4449 - acc: 0.800 - ETA: 3:58 - loss: 0.4449 - acc: 0.800 - ETA: 3:53 - loss: 0.4441 - acc: 0.801 - ETA: 3:49 - loss: 0.4424 - acc: 0.800 - ETA: 3:44 - loss: 0.4410 - acc: 0.802 - ETA: 3:40 - loss: 0.4396 - acc: 0.803 - ETA: 3:35 - loss: 0.4399 - acc: 0.802 - ETA: 3:31 - loss: 0.4403 - acc: 0.801 - ETA: 3:26 - loss: 0.4412 - acc: 0.801 - ETA: 3:22 - loss: 0.4410 - acc: 0.801 - ETA: 3:17 - loss: 0.4393 - acc: 0.803 - ETA: 3:13 - loss: 0.4399 - acc: 0.802 - ETA: 3:08 - loss: 0.4408 - acc: 0.801 - ETA: 3:03 - loss: 0.4420 - acc: 0.801 - ETA: 2:59 - loss: 0.4417 - acc: 0.801 - ETA: 2:54 - loss: 0.4400 - acc: 0.802 - ETA: 2:50 - loss: 0.4384 - acc: 0.804 - ETA: 2:45 - loss: 0.4387 - acc: 0.803 - ETA: 2:40 - loss: 0.4395 - acc: 0.803 - ETA: 2:36 - loss: 0.4398 - acc: 0.803 - ETA: 2:28 - loss: 0.4402 - acc: 0.801 - ETA: 2:23 - loss: 0.4410 - acc: 0.801 - ETA: 2:19 - loss: 0.4404 - acc: 0.802 - ETA: 2:14 - loss: 0.4411 - acc: 0.801 - ETA: 2:10 - loss: 0.4408 - acc: 0.802 - ETA: 2:05 - loss: 0.4411 - acc: 0.801 - ETA: 2:01 - loss: 0.4398 - acc: 0.802 - ETA: 1:56 - loss: 0.4410 - acc: 0.801 - ETA: 1:52 - loss: 0.4406 - acc: 0.801 - ETA: 1:47 - loss: 0.4396 - acc: 0.802 - ETA: 1:42 - loss: 0.4407 - acc: 0.801 - ETA: 1:38 - loss: 0.4401 - acc: 0.801 - ETA: 1:33 - loss: 0.4407 - acc: 0.800 - ETA: 1:28 - loss: 0.4419 - acc: 0.799 - ETA: 1:24 - loss: 0.4415 - acc: 0.799 - ETA: 1:19 - loss: 0.4425 - acc: 0.798 - ETA: 1:15 - loss: 0.4417 - acc: 0.799 - ETA: 1:10 - loss: 0.4418 - acc: 0.799 - ETA: 1:05 - loss: 0.4425 - acc: 0.799 - ETA: 1:01 - loss: 0.4420 - acc: 0.799 - ETA: 56s - loss: 0.4433 - acc: 0.798 - ETA: 51s - loss: 0.4425 - acc: 0.79 - ETA: 47s - loss: 0.4428 - acc: 0.79 - ETA: 42s - loss: 0.4434 - acc: 0.79 - ETA: 37s - loss: 0.4423 - acc: 0.79 - ETA: 32s - loss: 0.4431 - acc: 0.79 - ETA: 28s - loss: 0.4424 - acc: 0.79 - ETA: 23s - loss: 0.4417 - acc: 0.79 - ETA: 18s - loss: 0.4417 - acc: 0.79 - ETA: 14s - loss: 0.4425 - acc: 0.79 - ETA: 9s - loss: 0.4425 - acc: 0.7996 - ETA: 4s - loss: 0.4423 - acc: 0.7998\n",
      "Epoch 00037: val_loss did not improve\n",
      "78/78 [==============================] - 459s 6s/step - loss: 0.4421 - acc: 0.8001 - val_loss: 0.6376 - val_acc: 0.6648\n",
      "Epoch 38/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3937 - acc: 0.854 - ETA: 6:06 - loss: 0.4263 - acc: 0.807 - ETA: 6:01 - loss: 0.4263 - acc: 0.819 - ETA: 5:56 - loss: 0.4220 - acc: 0.828 - ETA: 5:51 - loss: 0.4233 - acc: 0.822 - ETA: 5:46 - loss: 0.4250 - acc: 0.821 - ETA: 5:42 - loss: 0.4147 - acc: 0.833 - ETA: 5:37 - loss: 0.4176 - acc: 0.830 - ETA: 5:32 - loss: 0.4178 - acc: 0.829 - ETA: 5:27 - loss: 0.4219 - acc: 0.824 - ETA: 5:23 - loss: 0.4227 - acc: 0.821 - ETA: 5:18 - loss: 0.4213 - acc: 0.820 - ETA: 5:13 - loss: 0.4323 - acc: 0.810 - ETA: 5:08 - loss: 0.4303 - acc: 0.814 - ETA: 5:03 - loss: 0.4293 - acc: 0.813 - ETA: 4:58 - loss: 0.4299 - acc: 0.812 - ETA: 4:54 - loss: 0.4264 - acc: 0.817 - ETA: 4:49 - loss: 0.4259 - acc: 0.819 - ETA: 4:44 - loss: 0.4253 - acc: 0.819 - ETA: 4:39 - loss: 0.4273 - acc: 0.818 - ETA: 4:34 - loss: 0.4259 - acc: 0.818 - ETA: 4:29 - loss: 0.4263 - acc: 0.818 - ETA: 4:25 - loss: 0.4255 - acc: 0.818 - ETA: 4:20 - loss: 0.4268 - acc: 0.817 - ETA: 4:15 - loss: 0.4294 - acc: 0.815 - ETA: 4:10 - loss: 0.4293 - acc: 0.817 - ETA: 4:05 - loss: 0.4307 - acc: 0.814 - ETA: 4:01 - loss: 0.4287 - acc: 0.815 - ETA: 3:56 - loss: 0.4283 - acc: 0.815 - ETA: 3:51 - loss: 0.4294 - acc: 0.815 - ETA: 3:46 - loss: 0.4288 - acc: 0.815 - ETA: 3:41 - loss: 0.4290 - acc: 0.815 - ETA: 3:36 - loss: 0.4279 - acc: 0.816 - ETA: 3:32 - loss: 0.4272 - acc: 0.816 - ETA: 3:27 - loss: 0.4261 - acc: 0.818 - ETA: 3:22 - loss: 0.4265 - acc: 0.818 - ETA: 3:17 - loss: 0.4248 - acc: 0.819 - ETA: 3:12 - loss: 0.4248 - acc: 0.820 - ETA: 3:03 - loss: 0.4269 - acc: 0.822 - ETA: 2:59 - loss: 0.4276 - acc: 0.822 - ETA: 2:54 - loss: 0.4286 - acc: 0.820 - ETA: 2:49 - loss: 0.4291 - acc: 0.820 - ETA: 2:45 - loss: 0.4284 - acc: 0.821 - ETA: 2:40 - loss: 0.4293 - acc: 0.819 - ETA: 2:36 - loss: 0.4289 - acc: 0.820 - ETA: 2:31 - loss: 0.4291 - acc: 0.819 - ETA: 2:26 - loss: 0.4299 - acc: 0.818 - ETA: 2:22 - loss: 0.4308 - acc: 0.816 - ETA: 2:17 - loss: 0.4327 - acc: 0.815 - ETA: 2:12 - loss: 0.4327 - acc: 0.816 - ETA: 2:07 - loss: 0.4330 - acc: 0.815 - ETA: 2:03 - loss: 0.4338 - acc: 0.814 - ETA: 1:58 - loss: 0.4337 - acc: 0.815 - ETA: 1:53 - loss: 0.4344 - acc: 0.815 - ETA: 1:49 - loss: 0.4355 - acc: 0.814 - ETA: 1:44 - loss: 0.4358 - acc: 0.813 - ETA: 1:39 - loss: 0.4362 - acc: 0.812 - ETA: 1:35 - loss: 0.4363 - acc: 0.812 - ETA: 1:30 - loss: 0.4362 - acc: 0.812 - ETA: 1:25 - loss: 0.4359 - acc: 0.812 - ETA: 1:20 - loss: 0.4356 - acc: 0.813 - ETA: 1:16 - loss: 0.4345 - acc: 0.814 - ETA: 1:11 - loss: 0.4342 - acc: 0.814 - ETA: 1:06 - loss: 0.4337 - acc: 0.814 - ETA: 1:01 - loss: 0.4324 - acc: 0.816 - ETA: 57s - loss: 0.4322 - acc: 0.815 - ETA: 52s - loss: 0.4324 - acc: 0.81 - ETA: 47s - loss: 0.4329 - acc: 0.81 - ETA: 42s - loss: 0.4323 - acc: 0.81 - ETA: 38s - loss: 0.4323 - acc: 0.81 - ETA: 33s - loss: 0.4321 - acc: 0.81 - ETA: 28s - loss: 0.4317 - acc: 0.81 - ETA: 23s - loss: 0.4317 - acc: 0.81 - ETA: 19s - loss: 0.4316 - acc: 0.81 - ETA: 14s - loss: 0.4313 - acc: 0.81 - ETA: 9s - loss: 0.4313 - acc: 0.8176 - ETA: 4s - loss: 0.4311 - acc: 0.8175\n",
      "Epoch 00038: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4314 - acc: 0.8173 - val_loss: 0.6471 - val_acc: 0.6648\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4521 - acc: 0.781 - ETA: 6:05 - loss: 0.4138 - acc: 0.822 - ETA: 6:01 - loss: 0.4394 - acc: 0.805 - ETA: 5:56 - loss: 0.4415 - acc: 0.794 - ETA: 5:51 - loss: 0.4434 - acc: 0.795 - ETA: 5:46 - loss: 0.4422 - acc: 0.796 - ETA: 5:42 - loss: 0.4406 - acc: 0.800 - ETA: 5:37 - loss: 0.4403 - acc: 0.800 - ETA: 5:32 - loss: 0.4383 - acc: 0.804 - ETA: 5:27 - loss: 0.4389 - acc: 0.804 - ETA: 5:23 - loss: 0.4345 - acc: 0.805 - ETA: 5:18 - loss: 0.4358 - acc: 0.805 - ETA: 5:13 - loss: 0.4389 - acc: 0.803 - ETA: 5:08 - loss: 0.4383 - acc: 0.802 - ETA: 5:03 - loss: 0.4422 - acc: 0.797 - ETA: 4:59 - loss: 0.4404 - acc: 0.797 - ETA: 4:54 - loss: 0.4405 - acc: 0.797 - ETA: 4:49 - loss: 0.4386 - acc: 0.798 - ETA: 4:44 - loss: 0.4368 - acc: 0.799 - ETA: 4:39 - loss: 0.4357 - acc: 0.801 - ETA: 4:35 - loss: 0.4358 - acc: 0.803 - ETA: 4:30 - loss: 0.4367 - acc: 0.803 - ETA: 4:25 - loss: 0.4367 - acc: 0.803 - ETA: 4:20 - loss: 0.4367 - acc: 0.803 - ETA: 4:15 - loss: 0.4328 - acc: 0.806 - ETA: 4:10 - loss: 0.4342 - acc: 0.804 - ETA: 4:06 - loss: 0.4369 - acc: 0.801 - ETA: 4:01 - loss: 0.4358 - acc: 0.802 - ETA: 3:56 - loss: 0.4338 - acc: 0.805 - ETA: 3:51 - loss: 0.4343 - acc: 0.804 - ETA: 3:46 - loss: 0.4359 - acc: 0.804 - ETA: 3:41 - loss: 0.4332 - acc: 0.807 - ETA: 3:37 - loss: 0.4330 - acc: 0.808 - ETA: 3:32 - loss: 0.4325 - acc: 0.809 - ETA: 3:27 - loss: 0.4329 - acc: 0.809 - ETA: 3:22 - loss: 0.4327 - acc: 0.810 - ETA: 3:17 - loss: 0.4319 - acc: 0.811 - ETA: 3:12 - loss: 0.4302 - acc: 0.812 - ETA: 3:08 - loss: 0.4298 - acc: 0.811 - ETA: 3:03 - loss: 0.4289 - acc: 0.812 - ETA: 2:58 - loss: 0.4281 - acc: 0.813 - ETA: 2:53 - loss: 0.4281 - acc: 0.813 - ETA: 2:48 - loss: 0.4302 - acc: 0.813 - ETA: 2:43 - loss: 0.4322 - acc: 0.811 - ETA: 2:39 - loss: 0.4316 - acc: 0.812 - ETA: 2:34 - loss: 0.4321 - acc: 0.811 - ETA: 2:29 - loss: 0.4322 - acc: 0.811 - ETA: 2:24 - loss: 0.4312 - acc: 0.812 - ETA: 2:19 - loss: 0.4306 - acc: 0.813 - ETA: 2:15 - loss: 0.4308 - acc: 0.812 - ETA: 2:10 - loss: 0.4307 - acc: 0.812 - ETA: 2:05 - loss: 0.4325 - acc: 0.811 - ETA: 2:00 - loss: 0.4320 - acc: 0.812 - ETA: 1:55 - loss: 0.4322 - acc: 0.812 - ETA: 1:50 - loss: 0.4327 - acc: 0.812 - ETA: 1:46 - loss: 0.4331 - acc: 0.811 - ETA: 1:41 - loss: 0.4321 - acc: 0.813 - ETA: 1:36 - loss: 0.4317 - acc: 0.813 - ETA: 1:31 - loss: 0.4315 - acc: 0.813 - ETA: 1:26 - loss: 0.4315 - acc: 0.813 - ETA: 1:21 - loss: 0.4315 - acc: 0.814 - ETA: 1:17 - loss: 0.4322 - acc: 0.814 - ETA: 1:12 - loss: 0.4323 - acc: 0.814 - ETA: 1:07 - loss: 0.4319 - acc: 0.814 - ETA: 1:02 - loss: 0.4315 - acc: 0.814 - ETA: 57s - loss: 0.4330 - acc: 0.812 - ETA: 53s - loss: 0.4325 - acc: 0.81 - ETA: 48s - loss: 0.4324 - acc: 0.81 - ETA: 43s - loss: 0.4321 - acc: 0.81 - ETA: 38s - loss: 0.4321 - acc: 0.81 - ETA: 33s - loss: 0.4327 - acc: 0.81 - ETA: 28s - loss: 0.4327 - acc: 0.81 - ETA: 24s - loss: 0.4319 - acc: 0.81 - ETA: 19s - loss: 0.4314 - acc: 0.81 - ETA: 14s - loss: 0.4312 - acc: 0.81 - ETA: 9s - loss: 0.4317 - acc: 0.8135 - ETA: 4s - loss: 0.4308 - acc: 0.8140\n",
      "Epoch 00039: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.4296 - acc: 0.8153 - val_loss: 0.6413 - val_acc: 0.6698\n",
      "Epoch 40/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.5164 - acc: 0.718 - ETA: 6:05 - loss: 0.4895 - acc: 0.739 - ETA: 6:01 - loss: 0.4747 - acc: 0.753 - ETA: 5:56 - loss: 0.4570 - acc: 0.781 - ETA: 5:51 - loss: 0.4375 - acc: 0.797 - ETA: 5:46 - loss: 0.4260 - acc: 0.809 - ETA: 5:41 - loss: 0.4216 - acc: 0.819 - ETA: 5:37 - loss: 0.4213 - acc: 0.820 - ETA: 5:32 - loss: 0.4189 - acc: 0.826 - ETA: 5:27 - loss: 0.4194 - acc: 0.825 - ETA: 5:22 - loss: 0.4190 - acc: 0.827 - ETA: 5:17 - loss: 0.4175 - acc: 0.829 - ETA: 5:13 - loss: 0.4258 - acc: 0.821 - ETA: 5:08 - loss: 0.4269 - acc: 0.821 - ETA: 5:03 - loss: 0.4211 - acc: 0.827 - ETA: 4:58 - loss: 0.4169 - acc: 0.830 - ETA: 4:53 - loss: 0.4180 - acc: 0.827 - ETA: 4:49 - loss: 0.4189 - acc: 0.827 - ETA: 4:44 - loss: 0.4148 - acc: 0.831 - ETA: 4:39 - loss: 0.4170 - acc: 0.830 - ETA: 4:34 - loss: 0.4176 - acc: 0.831 - ETA: 4:29 - loss: 0.4177 - acc: 0.831 - ETA: 4:25 - loss: 0.4166 - acc: 0.832 - ETA: 4:20 - loss: 0.4176 - acc: 0.831 - ETA: 4:15 - loss: 0.4173 - acc: 0.830 - ETA: 4:10 - loss: 0.4202 - acc: 0.826 - ETA: 4:05 - loss: 0.4190 - acc: 0.828 - ETA: 4:01 - loss: 0.4191 - acc: 0.827 - ETA: 3:56 - loss: 0.4168 - acc: 0.828 - ETA: 3:51 - loss: 0.4171 - acc: 0.828 - ETA: 3:46 - loss: 0.4181 - acc: 0.826 - ETA: 3:41 - loss: 0.4167 - acc: 0.827 - ETA: 3:36 - loss: 0.4160 - acc: 0.827 - ETA: 3:32 - loss: 0.4167 - acc: 0.826 - ETA: 3:27 - loss: 0.4170 - acc: 0.826 - ETA: 3:22 - loss: 0.4149 - acc: 0.828 - ETA: 3:13 - loss: 0.4119 - acc: 0.828 - ETA: 3:08 - loss: 0.4118 - acc: 0.828 - ETA: 3:03 - loss: 0.4103 - acc: 0.830 - ETA: 2:59 - loss: 0.4088 - acc: 0.832 - ETA: 2:54 - loss: 0.4097 - acc: 0.831 - ETA: 2:49 - loss: 0.4107 - acc: 0.831 - ETA: 2:45 - loss: 0.4105 - acc: 0.832 - ETA: 2:40 - loss: 0.4110 - acc: 0.832 - ETA: 2:36 - loss: 0.4115 - acc: 0.832 - ETA: 2:31 - loss: 0.4117 - acc: 0.832 - ETA: 2:26 - loss: 0.4119 - acc: 0.831 - ETA: 2:22 - loss: 0.4126 - acc: 0.830 - ETA: 2:17 - loss: 0.4117 - acc: 0.831 - ETA: 2:12 - loss: 0.4137 - acc: 0.829 - ETA: 2:07 - loss: 0.4147 - acc: 0.828 - ETA: 2:03 - loss: 0.4138 - acc: 0.829 - ETA: 1:58 - loss: 0.4137 - acc: 0.830 - ETA: 1:53 - loss: 0.4141 - acc: 0.829 - ETA: 1:49 - loss: 0.4142 - acc: 0.829 - ETA: 1:44 - loss: 0.4148 - acc: 0.829 - ETA: 1:39 - loss: 0.4143 - acc: 0.830 - ETA: 1:34 - loss: 0.4156 - acc: 0.829 - ETA: 1:30 - loss: 0.4162 - acc: 0.828 - ETA: 1:25 - loss: 0.4173 - acc: 0.827 - ETA: 1:20 - loss: 0.4181 - acc: 0.827 - ETA: 1:16 - loss: 0.4189 - acc: 0.826 - ETA: 1:11 - loss: 0.4188 - acc: 0.826 - ETA: 1:05 - loss: 0.4180 - acc: 0.827 - ETA: 1:01 - loss: 0.4192 - acc: 0.826 - ETA: 56s - loss: 0.4195 - acc: 0.826 - ETA: 51s - loss: 0.4198 - acc: 0.82 - ETA: 46s - loss: 0.4203 - acc: 0.82 - ETA: 42s - loss: 0.4200 - acc: 0.82 - ETA: 37s - loss: 0.4204 - acc: 0.82 - ETA: 32s - loss: 0.4207 - acc: 0.82 - ETA: 28s - loss: 0.4210 - acc: 0.82 - ETA: 23s - loss: 0.4203 - acc: 0.82 - ETA: 18s - loss: 0.4199 - acc: 0.82 - ETA: 14s - loss: 0.4194 - acc: 0.82 - ETA: 9s - loss: 0.4192 - acc: 0.8268 - ETA: 4s - loss: 0.4193 - acc: 0.8264\n",
      "Epoch 00040: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.4196 - acc: 0.8262 - val_loss: 0.6431 - val_acc: 0.6641\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3878 - acc: 0.854 - ETA: 6:06 - loss: 0.3753 - acc: 0.875 - ETA: 6:01 - loss: 0.3801 - acc: 0.868 - ETA: 5:56 - loss: 0.3769 - acc: 0.864 - ETA: 5:51 - loss: 0.3885 - acc: 0.856 - ETA: 5:47 - loss: 0.3929 - acc: 0.850 - ETA: 5:42 - loss: 0.3940 - acc: 0.845 - ETA: 5:37 - loss: 0.3981 - acc: 0.845 - ETA: 5:32 - loss: 0.4040 - acc: 0.840 - ETA: 5:27 - loss: 0.4062 - acc: 0.837 - ETA: 5:23 - loss: 0.4047 - acc: 0.837 - ETA: 5:18 - loss: 0.4010 - acc: 0.842 - ETA: 5:13 - loss: 0.4065 - acc: 0.838 - ETA: 5:08 - loss: 0.4095 - acc: 0.835 - ETA: 5:03 - loss: 0.4146 - acc: 0.831 - ETA: 4:59 - loss: 0.4163 - acc: 0.829 - ETA: 4:54 - loss: 0.4135 - acc: 0.832 - ETA: 4:49 - loss: 0.4121 - acc: 0.834 - ETA: 4:44 - loss: 0.4108 - acc: 0.837 - ETA: 4:39 - loss: 0.4113 - acc: 0.837 - ETA: 4:34 - loss: 0.4128 - acc: 0.834 - ETA: 4:30 - loss: 0.4164 - acc: 0.831 - ETA: 4:25 - loss: 0.4164 - acc: 0.828 - ETA: 4:20 - loss: 0.4203 - acc: 0.824 - ETA: 4:15 - loss: 0.4185 - acc: 0.827 - ETA: 4:10 - loss: 0.4180 - acc: 0.826 - ETA: 4:06 - loss: 0.4182 - acc: 0.827 - ETA: 4:01 - loss: 0.4181 - acc: 0.827 - ETA: 3:56 - loss: 0.4181 - acc: 0.827 - ETA: 3:51 - loss: 0.4181 - acc: 0.826 - ETA: 3:46 - loss: 0.4173 - acc: 0.827 - ETA: 3:41 - loss: 0.4182 - acc: 0.827 - ETA: 3:37 - loss: 0.4190 - acc: 0.825 - ETA: 3:32 - loss: 0.4193 - acc: 0.825 - ETA: 3:27 - loss: 0.4206 - acc: 0.823 - ETA: 3:22 - loss: 0.4210 - acc: 0.823 - ETA: 3:17 - loss: 0.4203 - acc: 0.823 - ETA: 3:12 - loss: 0.4201 - acc: 0.824 - ETA: 3:08 - loss: 0.4206 - acc: 0.823 - ETA: 3:03 - loss: 0.4206 - acc: 0.823 - ETA: 2:58 - loss: 0.4197 - acc: 0.823 - ETA: 2:53 - loss: 0.4180 - acc: 0.825 - ETA: 2:48 - loss: 0.4192 - acc: 0.823 - ETA: 2:43 - loss: 0.4193 - acc: 0.824 - ETA: 2:39 - loss: 0.4189 - acc: 0.825 - ETA: 2:34 - loss: 0.4184 - acc: 0.825 - ETA: 2:29 - loss: 0.4186 - acc: 0.825 - ETA: 2:24 - loss: 0.4172 - acc: 0.826 - ETA: 2:19 - loss: 0.4167 - acc: 0.827 - ETA: 2:15 - loss: 0.4167 - acc: 0.826 - ETA: 2:10 - loss: 0.4170 - acc: 0.826 - ETA: 2:05 - loss: 0.4171 - acc: 0.826 - ETA: 2:00 - loss: 0.4175 - acc: 0.826 - ETA: 1:55 - loss: 0.4171 - acc: 0.826 - ETA: 1:50 - loss: 0.4174 - acc: 0.826 - ETA: 1:46 - loss: 0.4166 - acc: 0.827 - ETA: 1:41 - loss: 0.4163 - acc: 0.828 - ETA: 1:36 - loss: 0.4166 - acc: 0.827 - ETA: 1:31 - loss: 0.4162 - acc: 0.827 - ETA: 1:26 - loss: 0.4174 - acc: 0.827 - ETA: 1:21 - loss: 0.4179 - acc: 0.826 - ETA: 1:17 - loss: 0.4181 - acc: 0.826 - ETA: 1:12 - loss: 0.4185 - acc: 0.826 - ETA: 1:07 - loss: 0.4191 - acc: 0.825 - ETA: 1:02 - loss: 0.4185 - acc: 0.826 - ETA: 57s - loss: 0.4191 - acc: 0.825 - ETA: 53s - loss: 0.4193 - acc: 0.82 - ETA: 48s - loss: 0.4182 - acc: 0.82 - ETA: 43s - loss: 0.4200 - acc: 0.82 - ETA: 38s - loss: 0.4198 - acc: 0.82 - ETA: 33s - loss: 0.4200 - acc: 0.82 - ETA: 28s - loss: 0.4201 - acc: 0.82 - ETA: 24s - loss: 0.4206 - acc: 0.82 - ETA: 19s - loss: 0.4202 - acc: 0.82 - ETA: 14s - loss: 0.4208 - acc: 0.82 - ETA: 9s - loss: 0.4208 - acc: 0.8225 - ETA: 4s - loss: 0.4207 - acc: 0.8228\n",
      "Epoch 00041: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.4198 - acc: 0.8240 - val_loss: 0.6559 - val_acc: 0.6604\n",
      "Epoch 42/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4206 - acc: 0.822 - ETA: 6:05 - loss: 0.3956 - acc: 0.833 - ETA: 6:00 - loss: 0.3837 - acc: 0.843 - ETA: 5:56 - loss: 0.3846 - acc: 0.846 - ETA: 5:52 - loss: 0.3961 - acc: 0.833 - ETA: 5:47 - loss: 0.3932 - acc: 0.840 - ETA: 5:42 - loss: 0.3952 - acc: 0.843 - ETA: 5:37 - loss: 0.3911 - acc: 0.849 - ETA: 5:00 - loss: 0.3932 - acc: 0.847 - ETA: 4:59 - loss: 0.3940 - acc: 0.846 - ETA: 4:57 - loss: 0.3954 - acc: 0.845 - ETA: 4:55 - loss: 0.4024 - acc: 0.836 - ETA: 4:52 - loss: 0.4032 - acc: 0.835 - ETA: 4:49 - loss: 0.4040 - acc: 0.834 - ETA: 4:46 - loss: 0.4056 - acc: 0.832 - ETA: 4:42 - loss: 0.4072 - acc: 0.833 - ETA: 4:39 - loss: 0.4091 - acc: 0.830 - ETA: 4:35 - loss: 0.4125 - acc: 0.828 - ETA: 4:31 - loss: 0.4139 - acc: 0.826 - ETA: 4:27 - loss: 0.4137 - acc: 0.828 - ETA: 4:23 - loss: 0.4120 - acc: 0.830 - ETA: 4:19 - loss: 0.4096 - acc: 0.831 - ETA: 4:15 - loss: 0.4118 - acc: 0.832 - ETA: 4:11 - loss: 0.4119 - acc: 0.831 - ETA: 4:06 - loss: 0.4125 - acc: 0.830 - ETA: 4:02 - loss: 0.4108 - acc: 0.832 - ETA: 3:58 - loss: 0.4097 - acc: 0.832 - ETA: 3:53 - loss: 0.4126 - acc: 0.830 - ETA: 3:49 - loss: 0.4126 - acc: 0.830 - ETA: 3:44 - loss: 0.4135 - acc: 0.829 - ETA: 3:40 - loss: 0.4122 - acc: 0.830 - ETA: 3:35 - loss: 0.4143 - acc: 0.828 - ETA: 3:31 - loss: 0.4130 - acc: 0.830 - ETA: 3:26 - loss: 0.4132 - acc: 0.829 - ETA: 3:22 - loss: 0.4127 - acc: 0.829 - ETA: 3:17 - loss: 0.4149 - acc: 0.827 - ETA: 3:13 - loss: 0.4142 - acc: 0.827 - ETA: 3:08 - loss: 0.4135 - acc: 0.828 - ETA: 3:03 - loss: 0.4140 - acc: 0.829 - ETA: 2:59 - loss: 0.4137 - acc: 0.828 - ETA: 2:54 - loss: 0.4146 - acc: 0.827 - ETA: 2:50 - loss: 0.4139 - acc: 0.828 - ETA: 2:45 - loss: 0.4142 - acc: 0.828 - ETA: 2:40 - loss: 0.4127 - acc: 0.830 - ETA: 2:36 - loss: 0.4137 - acc: 0.828 - ETA: 2:31 - loss: 0.4147 - acc: 0.827 - ETA: 2:26 - loss: 0.4135 - acc: 0.828 - ETA: 2:22 - loss: 0.4135 - acc: 0.827 - ETA: 2:17 - loss: 0.4143 - acc: 0.827 - ETA: 2:12 - loss: 0.4144 - acc: 0.827 - ETA: 2:08 - loss: 0.4151 - acc: 0.826 - ETA: 2:03 - loss: 0.4157 - acc: 0.825 - ETA: 1:58 - loss: 0.4151 - acc: 0.826 - ETA: 1:53 - loss: 0.4149 - acc: 0.826 - ETA: 1:49 - loss: 0.4150 - acc: 0.826 - ETA: 1:44 - loss: 0.4147 - acc: 0.826 - ETA: 1:39 - loss: 0.4154 - acc: 0.826 - ETA: 1:35 - loss: 0.4153 - acc: 0.826 - ETA: 1:30 - loss: 0.4145 - acc: 0.827 - ETA: 1:25 - loss: 0.4145 - acc: 0.827 - ETA: 1:20 - loss: 0.4143 - acc: 0.827 - ETA: 1:16 - loss: 0.4145 - acc: 0.827 - ETA: 1:11 - loss: 0.4142 - acc: 0.827 - ETA: 1:06 - loss: 0.4142 - acc: 0.827 - ETA: 1:01 - loss: 0.4136 - acc: 0.827 - ETA: 57s - loss: 0.4141 - acc: 0.827 - ETA: 52s - loss: 0.4141 - acc: 0.82 - ETA: 47s - loss: 0.4142 - acc: 0.82 - ETA: 42s - loss: 0.4143 - acc: 0.82 - ETA: 38s - loss: 0.4144 - acc: 0.82 - ETA: 33s - loss: 0.4140 - acc: 0.82 - ETA: 28s - loss: 0.4141 - acc: 0.82 - ETA: 23s - loss: 0.4149 - acc: 0.82 - ETA: 19s - loss: 0.4147 - acc: 0.82 - ETA: 14s - loss: 0.4154 - acc: 0.82 - ETA: 9s - loss: 0.4147 - acc: 0.8272 - ETA: 4s - loss: 0.4149 - acc: 0.8267\n",
      "Epoch 00042: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4147 - acc: 0.8268 - val_loss: 0.6459 - val_acc: 0.6812\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4080 - acc: 0.822 - ETA: 6:06 - loss: 0.3929 - acc: 0.828 - ETA: 6:01 - loss: 0.4148 - acc: 0.812 - ETA: 5:56 - loss: 0.4209 - acc: 0.809 - ETA: 5:51 - loss: 0.4143 - acc: 0.816 - ETA: 5:46 - loss: 0.4139 - acc: 0.819 - ETA: 5:42 - loss: 0.4080 - acc: 0.827 - ETA: 5:37 - loss: 0.3998 - acc: 0.833 - ETA: 5:32 - loss: 0.3985 - acc: 0.838 - ETA: 5:27 - loss: 0.4018 - acc: 0.838 - ETA: 5:22 - loss: 0.3991 - acc: 0.842 - ETA: 5:18 - loss: 0.3971 - acc: 0.844 - ETA: 5:13 - loss: 0.3982 - acc: 0.845 - ETA: 5:08 - loss: 0.4010 - acc: 0.843 - ETA: 5:03 - loss: 0.4021 - acc: 0.840 - ETA: 4:58 - loss: 0.4037 - acc: 0.837 - ETA: 4:54 - loss: 0.4079 - acc: 0.830 - ETA: 4:49 - loss: 0.4085 - acc: 0.830 - ETA: 4:44 - loss: 0.4133 - acc: 0.827 - ETA: 4:39 - loss: 0.4122 - acc: 0.829 - ETA: 4:34 - loss: 0.4156 - acc: 0.827 - ETA: 4:30 - loss: 0.4154 - acc: 0.827 - ETA: 4:25 - loss: 0.4158 - acc: 0.827 - ETA: 4:20 - loss: 0.4154 - acc: 0.827 - ETA: 4:15 - loss: 0.4148 - acc: 0.828 - ETA: 4:10 - loss: 0.4138 - acc: 0.829 - ETA: 4:05 - loss: 0.4133 - acc: 0.829 - ETA: 4:01 - loss: 0.4146 - acc: 0.828 - ETA: 3:56 - loss: 0.4124 - acc: 0.828 - ETA: 3:51 - loss: 0.4102 - acc: 0.830 - ETA: 3:46 - loss: 0.4121 - acc: 0.828 - ETA: 3:41 - loss: 0.4135 - acc: 0.828 - ETA: 3:37 - loss: 0.4126 - acc: 0.829 - ETA: 3:32 - loss: 0.4150 - acc: 0.828 - ETA: 3:27 - loss: 0.4164 - acc: 0.828 - ETA: 3:17 - loss: 0.4161 - acc: 0.829 - ETA: 3:13 - loss: 0.4153 - acc: 0.830 - ETA: 3:08 - loss: 0.4142 - acc: 0.830 - ETA: 3:03 - loss: 0.4134 - acc: 0.831 - ETA: 2:59 - loss: 0.4151 - acc: 0.829 - ETA: 2:54 - loss: 0.4164 - acc: 0.829 - ETA: 2:50 - loss: 0.4158 - acc: 0.830 - ETA: 2:45 - loss: 0.4157 - acc: 0.830 - ETA: 2:40 - loss: 0.4160 - acc: 0.829 - ETA: 2:36 - loss: 0.4154 - acc: 0.830 - ETA: 2:31 - loss: 0.4145 - acc: 0.831 - ETA: 2:26 - loss: 0.4143 - acc: 0.831 - ETA: 2:22 - loss: 0.4144 - acc: 0.831 - ETA: 2:17 - loss: 0.4141 - acc: 0.831 - ETA: 2:12 - loss: 0.4137 - acc: 0.832 - ETA: 2:07 - loss: 0.4133 - acc: 0.832 - ETA: 2:03 - loss: 0.4134 - acc: 0.831 - ETA: 1:58 - loss: 0.4141 - acc: 0.831 - ETA: 1:53 - loss: 0.4139 - acc: 0.832 - ETA: 1:49 - loss: 0.4140 - acc: 0.831 - ETA: 1:44 - loss: 0.4149 - acc: 0.829 - ETA: 1:38 - loss: 0.4137 - acc: 0.832 - ETA: 1:33 - loss: 0.4132 - acc: 0.833 - ETA: 1:28 - loss: 0.4141 - acc: 0.832 - ETA: 1:24 - loss: 0.4136 - acc: 0.832 - ETA: 1:19 - loss: 0.4124 - acc: 0.833 - ETA: 1:15 - loss: 0.4117 - acc: 0.834 - ETA: 1:10 - loss: 0.4123 - acc: 0.833 - ETA: 1:05 - loss: 0.4134 - acc: 0.832 - ETA: 1:01 - loss: 0.4139 - acc: 0.832 - ETA: 56s - loss: 0.4135 - acc: 0.832 - ETA: 51s - loss: 0.4130 - acc: 0.83 - ETA: 47s - loss: 0.4129 - acc: 0.83 - ETA: 42s - loss: 0.4129 - acc: 0.83 - ETA: 37s - loss: 0.4130 - acc: 0.83 - ETA: 32s - loss: 0.4131 - acc: 0.83 - ETA: 28s - loss: 0.4127 - acc: 0.83 - ETA: 23s - loss: 0.4134 - acc: 0.83 - ETA: 18s - loss: 0.4132 - acc: 0.83 - ETA: 14s - loss: 0.4120 - acc: 0.83 - ETA: 9s - loss: 0.4120 - acc: 0.8331 - ETA: 4s - loss: 0.4117 - acc: 0.8335\n",
      "Epoch 00043: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.4114 - acc: 0.8340 - val_loss: 0.6444 - val_acc: 0.6654\n",
      "Epoch 44/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.4800 - acc: 0.750 - ETA: 6:06 - loss: 0.4611 - acc: 0.765 - ETA: 6:01 - loss: 0.4625 - acc: 0.767 - ETA: 5:56 - loss: 0.4368 - acc: 0.796 - ETA: 5:51 - loss: 0.4488 - acc: 0.789 - ETA: 5:47 - loss: 0.4336 - acc: 0.798 - ETA: 5:42 - loss: 0.4233 - acc: 0.815 - ETA: 5:37 - loss: 0.4159 - acc: 0.824 - ETA: 5:32 - loss: 0.4221 - acc: 0.816 - ETA: 5:27 - loss: 0.4141 - acc: 0.822 - ETA: 5:23 - loss: 0.4127 - acc: 0.822 - ETA: 5:18 - loss: 0.4139 - acc: 0.819 - ETA: 5:13 - loss: 0.4115 - acc: 0.822 - ETA: 5:08 - loss: 0.4125 - acc: 0.823 - ETA: 5:03 - loss: 0.4158 - acc: 0.823 - ETA: 4:58 - loss: 0.4125 - acc: 0.826 - ETA: 4:54 - loss: 0.4137 - acc: 0.826 - ETA: 4:49 - loss: 0.4125 - acc: 0.827 - ETA: 4:44 - loss: 0.4142 - acc: 0.826 - ETA: 4:39 - loss: 0.4138 - acc: 0.827 - ETA: 4:34 - loss: 0.4125 - acc: 0.829 - ETA: 4:30 - loss: 0.4141 - acc: 0.828 - ETA: 4:25 - loss: 0.4121 - acc: 0.830 - ETA: 4:20 - loss: 0.4145 - acc: 0.828 - ETA: 4:15 - loss: 0.4161 - acc: 0.826 - ETA: 4:10 - loss: 0.4159 - acc: 0.826 - ETA: 4:05 - loss: 0.4167 - acc: 0.826 - ETA: 4:01 - loss: 0.4156 - acc: 0.827 - ETA: 3:56 - loss: 0.4154 - acc: 0.826 - ETA: 3:51 - loss: 0.4169 - acc: 0.824 - ETA: 3:46 - loss: 0.4165 - acc: 0.824 - ETA: 3:41 - loss: 0.4190 - acc: 0.821 - ETA: 3:36 - loss: 0.4189 - acc: 0.821 - ETA: 3:32 - loss: 0.4179 - acc: 0.822 - ETA: 3:27 - loss: 0.4171 - acc: 0.822 - ETA: 3:22 - loss: 0.4168 - acc: 0.823 - ETA: 3:17 - loss: 0.4164 - acc: 0.823 - ETA: 3:12 - loss: 0.4167 - acc: 0.823 - ETA: 3:08 - loss: 0.4189 - acc: 0.821 - ETA: 3:03 - loss: 0.4183 - acc: 0.822 - ETA: 2:58 - loss: 0.4173 - acc: 0.823 - ETA: 2:53 - loss: 0.4164 - acc: 0.824 - ETA: 2:48 - loss: 0.4178 - acc: 0.823 - ETA: 2:43 - loss: 0.4185 - acc: 0.822 - ETA: 2:39 - loss: 0.4179 - acc: 0.822 - ETA: 2:34 - loss: 0.4176 - acc: 0.822 - ETA: 2:29 - loss: 0.4180 - acc: 0.822 - ETA: 2:24 - loss: 0.4164 - acc: 0.823 - ETA: 2:19 - loss: 0.4153 - acc: 0.825 - ETA: 2:15 - loss: 0.4151 - acc: 0.825 - ETA: 2:10 - loss: 0.4147 - acc: 0.826 - ETA: 2:05 - loss: 0.4141 - acc: 0.826 - ETA: 2:00 - loss: 0.4146 - acc: 0.826 - ETA: 1:55 - loss: 0.4144 - acc: 0.826 - ETA: 1:50 - loss: 0.4144 - acc: 0.826 - ETA: 1:46 - loss: 0.4153 - acc: 0.825 - ETA: 1:41 - loss: 0.4153 - acc: 0.825 - ETA: 1:36 - loss: 0.4148 - acc: 0.826 - ETA: 1:31 - loss: 0.4158 - acc: 0.824 - ETA: 1:26 - loss: 0.4156 - acc: 0.824 - ETA: 1:21 - loss: 0.4156 - acc: 0.824 - ETA: 1:17 - loss: 0.4159 - acc: 0.824 - ETA: 1:12 - loss: 0.4162 - acc: 0.824 - ETA: 1:07 - loss: 0.4165 - acc: 0.823 - ETA: 1:02 - loss: 0.4173 - acc: 0.822 - ETA: 57s - loss: 0.4173 - acc: 0.822 - ETA: 53s - loss: 0.4177 - acc: 0.82 - ETA: 48s - loss: 0.4173 - acc: 0.82 - ETA: 43s - loss: 0.4165 - acc: 0.82 - ETA: 38s - loss: 0.4172 - acc: 0.82 - ETA: 33s - loss: 0.4168 - acc: 0.82 - ETA: 28s - loss: 0.4180 - acc: 0.82 - ETA: 23s - loss: 0.4179 - acc: 0.82 - ETA: 19s - loss: 0.4179 - acc: 0.82 - ETA: 14s - loss: 0.4185 - acc: 0.82 - ETA: 9s - loss: 0.4181 - acc: 0.8221 - ETA: 4s - loss: 0.4180 - acc: 0.8220\n",
      "Epoch 00044: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4184 - acc: 0.8217 - val_loss: 0.6481 - val_acc: 0.6761\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3994 - acc: 0.864 - ETA: 6:05 - loss: 0.3813 - acc: 0.890 - ETA: 6:01 - loss: 0.3706 - acc: 0.878 - ETA: 5:56 - loss: 0.3927 - acc: 0.859 - ETA: 5:51 - loss: 0.3938 - acc: 0.856 - ETA: 5:47 - loss: 0.3788 - acc: 0.871 - ETA: 5:42 - loss: 0.3921 - acc: 0.857 - ETA: 5:37 - loss: 0.3935 - acc: 0.852 - ETA: 5:32 - loss: 0.3897 - acc: 0.855 - ETA: 5:27 - loss: 0.3954 - acc: 0.846 - ETA: 5:22 - loss: 0.3920 - acc: 0.846 - ETA: 5:18 - loss: 0.3967 - acc: 0.842 - ETA: 5:13 - loss: 0.3995 - acc: 0.840 - ETA: 5:08 - loss: 0.4006 - acc: 0.840 - ETA: 5:03 - loss: 0.3999 - acc: 0.840 - ETA: 4:58 - loss: 0.3999 - acc: 0.840 - ETA: 4:54 - loss: 0.3984 - acc: 0.840 - ETA: 4:49 - loss: 0.3986 - acc: 0.842 - ETA: 4:44 - loss: 0.3972 - acc: 0.842 - ETA: 4:39 - loss: 0.3977 - acc: 0.842 - ETA: 4:34 - loss: 0.3966 - acc: 0.842 - ETA: 4:30 - loss: 0.3957 - acc: 0.842 - ETA: 4:25 - loss: 0.3944 - acc: 0.843 - ETA: 4:20 - loss: 0.3951 - acc: 0.843 - ETA: 4:15 - loss: 0.3956 - acc: 0.843 - ETA: 4:10 - loss: 0.3960 - acc: 0.843 - ETA: 4:06 - loss: 0.3953 - acc: 0.843 - ETA: 4:01 - loss: 0.3941 - acc: 0.844 - ETA: 3:56 - loss: 0.3929 - acc: 0.845 - ETA: 3:51 - loss: 0.3933 - acc: 0.846 - ETA: 3:46 - loss: 0.3926 - acc: 0.846 - ETA: 3:41 - loss: 0.3907 - acc: 0.849 - ETA: 3:37 - loss: 0.3894 - acc: 0.850 - ETA: 3:32 - loss: 0.3908 - acc: 0.849 - ETA: 3:27 - loss: 0.3906 - acc: 0.849 - ETA: 3:22 - loss: 0.3928 - acc: 0.846 - ETA: 3:17 - loss: 0.3919 - acc: 0.847 - ETA: 3:12 - loss: 0.3924 - acc: 0.847 - ETA: 3:08 - loss: 0.3944 - acc: 0.845 - ETA: 3:03 - loss: 0.3938 - acc: 0.845 - ETA: 2:58 - loss: 0.3927 - acc: 0.847 - ETA: 2:53 - loss: 0.3931 - acc: 0.847 - ETA: 2:48 - loss: 0.3953 - acc: 0.845 - ETA: 2:43 - loss: 0.3970 - acc: 0.844 - ETA: 2:39 - loss: 0.3953 - acc: 0.845 - ETA: 2:34 - loss: 0.3965 - acc: 0.844 - ETA: 2:29 - loss: 0.3967 - acc: 0.844 - ETA: 2:24 - loss: 0.3969 - acc: 0.844 - ETA: 2:19 - loss: 0.3966 - acc: 0.844 - ETA: 2:15 - loss: 0.3980 - acc: 0.843 - ETA: 2:10 - loss: 0.3981 - acc: 0.843 - ETA: 2:05 - loss: 0.3997 - acc: 0.842 - ETA: 2:00 - loss: 0.4004 - acc: 0.841 - ETA: 1:55 - loss: 0.4010 - acc: 0.840 - ETA: 1:50 - loss: 0.4008 - acc: 0.840 - ETA: 1:46 - loss: 0.4019 - acc: 0.839 - ETA: 1:41 - loss: 0.4029 - acc: 0.837 - ETA: 1:36 - loss: 0.4024 - acc: 0.838 - ETA: 1:31 - loss: 0.4031 - acc: 0.837 - ETA: 1:26 - loss: 0.4037 - acc: 0.837 - ETA: 1:21 - loss: 0.4040 - acc: 0.836 - ETA: 1:17 - loss: 0.4038 - acc: 0.836 - ETA: 1:12 - loss: 0.4047 - acc: 0.835 - ETA: 1:07 - loss: 0.4052 - acc: 0.834 - ETA: 1:02 - loss: 0.4055 - acc: 0.834 - ETA: 57s - loss: 0.4059 - acc: 0.834 - ETA: 53s - loss: 0.4054 - acc: 0.83 - ETA: 48s - loss: 0.4047 - acc: 0.83 - ETA: 43s - loss: 0.4051 - acc: 0.83 - ETA: 38s - loss: 0.4051 - acc: 0.83 - ETA: 33s - loss: 0.4059 - acc: 0.83 - ETA: 28s - loss: 0.4061 - acc: 0.83 - ETA: 24s - loss: 0.4065 - acc: 0.83 - ETA: 19s - loss: 0.4060 - acc: 0.83 - ETA: 14s - loss: 0.4050 - acc: 0.83 - ETA: 9s - loss: 0.4060 - acc: 0.8324 - ETA: 4s - loss: 0.4055 - acc: 0.8328\n",
      "Epoch 00045: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.4050 - acc: 0.8333 - val_loss: 0.6472 - val_acc: 0.6742\n",
      "Epoch 46/100\n",
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.4197 - acc: 0.791 - ETA: 6:05 - loss: 0.4268 - acc: 0.812 - ETA: 6:01 - loss: 0.4356 - acc: 0.791 - ETA: 5:56 - loss: 0.4356 - acc: 0.794 - ETA: 5:51 - loss: 0.4324 - acc: 0.793 - ETA: 5:47 - loss: 0.4228 - acc: 0.798 - ETA: 5:42 - loss: 0.4113 - acc: 0.811 - ETA: 5:01 - loss: 0.4198 - acc: 0.793 - ETA: 5:01 - loss: 0.4191 - acc: 0.794 - ETA: 4:59 - loss: 0.4175 - acc: 0.797 - ETA: 4:57 - loss: 0.4159 - acc: 0.803 - ETA: 4:55 - loss: 0.4166 - acc: 0.806 - ETA: 4:52 - loss: 0.4118 - acc: 0.811 - ETA: 4:49 - loss: 0.4106 - acc: 0.812 - ETA: 4:46 - loss: 0.4098 - acc: 0.813 - ETA: 4:42 - loss: 0.4123 - acc: 0.811 - ETA: 4:39 - loss: 0.4114 - acc: 0.812 - ETA: 4:35 - loss: 0.4116 - acc: 0.814 - ETA: 4:31 - loss: 0.4068 - acc: 0.818 - ETA: 4:27 - loss: 0.4093 - acc: 0.817 - ETA: 4:23 - loss: 0.4117 - acc: 0.818 - ETA: 4:19 - loss: 0.4100 - acc: 0.820 - ETA: 4:15 - loss: 0.4082 - acc: 0.822 - ETA: 4:11 - loss: 0.4089 - acc: 0.820 - ETA: 4:06 - loss: 0.4091 - acc: 0.821 - ETA: 4:02 - loss: 0.4088 - acc: 0.821 - ETA: 3:58 - loss: 0.4090 - acc: 0.822 - ETA: 3:53 - loss: 0.4099 - acc: 0.822 - ETA: 3:49 - loss: 0.4083 - acc: 0.822 - ETA: 3:44 - loss: 0.4089 - acc: 0.822 - ETA: 3:40 - loss: 0.4075 - acc: 0.824 - ETA: 3:35 - loss: 0.4066 - acc: 0.824 - ETA: 3:31 - loss: 0.4090 - acc: 0.823 - ETA: 3:26 - loss: 0.4058 - acc: 0.826 - ETA: 3:22 - loss: 0.4054 - acc: 0.826 - ETA: 3:17 - loss: 0.4046 - acc: 0.826 - ETA: 3:13 - loss: 0.4062 - acc: 0.824 - ETA: 3:08 - loss: 0.4071 - acc: 0.824 - ETA: 3:03 - loss: 0.4075 - acc: 0.823 - ETA: 2:59 - loss: 0.4082 - acc: 0.822 - ETA: 2:54 - loss: 0.4078 - acc: 0.823 - ETA: 2:50 - loss: 0.4070 - acc: 0.824 - ETA: 2:45 - loss: 0.4065 - acc: 0.825 - ETA: 2:40 - loss: 0.4073 - acc: 0.825 - ETA: 2:36 - loss: 0.4080 - acc: 0.824 - ETA: 2:31 - loss: 0.4090 - acc: 0.822 - ETA: 2:26 - loss: 0.4079 - acc: 0.823 - ETA: 2:22 - loss: 0.4089 - acc: 0.822 - ETA: 2:17 - loss: 0.4090 - acc: 0.822 - ETA: 2:12 - loss: 0.4091 - acc: 0.822 - ETA: 2:07 - loss: 0.4095 - acc: 0.822 - ETA: 2:03 - loss: 0.4098 - acc: 0.822 - ETA: 1:58 - loss: 0.4081 - acc: 0.823 - ETA: 1:53 - loss: 0.4076 - acc: 0.824 - ETA: 1:49 - loss: 0.4088 - acc: 0.822 - ETA: 1:44 - loss: 0.4094 - acc: 0.821 - ETA: 1:39 - loss: 0.4093 - acc: 0.822 - ETA: 1:34 - loss: 0.4098 - acc: 0.821 - ETA: 1:30 - loss: 0.4090 - acc: 0.821 - ETA: 1:25 - loss: 0.4089 - acc: 0.821 - ETA: 1:20 - loss: 0.4092 - acc: 0.821 - ETA: 1:16 - loss: 0.4089 - acc: 0.821 - ETA: 1:11 - loss: 0.4083 - acc: 0.822 - ETA: 1:06 - loss: 0.4088 - acc: 0.822 - ETA: 1:01 - loss: 0.4084 - acc: 0.822 - ETA: 57s - loss: 0.4073 - acc: 0.823 - ETA: 52s - loss: 0.4079 - acc: 0.82 - ETA: 47s - loss: 0.4084 - acc: 0.82 - ETA: 42s - loss: 0.4081 - acc: 0.82 - ETA: 38s - loss: 0.4080 - acc: 0.82 - ETA: 33s - loss: 0.4079 - acc: 0.82 - ETA: 28s - loss: 0.4083 - acc: 0.82 - ETA: 23s - loss: 0.4078 - acc: 0.82 - ETA: 19s - loss: 0.4091 - acc: 0.82 - ETA: 14s - loss: 0.4092 - acc: 0.82 - ETA: 9s - loss: 0.4083 - acc: 0.8225 - ETA: 4s - loss: 0.4090 - acc: 0.8221\n",
      "Epoch 00046: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.4080 - acc: 0.8228 - val_loss: 0.6413 - val_acc: 0.6774\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.3226 - acc: 0.875 - ETA: 6:06 - loss: 0.3604 - acc: 0.854 - ETA: 6:01 - loss: 0.3810 - acc: 0.850 - ETA: 5:56 - loss: 0.3917 - acc: 0.856 - ETA: 5:51 - loss: 0.3903 - acc: 0.856 - ETA: 5:46 - loss: 0.3941 - acc: 0.850 - ETA: 5:42 - loss: 0.3931 - acc: 0.852 - ETA: 5:37 - loss: 0.3893 - acc: 0.856 - ETA: 5:32 - loss: 0.3919 - acc: 0.850 - ETA: 5:27 - loss: 0.3985 - acc: 0.840 - ETA: 4:57 - loss: 0.4088 - acc: 0.832 - ETA: 4:55 - loss: 0.4102 - acc: 0.832 - ETA: 4:52 - loss: 0.4122 - acc: 0.833 - ETA: 4:49 - loss: 0.4078 - acc: 0.835 - ETA: 4:46 - loss: 0.4058 - acc: 0.838 - ETA: 4:42 - loss: 0.4086 - acc: 0.834 - ETA: 4:39 - loss: 0.4035 - acc: 0.839 - ETA: 4:35 - loss: 0.4034 - acc: 0.838 - ETA: 4:31 - loss: 0.4019 - acc: 0.838 - ETA: 4:27 - loss: 0.4002 - acc: 0.840 - ETA: 4:23 - loss: 0.4022 - acc: 0.838 - ETA: 4:19 - loss: 0.4005 - acc: 0.840 - ETA: 4:15 - loss: 0.4021 - acc: 0.839 - ETA: 4:10 - loss: 0.4006 - acc: 0.839 - ETA: 4:06 - loss: 0.4004 - acc: 0.839 - ETA: 4:02 - loss: 0.3981 - acc: 0.842 - ETA: 3:57 - loss: 0.3987 - acc: 0.841 - ETA: 3:53 - loss: 0.3984 - acc: 0.841 - ETA: 3:49 - loss: 0.3970 - acc: 0.842 - ETA: 3:44 - loss: 0.3954 - acc: 0.843 - ETA: 3:40 - loss: 0.3966 - acc: 0.842 - ETA: 3:35 - loss: 0.3983 - acc: 0.841 - ETA: 3:31 - loss: 0.3984 - acc: 0.841 - ETA: 3:26 - loss: 0.3975 - acc: 0.842 - ETA: 3:22 - loss: 0.3986 - acc: 0.841 - ETA: 3:17 - loss: 0.3975 - acc: 0.842 - ETA: 3:13 - loss: 0.3964 - acc: 0.843 - ETA: 3:08 - loss: 0.3964 - acc: 0.842 - ETA: 3:03 - loss: 0.3971 - acc: 0.842 - ETA: 2:59 - loss: 0.3959 - acc: 0.842 - ETA: 2:54 - loss: 0.3964 - acc: 0.841 - ETA: 2:49 - loss: 0.3965 - acc: 0.842 - ETA: 2:45 - loss: 0.3972 - acc: 0.840 - ETA: 2:40 - loss: 0.3976 - acc: 0.840 - ETA: 2:36 - loss: 0.3968 - acc: 0.841 - ETA: 2:31 - loss: 0.3965 - acc: 0.841 - ETA: 2:26 - loss: 0.3970 - acc: 0.841 - ETA: 2:22 - loss: 0.3974 - acc: 0.840 - ETA: 2:17 - loss: 0.3980 - acc: 0.840 - ETA: 2:12 - loss: 0.3974 - acc: 0.841 - ETA: 2:07 - loss: 0.3969 - acc: 0.841 - ETA: 2:01 - loss: 0.3968 - acc: 0.841 - ETA: 1:56 - loss: 0.3964 - acc: 0.841 - ETA: 1:51 - loss: 0.3967 - acc: 0.841 - ETA: 1:47 - loss: 0.3975 - acc: 0.840 - ETA: 1:42 - loss: 0.3971 - acc: 0.841 - ETA: 1:38 - loss: 0.3983 - acc: 0.840 - ETA: 1:33 - loss: 0.3976 - acc: 0.840 - ETA: 1:28 - loss: 0.3978 - acc: 0.840 - ETA: 1:24 - loss: 0.3964 - acc: 0.841 - ETA: 1:19 - loss: 0.3963 - acc: 0.841 - ETA: 1:14 - loss: 0.3963 - acc: 0.841 - ETA: 1:10 - loss: 0.3951 - acc: 0.842 - ETA: 1:05 - loss: 0.3939 - acc: 0.843 - ETA: 1:00 - loss: 0.3937 - acc: 0.843 - ETA: 56s - loss: 0.3935 - acc: 0.843 - ETA: 51s - loss: 0.3934 - acc: 0.84 - ETA: 46s - loss: 0.3938 - acc: 0.84 - ETA: 42s - loss: 0.3936 - acc: 0.84 - ETA: 37s - loss: 0.3943 - acc: 0.84 - ETA: 32s - loss: 0.3947 - acc: 0.84 - ETA: 28s - loss: 0.3958 - acc: 0.84 - ETA: 23s - loss: 0.3967 - acc: 0.83 - ETA: 18s - loss: 0.3963 - acc: 0.83 - ETA: 14s - loss: 0.3963 - acc: 0.83 - ETA: 9s - loss: 0.3958 - acc: 0.8405 - ETA: 4s - loss: 0.3964 - acc: 0.8396\n",
      "Epoch 00047: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.3966 - acc: 0.8396 - val_loss: 0.6520 - val_acc: 0.6723\n",
      "Epoch 48/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3302 - acc: 0.916 - ETA: 6:06 - loss: 0.3688 - acc: 0.849 - ETA: 6:01 - loss: 0.3993 - acc: 0.840 - ETA: 5:56 - loss: 0.3926 - acc: 0.843 - ETA: 5:51 - loss: 0.3982 - acc: 0.841 - ETA: 5:46 - loss: 0.3857 - acc: 0.852 - ETA: 5:42 - loss: 0.3832 - acc: 0.851 - ETA: 5:37 - loss: 0.3923 - acc: 0.846 - ETA: 5:32 - loss: 0.3889 - acc: 0.850 - ETA: 5:27 - loss: 0.3918 - acc: 0.844 - ETA: 5:22 - loss: 0.3913 - acc: 0.842 - ETA: 5:18 - loss: 0.3964 - acc: 0.836 - ETA: 5:13 - loss: 0.3917 - acc: 0.839 - ETA: 5:08 - loss: 0.3951 - acc: 0.837 - ETA: 5:03 - loss: 0.3930 - acc: 0.838 - ETA: 4:58 - loss: 0.3919 - acc: 0.840 - ETA: 4:53 - loss: 0.3936 - acc: 0.838 - ETA: 4:49 - loss: 0.3929 - acc: 0.837 - ETA: 4:44 - loss: 0.3950 - acc: 0.836 - ETA: 4:39 - loss: 0.3944 - acc: 0.837 - ETA: 4:34 - loss: 0.3945 - acc: 0.835 - ETA: 4:29 - loss: 0.3942 - acc: 0.835 - ETA: 4:25 - loss: 0.3912 - acc: 0.837 - ETA: 4:20 - loss: 0.3923 - acc: 0.837 - ETA: 4:15 - loss: 0.3920 - acc: 0.835 - ETA: 4:10 - loss: 0.3944 - acc: 0.833 - ETA: 4:05 - loss: 0.3934 - acc: 0.834 - ETA: 4:00 - loss: 0.3964 - acc: 0.831 - ETA: 3:56 - loss: 0.3974 - acc: 0.830 - ETA: 3:51 - loss: 0.3963 - acc: 0.831 - ETA: 3:46 - loss: 0.3943 - acc: 0.833 - ETA: 3:41 - loss: 0.3929 - acc: 0.835 - ETA: 3:36 - loss: 0.3932 - acc: 0.834 - ETA: 3:32 - loss: 0.3959 - acc: 0.831 - ETA: 3:27 - loss: 0.3945 - acc: 0.833 - ETA: 3:22 - loss: 0.3919 - acc: 0.835 - ETA: 3:17 - loss: 0.3926 - acc: 0.835 - ETA: 3:12 - loss: 0.3946 - acc: 0.834 - ETA: 3:07 - loss: 0.3941 - acc: 0.833 - ETA: 3:03 - loss: 0.3942 - acc: 0.833 - ETA: 2:58 - loss: 0.3963 - acc: 0.831 - ETA: 2:53 - loss: 0.3965 - acc: 0.831 - ETA: 2:48 - loss: 0.3962 - acc: 0.831 - ETA: 2:43 - loss: 0.3985 - acc: 0.830 - ETA: 2:39 - loss: 0.3980 - acc: 0.830 - ETA: 2:34 - loss: 0.3967 - acc: 0.831 - ETA: 2:29 - loss: 0.3971 - acc: 0.830 - ETA: 2:24 - loss: 0.3967 - acc: 0.831 - ETA: 2:17 - loss: 0.3972 - acc: 0.831 - ETA: 2:12 - loss: 0.3980 - acc: 0.831 - ETA: 2:07 - loss: 0.3988 - acc: 0.831 - ETA: 2:03 - loss: 0.3981 - acc: 0.832 - ETA: 1:58 - loss: 0.3979 - acc: 0.832 - ETA: 1:53 - loss: 0.3978 - acc: 0.832 - ETA: 1:49 - loss: 0.3975 - acc: 0.832 - ETA: 1:44 - loss: 0.3990 - acc: 0.830 - ETA: 1:39 - loss: 0.3984 - acc: 0.831 - ETA: 1:34 - loss: 0.3983 - acc: 0.830 - ETA: 1:30 - loss: 0.3994 - acc: 0.830 - ETA: 1:25 - loss: 0.4001 - acc: 0.829 - ETA: 1:20 - loss: 0.4005 - acc: 0.829 - ETA: 1:16 - loss: 0.4001 - acc: 0.830 - ETA: 1:11 - loss: 0.3997 - acc: 0.831 - ETA: 1:06 - loss: 0.3984 - acc: 0.832 - ETA: 1:01 - loss: 0.3988 - acc: 0.832 - ETA: 57s - loss: 0.3981 - acc: 0.833 - ETA: 52s - loss: 0.3976 - acc: 0.83 - ETA: 47s - loss: 0.3982 - acc: 0.83 - ETA: 42s - loss: 0.3984 - acc: 0.83 - ETA: 38s - loss: 0.3997 - acc: 0.83 - ETA: 33s - loss: 0.3993 - acc: 0.83 - ETA: 28s - loss: 0.3991 - acc: 0.83 - ETA: 23s - loss: 0.3989 - acc: 0.83 - ETA: 19s - loss: 0.3990 - acc: 0.83 - ETA: 14s - loss: 0.3990 - acc: 0.83 - ETA: 9s - loss: 0.3986 - acc: 0.8329 - ETA: 4s - loss: 0.3995 - acc: 0.8324\n",
      "Epoch 00048: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3997 - acc: 0.8317 - val_loss: 0.6499 - val_acc: 0.6831\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3548 - acc: 0.895 - ETA: 6:05 - loss: 0.4159 - acc: 0.822 - ETA: 6:01 - loss: 0.4062 - acc: 0.822 - ETA: 5:56 - loss: 0.4023 - acc: 0.820 - ETA: 5:51 - loss: 0.3972 - acc: 0.835 - ETA: 5:46 - loss: 0.4025 - acc: 0.829 - ETA: 5:42 - loss: 0.4005 - acc: 0.828 - ETA: 5:37 - loss: 0.4061 - acc: 0.824 - ETA: 5:32 - loss: 0.4033 - acc: 0.831 - ETA: 5:27 - loss: 0.4030 - acc: 0.831 - ETA: 5:22 - loss: 0.4014 - acc: 0.832 - ETA: 5:18 - loss: 0.4053 - acc: 0.830 - ETA: 5:13 - loss: 0.3997 - acc: 0.835 - ETA: 5:08 - loss: 0.3994 - acc: 0.835 - ETA: 5:03 - loss: 0.4003 - acc: 0.834 - ETA: 4:58 - loss: 0.4021 - acc: 0.833 - ETA: 4:54 - loss: 0.4031 - acc: 0.832 - ETA: 4:49 - loss: 0.4048 - acc: 0.829 - ETA: 4:44 - loss: 0.4042 - acc: 0.830 - ETA: 4:39 - loss: 0.4024 - acc: 0.831 - ETA: 4:34 - loss: 0.4037 - acc: 0.827 - ETA: 4:29 - loss: 0.4037 - acc: 0.827 - ETA: 4:25 - loss: 0.4004 - acc: 0.831 - ETA: 4:20 - loss: 0.4015 - acc: 0.830 - ETA: 4:15 - loss: 0.4022 - acc: 0.830 - ETA: 4:10 - loss: 0.4006 - acc: 0.831 - ETA: 4:05 - loss: 0.4000 - acc: 0.832 - ETA: 4:01 - loss: 0.3982 - acc: 0.834 - ETA: 3:56 - loss: 0.3986 - acc: 0.834 - ETA: 3:51 - loss: 0.3995 - acc: 0.833 - ETA: 3:46 - loss: 0.3996 - acc: 0.833 - ETA: 3:41 - loss: 0.3996 - acc: 0.832 - ETA: 3:36 - loss: 0.3996 - acc: 0.833 - ETA: 3:32 - loss: 0.3978 - acc: 0.834 - ETA: 3:27 - loss: 0.3976 - acc: 0.834 - ETA: 3:22 - loss: 0.3967 - acc: 0.835 - ETA: 3:17 - loss: 0.3972 - acc: 0.835 - ETA: 3:12 - loss: 0.3971 - acc: 0.835 - ETA: 3:08 - loss: 0.3969 - acc: 0.834 - ETA: 3:03 - loss: 0.3975 - acc: 0.834 - ETA: 2:58 - loss: 0.3975 - acc: 0.833 - ETA: 2:53 - loss: 0.3967 - acc: 0.834 - ETA: 2:48 - loss: 0.3967 - acc: 0.834 - ETA: 2:43 - loss: 0.3966 - acc: 0.834 - ETA: 2:39 - loss: 0.3959 - acc: 0.836 - ETA: 2:34 - loss: 0.3959 - acc: 0.836 - ETA: 2:29 - loss: 0.3958 - acc: 0.836 - ETA: 2:24 - loss: 0.3947 - acc: 0.836 - ETA: 2:19 - loss: 0.3934 - acc: 0.838 - ETA: 2:14 - loss: 0.3930 - acc: 0.838 - ETA: 2:10 - loss: 0.3926 - acc: 0.838 - ETA: 2:05 - loss: 0.3925 - acc: 0.838 - ETA: 2:00 - loss: 0.3915 - acc: 0.839 - ETA: 1:55 - loss: 0.3921 - acc: 0.839 - ETA: 1:50 - loss: 0.3918 - acc: 0.839 - ETA: 1:46 - loss: 0.3915 - acc: 0.839 - ETA: 1:41 - loss: 0.3916 - acc: 0.839 - ETA: 1:36 - loss: 0.3919 - acc: 0.840 - ETA: 1:31 - loss: 0.3929 - acc: 0.839 - ETA: 1:26 - loss: 0.3932 - acc: 0.838 - ETA: 1:21 - loss: 0.3926 - acc: 0.838 - ETA: 1:17 - loss: 0.3919 - acc: 0.839 - ETA: 1:12 - loss: 0.3925 - acc: 0.838 - ETA: 1:07 - loss: 0.3924 - acc: 0.838 - ETA: 1:02 - loss: 0.3936 - acc: 0.837 - ETA: 57s - loss: 0.3935 - acc: 0.837 - ETA: 53s - loss: 0.3938 - acc: 0.83 - ETA: 48s - loss: 0.3928 - acc: 0.83 - ETA: 43s - loss: 0.3926 - acc: 0.83 - ETA: 38s - loss: 0.3927 - acc: 0.83 - ETA: 33s - loss: 0.3935 - acc: 0.83 - ETA: 28s - loss: 0.3948 - acc: 0.83 - ETA: 24s - loss: 0.3942 - acc: 0.83 - ETA: 19s - loss: 0.3941 - acc: 0.83 - ETA: 14s - loss: 0.3938 - acc: 0.83 - ETA: 9s - loss: 0.3946 - acc: 0.8362 - ETA: 4s - loss: 0.3937 - acc: 0.8369\n",
      "Epoch 00049: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.3937 - acc: 0.8368 - val_loss: 0.6495 - val_acc: 0.6755\n",
      "Epoch 50/100\n",
      "77/78 [============================>.] - ETA: 6:12 - loss: 0.3229 - acc: 0.906 - ETA: 6:07 - loss: 0.3289 - acc: 0.890 - ETA: 6:01 - loss: 0.3558 - acc: 0.871 - ETA: 5:57 - loss: 0.3690 - acc: 0.862 - ETA: 4:51 - loss: 0.4014 - acc: 0.822 - ETA: 4:57 - loss: 0.3928 - acc: 0.831 - ETA: 5:00 - loss: 0.3867 - acc: 0.842 - ETA: 5:00 - loss: 0.3860 - acc: 0.845 - ETA: 5:00 - loss: 0.3819 - acc: 0.848 - ETA: 4:59 - loss: 0.3860 - acc: 0.841 - ETA: 4:57 - loss: 0.3857 - acc: 0.843 - ETA: 4:55 - loss: 0.3837 - acc: 0.846 - ETA: 4:52 - loss: 0.3853 - acc: 0.844 - ETA: 4:49 - loss: 0.3875 - acc: 0.840 - ETA: 4:46 - loss: 0.3890 - acc: 0.836 - ETA: 4:42 - loss: 0.3888 - acc: 0.836 - ETA: 4:39 - loss: 0.3875 - acc: 0.836 - ETA: 4:35 - loss: 0.3853 - acc: 0.839 - ETA: 4:31 - loss: 0.3896 - acc: 0.836 - ETA: 4:27 - loss: 0.3922 - acc: 0.834 - ETA: 4:23 - loss: 0.3908 - acc: 0.834 - ETA: 4:19 - loss: 0.3892 - acc: 0.837 - ETA: 4:15 - loss: 0.3852 - acc: 0.841 - ETA: 4:10 - loss: 0.3879 - acc: 0.838 - ETA: 4:06 - loss: 0.3866 - acc: 0.840 - ETA: 4:02 - loss: 0.3861 - acc: 0.841 - ETA: 3:57 - loss: 0.3902 - acc: 0.838 - ETA: 3:53 - loss: 0.3887 - acc: 0.839 - ETA: 3:49 - loss: 0.3897 - acc: 0.839 - ETA: 3:44 - loss: 0.3918 - acc: 0.838 - ETA: 3:40 - loss: 0.3923 - acc: 0.839 - ETA: 3:35 - loss: 0.3926 - acc: 0.838 - ETA: 3:31 - loss: 0.3933 - acc: 0.838 - ETA: 3:26 - loss: 0.3930 - acc: 0.838 - ETA: 3:22 - loss: 0.3960 - acc: 0.835 - ETA: 3:17 - loss: 0.3971 - acc: 0.834 - ETA: 3:13 - loss: 0.3959 - acc: 0.834 - ETA: 3:08 - loss: 0.3959 - acc: 0.834 - ETA: 3:03 - loss: 0.3948 - acc: 0.836 - ETA: 2:59 - loss: 0.3961 - acc: 0.834 - ETA: 2:54 - loss: 0.3946 - acc: 0.836 - ETA: 2:49 - loss: 0.3926 - acc: 0.838 - ETA: 2:45 - loss: 0.3923 - acc: 0.838 - ETA: 2:40 - loss: 0.3925 - acc: 0.838 - ETA: 2:36 - loss: 0.3931 - acc: 0.838 - ETA: 2:31 - loss: 0.3940 - acc: 0.837 - ETA: 2:26 - loss: 0.3927 - acc: 0.838 - ETA: 2:22 - loss: 0.3927 - acc: 0.838 - ETA: 2:17 - loss: 0.3936 - acc: 0.837 - ETA: 2:12 - loss: 0.3937 - acc: 0.837 - ETA: 2:07 - loss: 0.3920 - acc: 0.839 - ETA: 2:03 - loss: 0.3925 - acc: 0.838 - ETA: 1:58 - loss: 0.3920 - acc: 0.838 - ETA: 1:53 - loss: 0.3914 - acc: 0.839 - ETA: 1:49 - loss: 0.3923 - acc: 0.839 - ETA: 1:44 - loss: 0.3922 - acc: 0.838 - ETA: 1:39 - loss: 0.3922 - acc: 0.838 - ETA: 1:35 - loss: 0.3919 - acc: 0.838 - ETA: 1:30 - loss: 0.3924 - acc: 0.838 - ETA: 1:25 - loss: 0.3914 - acc: 0.839 - ETA: 1:20 - loss: 0.3904 - acc: 0.840 - ETA: 1:16 - loss: 0.3892 - acc: 0.841 - ETA: 1:11 - loss: 0.3902 - acc: 0.840 - ETA: 1:06 - loss: 0.3900 - acc: 0.840 - ETA: 1:01 - loss: 0.3901 - acc: 0.840 - ETA: 57s - loss: 0.3902 - acc: 0.839 - ETA: 52s - loss: 0.3896 - acc: 0.84 - ETA: 46s - loss: 0.3906 - acc: 0.83 - ETA: 42s - loss: 0.3902 - acc: 0.83 - ETA: 37s - loss: 0.3899 - acc: 0.84 - ETA: 32s - loss: 0.3899 - acc: 0.84 - ETA: 28s - loss: 0.3903 - acc: 0.84 - ETA: 23s - loss: 0.3903 - acc: 0.84 - ETA: 18s - loss: 0.3899 - acc: 0.84 - ETA: 14s - loss: 0.3891 - acc: 0.84 - ETA: 9s - loss: 0.3886 - acc: 0.8425 - ETA: 4s - loss: 0.3876 - acc: 0.8436\n",
      "Epoch 00050: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.3863 - acc: 0.8449 - val_loss: 0.6606 - val_acc: 0.6705\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.4487 - acc: 0.802 - ETA: 6:05 - loss: 0.3960 - acc: 0.843 - ETA: 6:01 - loss: 0.4083 - acc: 0.840 - ETA: 5:56 - loss: 0.4160 - acc: 0.825 - ETA: 5:51 - loss: 0.4167 - acc: 0.818 - ETA: 5:47 - loss: 0.4119 - acc: 0.821 - ETA: 5:42 - loss: 0.4190 - acc: 0.812 - ETA: 5:37 - loss: 0.4095 - acc: 0.821 - ETA: 5:32 - loss: 0.4071 - acc: 0.826 - ETA: 5:27 - loss: 0.4126 - acc: 0.825 - ETA: 5:23 - loss: 0.4096 - acc: 0.828 - ETA: 5:18 - loss: 0.4127 - acc: 0.825 - ETA: 5:13 - loss: 0.4117 - acc: 0.826 - ETA: 5:08 - loss: 0.4068 - acc: 0.831 - ETA: 5:03 - loss: 0.4069 - acc: 0.833 - ETA: 4:58 - loss: 0.4073 - acc: 0.831 - ETA: 4:54 - loss: 0.4064 - acc: 0.830 - ETA: 4:49 - loss: 0.4092 - acc: 0.828 - ETA: 4:44 - loss: 0.4118 - acc: 0.827 - ETA: 4:39 - loss: 0.4073 - acc: 0.830 - ETA: 4:34 - loss: 0.4088 - acc: 0.829 - ETA: 4:30 - loss: 0.4093 - acc: 0.830 - ETA: 4:25 - loss: 0.4089 - acc: 0.830 - ETA: 4:20 - loss: 0.4080 - acc: 0.830 - ETA: 4:15 - loss: 0.4081 - acc: 0.830 - ETA: 4:10 - loss: 0.4076 - acc: 0.830 - ETA: 4:05 - loss: 0.4069 - acc: 0.829 - ETA: 4:01 - loss: 0.4054 - acc: 0.831 - ETA: 3:56 - loss: 0.4050 - acc: 0.833 - ETA: 3:51 - loss: 0.4059 - acc: 0.833 - ETA: 3:46 - loss: 0.4058 - acc: 0.832 - ETA: 3:41 - loss: 0.4039 - acc: 0.834 - ETA: 3:36 - loss: 0.4023 - acc: 0.836 - ETA: 3:32 - loss: 0.4018 - acc: 0.836 - ETA: 3:27 - loss: 0.4014 - acc: 0.836 - ETA: 3:22 - loss: 0.4009 - acc: 0.836 - ETA: 3:17 - loss: 0.3994 - acc: 0.838 - ETA: 3:12 - loss: 0.4004 - acc: 0.837 - ETA: 3:08 - loss: 0.4000 - acc: 0.838 - ETA: 3:03 - loss: 0.4004 - acc: 0.838 - ETA: 2:58 - loss: 0.4005 - acc: 0.837 - ETA: 2:53 - loss: 0.4005 - acc: 0.837 - ETA: 2:48 - loss: 0.3995 - acc: 0.837 - ETA: 2:43 - loss: 0.4001 - acc: 0.837 - ETA: 2:39 - loss: 0.3992 - acc: 0.838 - ETA: 2:34 - loss: 0.3973 - acc: 0.839 - ETA: 2:29 - loss: 0.3983 - acc: 0.838 - ETA: 2:24 - loss: 0.3983 - acc: 0.839 - ETA: 2:19 - loss: 0.3982 - acc: 0.838 - ETA: 2:15 - loss: 0.3979 - acc: 0.839 - ETA: 2:10 - loss: 0.3974 - acc: 0.839 - ETA: 2:05 - loss: 0.3973 - acc: 0.839 - ETA: 2:00 - loss: 0.3973 - acc: 0.839 - ETA: 1:55 - loss: 0.3981 - acc: 0.838 - ETA: 1:50 - loss: 0.3966 - acc: 0.840 - ETA: 1:46 - loss: 0.3978 - acc: 0.838 - ETA: 1:41 - loss: 0.3978 - acc: 0.838 - ETA: 1:36 - loss: 0.3970 - acc: 0.838 - ETA: 1:31 - loss: 0.3969 - acc: 0.838 - ETA: 1:26 - loss: 0.3973 - acc: 0.837 - ETA: 1:21 - loss: 0.3976 - acc: 0.837 - ETA: 1:16 - loss: 0.3986 - acc: 0.837 - ETA: 1:11 - loss: 0.3979 - acc: 0.837 - ETA: 1:06 - loss: 0.3977 - acc: 0.837 - ETA: 1:01 - loss: 0.3977 - acc: 0.837 - ETA: 57s - loss: 0.3976 - acc: 0.837 - ETA: 52s - loss: 0.3968 - acc: 0.83 - ETA: 47s - loss: 0.3956 - acc: 0.83 - ETA: 42s - loss: 0.3961 - acc: 0.83 - ETA: 38s - loss: 0.3957 - acc: 0.83 - ETA: 33s - loss: 0.3948 - acc: 0.83 - ETA: 28s - loss: 0.3942 - acc: 0.83 - ETA: 23s - loss: 0.3940 - acc: 0.83 - ETA: 19s - loss: 0.3937 - acc: 0.83 - ETA: 14s - loss: 0.3932 - acc: 0.84 - ETA: 9s - loss: 0.3931 - acc: 0.8400 - ETA: 4s - loss: 0.3929 - acc: 0.8398\n",
      "Epoch 00051: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3926 - acc: 0.8403 - val_loss: 0.6779 - val_acc: 0.6534\n",
      "Epoch 52/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3999 - acc: 0.822 - ETA: 6:05 - loss: 0.4028 - acc: 0.822 - ETA: 6:00 - loss: 0.3863 - acc: 0.840 - ETA: 5:56 - loss: 0.3855 - acc: 0.833 - ETA: 5:51 - loss: 0.3781 - acc: 0.833 - ETA: 5:46 - loss: 0.3741 - acc: 0.835 - ETA: 5:41 - loss: 0.3762 - acc: 0.831 - ETA: 5:37 - loss: 0.3764 - acc: 0.833 - ETA: 5:32 - loss: 0.3775 - acc: 0.834 - ETA: 5:27 - loss: 0.3732 - acc: 0.839 - ETA: 5:22 - loss: 0.3729 - acc: 0.840 - ETA: 5:18 - loss: 0.3740 - acc: 0.840 - ETA: 5:13 - loss: 0.3773 - acc: 0.839 - ETA: 5:08 - loss: 0.3808 - acc: 0.839 - ETA: 5:03 - loss: 0.3780 - acc: 0.840 - ETA: 4:58 - loss: 0.3770 - acc: 0.839 - ETA: 4:53 - loss: 0.3776 - acc: 0.839 - ETA: 4:49 - loss: 0.3790 - acc: 0.838 - ETA: 4:44 - loss: 0.3775 - acc: 0.839 - ETA: 4:39 - loss: 0.3789 - acc: 0.838 - ETA: 4:34 - loss: 0.3802 - acc: 0.837 - ETA: 4:29 - loss: 0.3827 - acc: 0.835 - ETA: 4:24 - loss: 0.3825 - acc: 0.836 - ETA: 4:20 - loss: 0.3836 - acc: 0.835 - ETA: 4:15 - loss: 0.3799 - acc: 0.839 - ETA: 4:10 - loss: 0.3801 - acc: 0.839 - ETA: 4:05 - loss: 0.3804 - acc: 0.838 - ETA: 4:00 - loss: 0.3788 - acc: 0.837 - ETA: 3:56 - loss: 0.3810 - acc: 0.835 - ETA: 3:51 - loss: 0.3801 - acc: 0.836 - ETA: 3:46 - loss: 0.3797 - acc: 0.836 - ETA: 3:41 - loss: 0.3804 - acc: 0.835 - ETA: 3:36 - loss: 0.3816 - acc: 0.835 - ETA: 3:32 - loss: 0.3819 - acc: 0.835 - ETA: 3:27 - loss: 0.3830 - acc: 0.835 - ETA: 3:22 - loss: 0.3827 - acc: 0.835 - ETA: 3:17 - loss: 0.3814 - acc: 0.838 - ETA: 3:12 - loss: 0.3807 - acc: 0.838 - ETA: 3:08 - loss: 0.3804 - acc: 0.838 - ETA: 3:03 - loss: 0.3798 - acc: 0.839 - ETA: 2:58 - loss: 0.3806 - acc: 0.839 - ETA: 2:53 - loss: 0.3802 - acc: 0.839 - ETA: 2:48 - loss: 0.3791 - acc: 0.840 - ETA: 2:43 - loss: 0.3793 - acc: 0.841 - ETA: 2:39 - loss: 0.3783 - acc: 0.842 - ETA: 2:34 - loss: 0.3768 - acc: 0.843 - ETA: 2:29 - loss: 0.3773 - acc: 0.843 - ETA: 2:24 - loss: 0.3783 - acc: 0.842 - ETA: 2:19 - loss: 0.3783 - acc: 0.842 - ETA: 2:14 - loss: 0.3788 - acc: 0.842 - ETA: 2:10 - loss: 0.3791 - acc: 0.841 - ETA: 2:05 - loss: 0.3799 - acc: 0.839 - ETA: 2:00 - loss: 0.3810 - acc: 0.838 - ETA: 1:55 - loss: 0.3809 - acc: 0.838 - ETA: 1:50 - loss: 0.3811 - acc: 0.838 - ETA: 1:46 - loss: 0.3810 - acc: 0.838 - ETA: 1:41 - loss: 0.3813 - acc: 0.838 - ETA: 1:36 - loss: 0.3822 - acc: 0.838 - ETA: 1:31 - loss: 0.3836 - acc: 0.837 - ETA: 1:26 - loss: 0.3840 - acc: 0.837 - ETA: 1:21 - loss: 0.3833 - acc: 0.837 - ETA: 1:17 - loss: 0.3833 - acc: 0.837 - ETA: 1:12 - loss: 0.3827 - acc: 0.838 - ETA: 1:07 - loss: 0.3821 - acc: 0.838 - ETA: 1:02 - loss: 0.3830 - acc: 0.838 - ETA: 57s - loss: 0.3836 - acc: 0.837 - ETA: 53s - loss: 0.3841 - acc: 0.83 - ETA: 48s - loss: 0.3842 - acc: 0.83 - ETA: 43s - loss: 0.3851 - acc: 0.83 - ETA: 38s - loss: 0.3851 - acc: 0.83 - ETA: 33s - loss: 0.3852 - acc: 0.83 - ETA: 28s - loss: 0.3847 - acc: 0.83 - ETA: 24s - loss: 0.3849 - acc: 0.83 - ETA: 19s - loss: 0.3841 - acc: 0.83 - ETA: 14s - loss: 0.3848 - acc: 0.83 - ETA: 9s - loss: 0.3841 - acc: 0.8369 - ETA: 4s - loss: 0.3843 - acc: 0.8369\n",
      "Epoch 00052: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.3835 - acc: 0.8377 - val_loss: 0.6644 - val_acc: 0.6723\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3888 - acc: 0.812 - ETA: 6:06 - loss: 0.3857 - acc: 0.812 - ETA: 6:01 - loss: 0.3837 - acc: 0.812 - ETA: 5:56 - loss: 0.4031 - acc: 0.796 - ETA: 5:51 - loss: 0.3965 - acc: 0.806 - ETA: 5:47 - loss: 0.3987 - acc: 0.809 - ETA: 5:42 - loss: 0.4052 - acc: 0.811 - ETA: 5:37 - loss: 0.4160 - acc: 0.802 - ETA: 5:32 - loss: 0.4092 - acc: 0.810 - ETA: 5:27 - loss: 0.4143 - acc: 0.808 - ETA: 5:23 - loss: 0.4166 - acc: 0.808 - ETA: 5:18 - loss: 0.4169 - acc: 0.809 - ETA: 5:13 - loss: 0.4131 - acc: 0.814 - ETA: 5:08 - loss: 0.4139 - acc: 0.814 - ETA: 5:03 - loss: 0.4066 - acc: 0.820 - ETA: 4:59 - loss: 0.4052 - acc: 0.822 - ETA: 4:54 - loss: 0.4046 - acc: 0.824 - ETA: 4:49 - loss: 0.3985 - acc: 0.831 - ETA: 4:44 - loss: 0.3972 - acc: 0.832 - ETA: 4:39 - loss: 0.3971 - acc: 0.832 - ETA: 4:34 - loss: 0.3940 - acc: 0.835 - ETA: 4:30 - loss: 0.3942 - acc: 0.834 - ETA: 4:25 - loss: 0.3947 - acc: 0.833 - ETA: 4:20 - loss: 0.3967 - acc: 0.831 - ETA: 4:15 - loss: 0.3946 - acc: 0.832 - ETA: 4:10 - loss: 0.3954 - acc: 0.833 - ETA: 4:05 - loss: 0.3971 - acc: 0.832 - ETA: 3:53 - loss: 0.3976 - acc: 0.835 - ETA: 3:49 - loss: 0.3969 - acc: 0.836 - ETA: 3:44 - loss: 0.3944 - acc: 0.839 - ETA: 3:40 - loss: 0.3944 - acc: 0.839 - ETA: 3:35 - loss: 0.3953 - acc: 0.838 - ETA: 3:31 - loss: 0.3957 - acc: 0.837 - ETA: 3:26 - loss: 0.3948 - acc: 0.839 - ETA: 3:22 - loss: 0.3960 - acc: 0.839 - ETA: 3:17 - loss: 0.3946 - acc: 0.840 - ETA: 3:13 - loss: 0.3938 - acc: 0.842 - ETA: 3:08 - loss: 0.3932 - acc: 0.842 - ETA: 3:03 - loss: 0.3939 - acc: 0.841 - ETA: 2:59 - loss: 0.3952 - acc: 0.841 - ETA: 2:54 - loss: 0.3946 - acc: 0.841 - ETA: 2:50 - loss: 0.3933 - acc: 0.841 - ETA: 2:45 - loss: 0.3919 - acc: 0.842 - ETA: 2:40 - loss: 0.3910 - acc: 0.843 - ETA: 2:36 - loss: 0.3907 - acc: 0.843 - ETA: 2:31 - loss: 0.3910 - acc: 0.843 - ETA: 2:26 - loss: 0.3906 - acc: 0.842 - ETA: 2:22 - loss: 0.3899 - acc: 0.842 - ETA: 2:17 - loss: 0.3897 - acc: 0.843 - ETA: 2:12 - loss: 0.3890 - acc: 0.844 - ETA: 2:08 - loss: 0.3893 - acc: 0.843 - ETA: 2:03 - loss: 0.3899 - acc: 0.843 - ETA: 1:58 - loss: 0.3894 - acc: 0.844 - ETA: 1:53 - loss: 0.3897 - acc: 0.843 - ETA: 1:49 - loss: 0.3902 - acc: 0.843 - ETA: 1:44 - loss: 0.3888 - acc: 0.844 - ETA: 1:39 - loss: 0.3886 - acc: 0.845 - ETA: 1:35 - loss: 0.3888 - acc: 0.844 - ETA: 1:30 - loss: 0.3895 - acc: 0.842 - ETA: 1:25 - loss: 0.3887 - acc: 0.843 - ETA: 1:20 - loss: 0.3884 - acc: 0.843 - ETA: 1:16 - loss: 0.3887 - acc: 0.844 - ETA: 1:11 - loss: 0.3888 - acc: 0.844 - ETA: 1:06 - loss: 0.3892 - acc: 0.843 - ETA: 1:01 - loss: 0.3900 - acc: 0.842 - ETA: 57s - loss: 0.3896 - acc: 0.842 - ETA: 52s - loss: 0.3892 - acc: 0.84 - ETA: 47s - loss: 0.3885 - acc: 0.84 - ETA: 42s - loss: 0.3882 - acc: 0.84 - ETA: 38s - loss: 0.3886 - acc: 0.84 - ETA: 33s - loss: 0.3882 - acc: 0.84 - ETA: 28s - loss: 0.3876 - acc: 0.84 - ETA: 23s - loss: 0.3876 - acc: 0.84 - ETA: 19s - loss: 0.3867 - acc: 0.84 - ETA: 14s - loss: 0.3867 - acc: 0.84 - ETA: 9s - loss: 0.3861 - acc: 0.8462 - ETA: 4s - loss: 0.3863 - acc: 0.8458\n",
      "Epoch 00053: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3849 - acc: 0.8469 - val_loss: 0.6507 - val_acc: 0.6730\n",
      "Epoch 54/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4070 - acc: 0.833 - ETA: 6:06 - loss: 0.4153 - acc: 0.822 - ETA: 6:01 - loss: 0.4012 - acc: 0.843 - ETA: 5:56 - loss: 0.4030 - acc: 0.835 - ETA: 5:52 - loss: 0.4171 - acc: 0.816 - ETA: 5:47 - loss: 0.3959 - acc: 0.835 - ETA: 5:42 - loss: 0.3875 - acc: 0.840 - ETA: 5:37 - loss: 0.3848 - acc: 0.835 - ETA: 5:32 - loss: 0.3884 - acc: 0.836 - ETA: 5:27 - loss: 0.3877 - acc: 0.837 - ETA: 5:23 - loss: 0.3882 - acc: 0.835 - ETA: 5:18 - loss: 0.3927 - acc: 0.829 - ETA: 5:13 - loss: 0.3912 - acc: 0.830 - ETA: 5:08 - loss: 0.3915 - acc: 0.832 - ETA: 5:03 - loss: 0.3882 - acc: 0.836 - ETA: 4:59 - loss: 0.3872 - acc: 0.837 - ETA: 4:54 - loss: 0.3901 - acc: 0.835 - ETA: 4:49 - loss: 0.3891 - acc: 0.836 - ETA: 4:44 - loss: 0.3914 - acc: 0.834 - ETA: 4:39 - loss: 0.3933 - acc: 0.832 - ETA: 4:35 - loss: 0.3953 - acc: 0.830 - ETA: 4:30 - loss: 0.3969 - acc: 0.827 - ETA: 4:25 - loss: 0.3971 - acc: 0.827 - ETA: 4:20 - loss: 0.3994 - acc: 0.826 - ETA: 4:06 - loss: 0.3983 - acc: 0.826 - ETA: 4:02 - loss: 0.3995 - acc: 0.826 - ETA: 3:58 - loss: 0.4004 - acc: 0.825 - ETA: 3:53 - loss: 0.4005 - acc: 0.825 - ETA: 3:49 - loss: 0.3990 - acc: 0.826 - ETA: 3:44 - loss: 0.3971 - acc: 0.829 - ETA: 3:40 - loss: 0.3959 - acc: 0.831 - ETA: 3:35 - loss: 0.3946 - acc: 0.833 - ETA: 3:31 - loss: 0.3923 - acc: 0.834 - ETA: 3:26 - loss: 0.3919 - acc: 0.835 - ETA: 3:22 - loss: 0.3919 - acc: 0.835 - ETA: 3:17 - loss: 0.3896 - acc: 0.837 - ETA: 3:13 - loss: 0.3876 - acc: 0.839 - ETA: 3:08 - loss: 0.3873 - acc: 0.839 - ETA: 3:03 - loss: 0.3869 - acc: 0.840 - ETA: 2:59 - loss: 0.3877 - acc: 0.839 - ETA: 2:54 - loss: 0.3875 - acc: 0.840 - ETA: 2:50 - loss: 0.3893 - acc: 0.839 - ETA: 2:45 - loss: 0.3907 - acc: 0.837 - ETA: 2:40 - loss: 0.3900 - acc: 0.838 - ETA: 2:36 - loss: 0.3891 - acc: 0.838 - ETA: 2:31 - loss: 0.3881 - acc: 0.839 - ETA: 2:26 - loss: 0.3887 - acc: 0.838 - ETA: 2:22 - loss: 0.3884 - acc: 0.838 - ETA: 2:17 - loss: 0.3890 - acc: 0.837 - ETA: 2:12 - loss: 0.3889 - acc: 0.837 - ETA: 2:08 - loss: 0.3894 - acc: 0.838 - ETA: 2:03 - loss: 0.3907 - acc: 0.836 - ETA: 1:58 - loss: 0.3898 - acc: 0.837 - ETA: 1:53 - loss: 0.3899 - acc: 0.837 - ETA: 1:49 - loss: 0.3897 - acc: 0.836 - ETA: 1:44 - loss: 0.3897 - acc: 0.838 - ETA: 1:39 - loss: 0.3901 - acc: 0.837 - ETA: 1:35 - loss: 0.3899 - acc: 0.836 - ETA: 1:30 - loss: 0.3896 - acc: 0.837 - ETA: 1:25 - loss: 0.3888 - acc: 0.838 - ETA: 1:20 - loss: 0.3884 - acc: 0.839 - ETA: 1:16 - loss: 0.3882 - acc: 0.839 - ETA: 1:11 - loss: 0.3881 - acc: 0.839 - ETA: 1:06 - loss: 0.3883 - acc: 0.839 - ETA: 1:01 - loss: 0.3889 - acc: 0.839 - ETA: 57s - loss: 0.3892 - acc: 0.838 - ETA: 52s - loss: 0.3887 - acc: 0.83 - ETA: 47s - loss: 0.3881 - acc: 0.83 - ETA: 42s - loss: 0.3882 - acc: 0.83 - ETA: 38s - loss: 0.3870 - acc: 0.84 - ETA: 33s - loss: 0.3857 - acc: 0.84 - ETA: 28s - loss: 0.3849 - acc: 0.84 - ETA: 23s - loss: 0.3854 - acc: 0.84 - ETA: 19s - loss: 0.3855 - acc: 0.84 - ETA: 14s - loss: 0.3846 - acc: 0.84 - ETA: 9s - loss: 0.3848 - acc: 0.8418 - ETA: 4s - loss: 0.3852 - acc: 0.8415\n",
      "Epoch 00054: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3857 - acc: 0.8409 - val_loss: 0.6517 - val_acc: 0.6761\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3209 - acc: 0.854 - ETA: 6:06 - loss: 0.3648 - acc: 0.838 - ETA: 6:01 - loss: 0.3865 - acc: 0.829 - ETA: 5:56 - loss: 0.3822 - acc: 0.835 - ETA: 5:51 - loss: 0.3733 - acc: 0.845 - ETA: 5:47 - loss: 0.3694 - acc: 0.852 - ETA: 5:42 - loss: 0.3769 - acc: 0.843 - ETA: 5:37 - loss: 0.3779 - acc: 0.849 - ETA: 5:32 - loss: 0.3775 - acc: 0.849 - ETA: 5:27 - loss: 0.3827 - acc: 0.839 - ETA: 5:23 - loss: 0.3779 - acc: 0.843 - ETA: 5:18 - loss: 0.3791 - acc: 0.845 - ETA: 5:13 - loss: 0.3807 - acc: 0.843 - ETA: 5:08 - loss: 0.3755 - acc: 0.849 - ETA: 5:03 - loss: 0.3774 - acc: 0.846 - ETA: 4:59 - loss: 0.3782 - acc: 0.847 - ETA: 4:54 - loss: 0.3759 - acc: 0.848 - ETA: 4:49 - loss: 0.3746 - acc: 0.850 - ETA: 4:44 - loss: 0.3729 - acc: 0.851 - ETA: 4:39 - loss: 0.3764 - acc: 0.849 - ETA: 4:34 - loss: 0.3753 - acc: 0.849 - ETA: 4:30 - loss: 0.3789 - acc: 0.845 - ETA: 4:25 - loss: 0.3789 - acc: 0.846 - ETA: 4:20 - loss: 0.3806 - acc: 0.844 - ETA: 4:15 - loss: 0.3791 - acc: 0.845 - ETA: 4:10 - loss: 0.3786 - acc: 0.845 - ETA: 4:05 - loss: 0.3800 - acc: 0.846 - ETA: 4:01 - loss: 0.3825 - acc: 0.843 - ETA: 3:56 - loss: 0.3849 - acc: 0.841 - ETA: 3:51 - loss: 0.3837 - acc: 0.842 - ETA: 3:46 - loss: 0.3838 - acc: 0.841 - ETA: 3:41 - loss: 0.3813 - acc: 0.843 - ETA: 3:37 - loss: 0.3832 - acc: 0.841 - ETA: 3:32 - loss: 0.3842 - acc: 0.842 - ETA: 3:27 - loss: 0.3844 - acc: 0.842 - ETA: 3:22 - loss: 0.3855 - acc: 0.841 - ETA: 3:17 - loss: 0.3847 - acc: 0.842 - ETA: 3:08 - loss: 0.3911 - acc: 0.837 - ETA: 3:03 - loss: 0.3914 - acc: 0.837 - ETA: 2:59 - loss: 0.3911 - acc: 0.838 - ETA: 2:54 - loss: 0.3911 - acc: 0.838 - ETA: 2:50 - loss: 0.3904 - acc: 0.838 - ETA: 2:45 - loss: 0.3911 - acc: 0.837 - ETA: 2:40 - loss: 0.3916 - acc: 0.837 - ETA: 2:36 - loss: 0.3927 - acc: 0.836 - ETA: 2:31 - loss: 0.3930 - acc: 0.835 - ETA: 2:26 - loss: 0.3926 - acc: 0.836 - ETA: 2:22 - loss: 0.3918 - acc: 0.836 - ETA: 2:17 - loss: 0.3911 - acc: 0.836 - ETA: 2:12 - loss: 0.3905 - acc: 0.837 - ETA: 2:08 - loss: 0.3897 - acc: 0.838 - ETA: 2:03 - loss: 0.3898 - acc: 0.838 - ETA: 1:58 - loss: 0.3895 - acc: 0.839 - ETA: 1:53 - loss: 0.3903 - acc: 0.838 - ETA: 1:49 - loss: 0.3904 - acc: 0.838 - ETA: 1:44 - loss: 0.3901 - acc: 0.837 - ETA: 1:39 - loss: 0.3894 - acc: 0.837 - ETA: 1:35 - loss: 0.3885 - acc: 0.837 - ETA: 1:30 - loss: 0.3892 - acc: 0.837 - ETA: 1:25 - loss: 0.3885 - acc: 0.838 - ETA: 1:20 - loss: 0.3888 - acc: 0.838 - ETA: 1:16 - loss: 0.3889 - acc: 0.838 - ETA: 1:11 - loss: 0.3891 - acc: 0.838 - ETA: 1:06 - loss: 0.3877 - acc: 0.839 - ETA: 1:01 - loss: 0.3872 - acc: 0.839 - ETA: 57s - loss: 0.3875 - acc: 0.839 - ETA: 52s - loss: 0.3873 - acc: 0.83 - ETA: 47s - loss: 0.3864 - acc: 0.84 - ETA: 42s - loss: 0.3862 - acc: 0.84 - ETA: 38s - loss: 0.3859 - acc: 0.84 - ETA: 33s - loss: 0.3853 - acc: 0.84 - ETA: 28s - loss: 0.3859 - acc: 0.84 - ETA: 23s - loss: 0.3851 - acc: 0.84 - ETA: 19s - loss: 0.3854 - acc: 0.84 - ETA: 14s - loss: 0.3860 - acc: 0.84 - ETA: 9s - loss: 0.3854 - acc: 0.8413 - ETA: 4s - loss: 0.3852 - acc: 0.8410\n",
      "Epoch 00055: val_loss did not improve\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.3844 - acc: 0.8422 - val_loss: 0.6564 - val_acc: 0.6740\n",
      "Epoch 56/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3956 - acc: 0.885 - ETA: 6:06 - loss: 0.4161 - acc: 0.843 - ETA: 6:01 - loss: 0.4154 - acc: 0.829 - ETA: 5:56 - loss: 0.4053 - acc: 0.830 - ETA: 5:51 - loss: 0.3890 - acc: 0.845 - ETA: 5:47 - loss: 0.3861 - acc: 0.840 - ETA: 5:42 - loss: 0.3856 - acc: 0.842 - ETA: 5:37 - loss: 0.3840 - acc: 0.843 - ETA: 5:32 - loss: 0.3754 - acc: 0.847 - ETA: 5:27 - loss: 0.3761 - acc: 0.843 - ETA: 5:23 - loss: 0.3793 - acc: 0.840 - ETA: 5:18 - loss: 0.3746 - acc: 0.846 - ETA: 5:13 - loss: 0.3733 - acc: 0.845 - ETA: 5:08 - loss: 0.3733 - acc: 0.846 - ETA: 5:03 - loss: 0.3797 - acc: 0.843 - ETA: 4:59 - loss: 0.3773 - acc: 0.847 - ETA: 4:54 - loss: 0.3755 - acc: 0.849 - ETA: 4:49 - loss: 0.3705 - acc: 0.851 - ETA: 4:44 - loss: 0.3685 - acc: 0.854 - ETA: 4:39 - loss: 0.3713 - acc: 0.853 - ETA: 4:34 - loss: 0.3702 - acc: 0.855 - ETA: 4:30 - loss: 0.3699 - acc: 0.855 - ETA: 4:25 - loss: 0.3737 - acc: 0.851 - ETA: 4:20 - loss: 0.3756 - acc: 0.850 - ETA: 4:15 - loss: 0.3762 - acc: 0.850 - ETA: 4:10 - loss: 0.3775 - acc: 0.847 - ETA: 4:06 - loss: 0.3787 - acc: 0.845 - ETA: 4:01 - loss: 0.3776 - acc: 0.846 - ETA: 3:56 - loss: 0.3754 - acc: 0.848 - ETA: 3:51 - loss: 0.3745 - acc: 0.848 - ETA: 3:40 - loss: 0.3797 - acc: 0.842 - ETA: 3:35 - loss: 0.3781 - acc: 0.845 - ETA: 3:31 - loss: 0.3793 - acc: 0.845 - ETA: 3:26 - loss: 0.3784 - acc: 0.845 - ETA: 3:22 - loss: 0.3783 - acc: 0.847 - ETA: 3:17 - loss: 0.3766 - acc: 0.849 - ETA: 3:13 - loss: 0.3758 - acc: 0.851 - ETA: 3:08 - loss: 0.3760 - acc: 0.850 - ETA: 3:03 - loss: 0.3758 - acc: 0.850 - ETA: 2:59 - loss: 0.3766 - acc: 0.849 - ETA: 2:54 - loss: 0.3756 - acc: 0.849 - ETA: 2:50 - loss: 0.3769 - acc: 0.847 - ETA: 2:45 - loss: 0.3767 - acc: 0.848 - ETA: 2:40 - loss: 0.3760 - acc: 0.848 - ETA: 2:36 - loss: 0.3782 - acc: 0.847 - ETA: 2:31 - loss: 0.3786 - acc: 0.846 - ETA: 2:26 - loss: 0.3789 - acc: 0.846 - ETA: 2:22 - loss: 0.3814 - acc: 0.844 - ETA: 2:17 - loss: 0.3797 - acc: 0.846 - ETA: 2:12 - loss: 0.3800 - acc: 0.845 - ETA: 2:08 - loss: 0.3797 - acc: 0.846 - ETA: 2:03 - loss: 0.3786 - acc: 0.847 - ETA: 1:58 - loss: 0.3792 - acc: 0.846 - ETA: 1:53 - loss: 0.3789 - acc: 0.846 - ETA: 1:49 - loss: 0.3787 - acc: 0.847 - ETA: 1:44 - loss: 0.3786 - acc: 0.847 - ETA: 1:39 - loss: 0.3793 - acc: 0.846 - ETA: 1:35 - loss: 0.3792 - acc: 0.846 - ETA: 1:30 - loss: 0.3798 - acc: 0.845 - ETA: 1:25 - loss: 0.3800 - acc: 0.845 - ETA: 1:20 - loss: 0.3797 - acc: 0.845 - ETA: 1:16 - loss: 0.3786 - acc: 0.846 - ETA: 1:11 - loss: 0.3783 - acc: 0.846 - ETA: 1:06 - loss: 0.3779 - acc: 0.846 - ETA: 1:01 - loss: 0.3782 - acc: 0.846 - ETA: 57s - loss: 0.3772 - acc: 0.847 - ETA: 52s - loss: 0.3762 - acc: 0.84 - ETA: 47s - loss: 0.3772 - acc: 0.84 - ETA: 42s - loss: 0.3783 - acc: 0.84 - ETA: 38s - loss: 0.3774 - acc: 0.84 - ETA: 33s - loss: 0.3762 - acc: 0.84 - ETA: 28s - loss: 0.3762 - acc: 0.84 - ETA: 23s - loss: 0.3751 - acc: 0.84 - ETA: 19s - loss: 0.3752 - acc: 0.84 - ETA: 14s - loss: 0.3758 - acc: 0.84 - ETA: 9s - loss: 0.3754 - acc: 0.8492 - ETA: 4s - loss: 0.3770 - acc: 0.8479\n",
      "Epoch 00056: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.3770 - acc: 0.8481 - val_loss: 0.6708 - val_acc: 0.6660\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.3738 - acc: 0.875 - ETA: 6:05 - loss: 0.3366 - acc: 0.906 - ETA: 6:01 - loss: 0.3376 - acc: 0.895 - ETA: 5:56 - loss: 0.3545 - acc: 0.882 - ETA: 5:51 - loss: 0.3616 - acc: 0.868 - ETA: 5:46 - loss: 0.3603 - acc: 0.864 - ETA: 5:42 - loss: 0.3585 - acc: 0.867 - ETA: 5:37 - loss: 0.3631 - acc: 0.862 - ETA: 5:32 - loss: 0.3601 - acc: 0.865 - ETA: 5:27 - loss: 0.3647 - acc: 0.862 - ETA: 5:22 - loss: 0.3641 - acc: 0.862 - ETA: 5:18 - loss: 0.3666 - acc: 0.861 - ETA: 5:13 - loss: 0.3640 - acc: 0.860 - ETA: 5:08 - loss: 0.3672 - acc: 0.859 - ETA: 5:03 - loss: 0.3709 - acc: 0.854 - ETA: 4:58 - loss: 0.3717 - acc: 0.854 - ETA: 4:54 - loss: 0.3704 - acc: 0.855 - ETA: 4:49 - loss: 0.3721 - acc: 0.853 - ETA: 4:44 - loss: 0.3757 - acc: 0.849 - ETA: 4:39 - loss: 0.3782 - acc: 0.847 - ETA: 4:34 - loss: 0.3784 - acc: 0.846 - ETA: 4:29 - loss: 0.3801 - acc: 0.845 - ETA: 4:25 - loss: 0.3838 - acc: 0.843 - ETA: 4:20 - loss: 0.3840 - acc: 0.843 - ETA: 4:15 - loss: 0.3818 - acc: 0.847 - ETA: 4:10 - loss: 0.3808 - acc: 0.849 - ETA: 4:05 - loss: 0.3784 - acc: 0.851 - ETA: 4:01 - loss: 0.3763 - acc: 0.851 - ETA: 3:56 - loss: 0.3725 - acc: 0.855 - ETA: 3:51 - loss: 0.3721 - acc: 0.855 - ETA: 3:46 - loss: 0.3713 - acc: 0.855 - ETA: 3:41 - loss: 0.3716 - acc: 0.854 - ETA: 3:36 - loss: 0.3735 - acc: 0.854 - ETA: 3:32 - loss: 0.3760 - acc: 0.851 - ETA: 3:27 - loss: 0.3752 - acc: 0.851 - ETA: 3:22 - loss: 0.3758 - acc: 0.851 - ETA: 3:17 - loss: 0.3731 - acc: 0.853 - ETA: 3:12 - loss: 0.3737 - acc: 0.853 - ETA: 3:08 - loss: 0.3732 - acc: 0.852 - ETA: 3:03 - loss: 0.3739 - acc: 0.851 - ETA: 2:58 - loss: 0.3748 - acc: 0.850 - ETA: 2:53 - loss: 0.3747 - acc: 0.850 - ETA: 2:48 - loss: 0.3747 - acc: 0.849 - ETA: 2:43 - loss: 0.3755 - acc: 0.849 - ETA: 2:39 - loss: 0.3752 - acc: 0.849 - ETA: 2:34 - loss: 0.3749 - acc: 0.849 - ETA: 2:29 - loss: 0.3748 - acc: 0.849 - ETA: 2:24 - loss: 0.3736 - acc: 0.850 - ETA: 2:19 - loss: 0.3744 - acc: 0.849 - ETA: 2:15 - loss: 0.3744 - acc: 0.849 - ETA: 2:10 - loss: 0.3748 - acc: 0.849 - ETA: 2:05 - loss: 0.3756 - acc: 0.848 - ETA: 2:00 - loss: 0.3753 - acc: 0.849 - ETA: 1:55 - loss: 0.3744 - acc: 0.849 - ETA: 1:50 - loss: 0.3742 - acc: 0.849 - ETA: 1:46 - loss: 0.3734 - acc: 0.849 - ETA: 1:41 - loss: 0.3728 - acc: 0.850 - ETA: 1:36 - loss: 0.3726 - acc: 0.851 - ETA: 1:31 - loss: 0.3738 - acc: 0.850 - ETA: 1:26 - loss: 0.3735 - acc: 0.850 - ETA: 1:21 - loss: 0.3723 - acc: 0.850 - ETA: 1:17 - loss: 0.3727 - acc: 0.850 - ETA: 1:12 - loss: 0.3728 - acc: 0.850 - ETA: 1:07 - loss: 0.3745 - acc: 0.848 - ETA: 1:02 - loss: 0.3738 - acc: 0.849 - ETA: 57s - loss: 0.3758 - acc: 0.848 - ETA: 53s - loss: 0.3761 - acc: 0.84 - ETA: 48s - loss: 0.3760 - acc: 0.84 - ETA: 43s - loss: 0.3751 - acc: 0.84 - ETA: 38s - loss: 0.3759 - acc: 0.84 - ETA: 33s - loss: 0.3755 - acc: 0.84 - ETA: 28s - loss: 0.3756 - acc: 0.84 - ETA: 24s - loss: 0.3750 - acc: 0.84 - ETA: 19s - loss: 0.3749 - acc: 0.84 - ETA: 14s - loss: 0.3753 - acc: 0.84 - ETA: 9s - loss: 0.3752 - acc: 0.8480 - ETA: 4s - loss: 0.3742 - acc: 0.8488\n",
      "Epoch 00057: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.3739 - acc: 0.8494 - val_loss: 0.6509 - val_acc: 0.6881\n",
      "Epoch 58/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3772 - acc: 0.843 - ETA: 6:05 - loss: 0.3881 - acc: 0.822 - ETA: 6:01 - loss: 0.3920 - acc: 0.816 - ETA: 5:56 - loss: 0.3912 - acc: 0.812 - ETA: 5:51 - loss: 0.3840 - acc: 0.822 - ETA: 5:46 - loss: 0.3807 - acc: 0.826 - ETA: 5:42 - loss: 0.3798 - acc: 0.824 - ETA: 5:37 - loss: 0.3812 - acc: 0.822 - ETA: 5:32 - loss: 0.3871 - acc: 0.818 - ETA: 5:27 - loss: 0.3844 - acc: 0.824 - ETA: 5:22 - loss: 0.3858 - acc: 0.827 - ETA: 5:18 - loss: 0.3860 - acc: 0.826 - ETA: 5:13 - loss: 0.3868 - acc: 0.824 - ETA: 5:08 - loss: 0.3915 - acc: 0.824 - ETA: 5:03 - loss: 0.3894 - acc: 0.828 - ETA: 4:58 - loss: 0.3926 - acc: 0.825 - ETA: 4:53 - loss: 0.3887 - acc: 0.829 - ETA: 4:49 - loss: 0.3853 - acc: 0.831 - ETA: 4:44 - loss: 0.3844 - acc: 0.832 - ETA: 4:39 - loss: 0.3823 - acc: 0.835 - ETA: 4:34 - loss: 0.3816 - acc: 0.837 - ETA: 4:29 - loss: 0.3816 - acc: 0.838 - ETA: 4:25 - loss: 0.3859 - acc: 0.834 - ETA: 4:20 - loss: 0.3865 - acc: 0.834 - ETA: 4:15 - loss: 0.3866 - acc: 0.834 - ETA: 4:10 - loss: 0.3865 - acc: 0.833 - ETA: 4:05 - loss: 0.3859 - acc: 0.834 - ETA: 4:01 - loss: 0.3845 - acc: 0.835 - ETA: 3:56 - loss: 0.3864 - acc: 0.833 - ETA: 3:51 - loss: 0.3857 - acc: 0.834 - ETA: 3:46 - loss: 0.3843 - acc: 0.835 - ETA: 3:41 - loss: 0.3840 - acc: 0.835 - ETA: 3:36 - loss: 0.3831 - acc: 0.835 - ETA: 3:32 - loss: 0.3831 - acc: 0.835 - ETA: 3:27 - loss: 0.3829 - acc: 0.836 - ETA: 3:22 - loss: 0.3806 - acc: 0.837 - ETA: 3:17 - loss: 0.3788 - acc: 0.840 - ETA: 3:12 - loss: 0.3782 - acc: 0.841 - ETA: 3:07 - loss: 0.3799 - acc: 0.839 - ETA: 3:03 - loss: 0.3788 - acc: 0.840 - ETA: 2:58 - loss: 0.3806 - acc: 0.839 - ETA: 2:53 - loss: 0.3811 - acc: 0.839 - ETA: 2:48 - loss: 0.3796 - acc: 0.840 - ETA: 2:43 - loss: 0.3790 - acc: 0.840 - ETA: 2:39 - loss: 0.3776 - acc: 0.841 - ETA: 2:34 - loss: 0.3777 - acc: 0.841 - ETA: 2:29 - loss: 0.3792 - acc: 0.840 - ETA: 2:24 - loss: 0.3802 - acc: 0.839 - ETA: 2:19 - loss: 0.3804 - acc: 0.839 - ETA: 2:14 - loss: 0.3809 - acc: 0.839 - ETA: 2:07 - loss: 0.3783 - acc: 0.842 - ETA: 2:03 - loss: 0.3775 - acc: 0.844 - ETA: 1:58 - loss: 0.3787 - acc: 0.843 - ETA: 1:53 - loss: 0.3775 - acc: 0.843 - ETA: 1:49 - loss: 0.3778 - acc: 0.843 - ETA: 1:44 - loss: 0.3769 - acc: 0.845 - ETA: 1:39 - loss: 0.3758 - acc: 0.845 - ETA: 1:34 - loss: 0.3751 - acc: 0.846 - ETA: 1:30 - loss: 0.3758 - acc: 0.845 - ETA: 1:25 - loss: 0.3758 - acc: 0.845 - ETA: 1:20 - loss: 0.3769 - acc: 0.844 - ETA: 1:16 - loss: 0.3770 - acc: 0.844 - ETA: 1:11 - loss: 0.3787 - acc: 0.843 - ETA: 1:06 - loss: 0.3778 - acc: 0.844 - ETA: 1:01 - loss: 0.3781 - acc: 0.844 - ETA: 57s - loss: 0.3781 - acc: 0.844 - ETA: 52s - loss: 0.3777 - acc: 0.84 - ETA: 47s - loss: 0.3782 - acc: 0.84 - ETA: 42s - loss: 0.3775 - acc: 0.84 - ETA: 38s - loss: 0.3772 - acc: 0.84 - ETA: 33s - loss: 0.3773 - acc: 0.84 - ETA: 28s - loss: 0.3774 - acc: 0.84 - ETA: 23s - loss: 0.3767 - acc: 0.84 - ETA: 19s - loss: 0.3770 - acc: 0.84 - ETA: 14s - loss: 0.3772 - acc: 0.84 - ETA: 9s - loss: 0.3767 - acc: 0.8448 - ETA: 4s - loss: 0.3759 - acc: 0.8452\n",
      "Epoch 00058: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3751 - acc: 0.8460 - val_loss: 0.6761 - val_acc: 0.6597\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.3860 - acc: 0.812 - ETA: 6:06 - loss: 0.3800 - acc: 0.817 - ETA: 6:01 - loss: 0.3820 - acc: 0.829 - ETA: 5:57 - loss: 0.3782 - acc: 0.835 - ETA: 5:52 - loss: 0.3936 - acc: 0.820 - ETA: 5:47 - loss: 0.3955 - acc: 0.821 - ETA: 5:42 - loss: 0.3838 - acc: 0.836 - ETA: 5:37 - loss: 0.3871 - acc: 0.834 - ETA: 5:32 - loss: 0.3862 - acc: 0.835 - ETA: 5:28 - loss: 0.3818 - acc: 0.842 - ETA: 5:23 - loss: 0.3761 - acc: 0.846 - ETA: 5:18 - loss: 0.3779 - acc: 0.845 - ETA: 5:13 - loss: 0.3773 - acc: 0.843 - ETA: 5:08 - loss: 0.3784 - acc: 0.842 - ETA: 5:03 - loss: 0.3794 - acc: 0.841 - ETA: 4:59 - loss: 0.3807 - acc: 0.839 - ETA: 4:54 - loss: 0.3849 - acc: 0.838 - ETA: 4:49 - loss: 0.3834 - acc: 0.840 - ETA: 4:44 - loss: 0.3854 - acc: 0.836 - ETA: 4:39 - loss: 0.3844 - acc: 0.835 - ETA: 4:34 - loss: 0.3838 - acc: 0.836 - ETA: 4:30 - loss: 0.3832 - acc: 0.836 - ETA: 4:25 - loss: 0.3862 - acc: 0.835 - ETA: 4:20 - loss: 0.3872 - acc: 0.834 - ETA: 4:15 - loss: 0.3838 - acc: 0.837 - ETA: 4:10 - loss: 0.3813 - acc: 0.840 - ETA: 4:05 - loss: 0.3797 - acc: 0.841 - ETA: 4:01 - loss: 0.3761 - acc: 0.843 - ETA: 3:56 - loss: 0.3747 - acc: 0.844 - ETA: 3:51 - loss: 0.3752 - acc: 0.842 - ETA: 3:46 - loss: 0.3759 - acc: 0.843 - ETA: 3:41 - loss: 0.3769 - acc: 0.843 - ETA: 3:37 - loss: 0.3757 - acc: 0.843 - ETA: 3:32 - loss: 0.3749 - acc: 0.844 - ETA: 3:27 - loss: 0.3743 - acc: 0.844 - ETA: 3:17 - loss: 0.3710 - acc: 0.849 - ETA: 3:13 - loss: 0.3714 - acc: 0.849 - ETA: 3:08 - loss: 0.3712 - acc: 0.849 - ETA: 3:03 - loss: 0.3711 - acc: 0.849 - ETA: 2:59 - loss: 0.3701 - acc: 0.849 - ETA: 2:54 - loss: 0.3684 - acc: 0.850 - ETA: 2:50 - loss: 0.3684 - acc: 0.850 - ETA: 2:45 - loss: 0.3697 - acc: 0.849 - ETA: 2:40 - loss: 0.3687 - acc: 0.850 - ETA: 2:36 - loss: 0.3677 - acc: 0.851 - ETA: 2:31 - loss: 0.3670 - acc: 0.852 - ETA: 2:26 - loss: 0.3655 - acc: 0.853 - ETA: 2:22 - loss: 0.3646 - acc: 0.854 - ETA: 2:17 - loss: 0.3651 - acc: 0.854 - ETA: 2:12 - loss: 0.3646 - acc: 0.854 - ETA: 2:08 - loss: 0.3657 - acc: 0.854 - ETA: 2:03 - loss: 0.3656 - acc: 0.854 - ETA: 1:58 - loss: 0.3646 - acc: 0.855 - ETA: 1:53 - loss: 0.3633 - acc: 0.855 - ETA: 1:49 - loss: 0.3631 - acc: 0.855 - ETA: 1:44 - loss: 0.3631 - acc: 0.854 - ETA: 1:39 - loss: 0.3641 - acc: 0.854 - ETA: 1:35 - loss: 0.3643 - acc: 0.853 - ETA: 1:30 - loss: 0.3630 - acc: 0.855 - ETA: 1:25 - loss: 0.3635 - acc: 0.854 - ETA: 1:20 - loss: 0.3638 - acc: 0.854 - ETA: 1:16 - loss: 0.3637 - acc: 0.854 - ETA: 1:11 - loss: 0.3638 - acc: 0.854 - ETA: 1:06 - loss: 0.3644 - acc: 0.854 - ETA: 1:01 - loss: 0.3649 - acc: 0.854 - ETA: 57s - loss: 0.3663 - acc: 0.852 - ETA: 52s - loss: 0.3652 - acc: 0.85 - ETA: 47s - loss: 0.3642 - acc: 0.85 - ETA: 42s - loss: 0.3647 - acc: 0.85 - ETA: 38s - loss: 0.3649 - acc: 0.85 - ETA: 33s - loss: 0.3646 - acc: 0.85 - ETA: 28s - loss: 0.3654 - acc: 0.85 - ETA: 23s - loss: 0.3657 - acc: 0.85 - ETA: 19s - loss: 0.3651 - acc: 0.85 - ETA: 14s - loss: 0.3657 - acc: 0.85 - ETA: 9s - loss: 0.3653 - acc: 0.8542 - ETA: 4s - loss: 0.3657 - acc: 0.8536\n",
      "Epoch 00059: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3658 - acc: 0.8533 - val_loss: 0.6685 - val_acc: 0.6686\n",
      "Epoch 60/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3005 - acc: 0.927 - ETA: 6:06 - loss: 0.2924 - acc: 0.921 - ETA: 6:01 - loss: 0.3146 - acc: 0.906 - ETA: 5:56 - loss: 0.3306 - acc: 0.882 - ETA: 5:51 - loss: 0.3336 - acc: 0.885 - ETA: 5:46 - loss: 0.3312 - acc: 0.883 - ETA: 5:42 - loss: 0.3340 - acc: 0.881 - ETA: 5:37 - loss: 0.3463 - acc: 0.867 - ETA: 5:32 - loss: 0.3451 - acc: 0.869 - ETA: 5:27 - loss: 0.3396 - acc: 0.875 - ETA: 5:22 - loss: 0.3454 - acc: 0.871 - ETA: 5:18 - loss: 0.3475 - acc: 0.869 - ETA: 5:13 - loss: 0.3479 - acc: 0.867 - ETA: 5:08 - loss: 0.3515 - acc: 0.864 - ETA: 5:03 - loss: 0.3531 - acc: 0.863 - ETA: 4:58 - loss: 0.3528 - acc: 0.865 - ETA: 4:54 - loss: 0.3503 - acc: 0.867 - ETA: 4:49 - loss: 0.3547 - acc: 0.864 - ETA: 4:44 - loss: 0.3555 - acc: 0.863 - ETA: 4:39 - loss: 0.3559 - acc: 0.863 - ETA: 4:34 - loss: 0.3563 - acc: 0.862 - ETA: 4:30 - loss: 0.3579 - acc: 0.860 - ETA: 4:25 - loss: 0.3582 - acc: 0.859 - ETA: 4:20 - loss: 0.3587 - acc: 0.859 - ETA: 4:15 - loss: 0.3578 - acc: 0.860 - ETA: 4:10 - loss: 0.3579 - acc: 0.861 - ETA: 4:05 - loss: 0.3603 - acc: 0.858 - ETA: 4:01 - loss: 0.3596 - acc: 0.859 - ETA: 3:56 - loss: 0.3608 - acc: 0.858 - ETA: 3:51 - loss: 0.3606 - acc: 0.859 - ETA: 3:46 - loss: 0.3592 - acc: 0.860 - ETA: 3:41 - loss: 0.3607 - acc: 0.859 - ETA: 3:36 - loss: 0.3603 - acc: 0.859 - ETA: 3:32 - loss: 0.3621 - acc: 0.858 - ETA: 3:27 - loss: 0.3623 - acc: 0.858 - ETA: 3:22 - loss: 0.3621 - acc: 0.858 - ETA: 3:17 - loss: 0.3626 - acc: 0.858 - ETA: 3:12 - loss: 0.3643 - acc: 0.857 - ETA: 3:08 - loss: 0.3653 - acc: 0.856 - ETA: 3:03 - loss: 0.3668 - acc: 0.855 - ETA: 2:54 - loss: 0.3650 - acc: 0.857 - ETA: 2:50 - loss: 0.3638 - acc: 0.857 - ETA: 2:45 - loss: 0.3648 - acc: 0.855 - ETA: 2:40 - loss: 0.3639 - acc: 0.856 - ETA: 2:36 - loss: 0.3640 - acc: 0.857 - ETA: 2:31 - loss: 0.3646 - acc: 0.856 - ETA: 2:26 - loss: 0.3655 - acc: 0.855 - ETA: 2:22 - loss: 0.3652 - acc: 0.855 - ETA: 2:17 - loss: 0.3655 - acc: 0.855 - ETA: 2:12 - loss: 0.3667 - acc: 0.854 - ETA: 2:07 - loss: 0.3677 - acc: 0.853 - ETA: 2:03 - loss: 0.3686 - acc: 0.852 - ETA: 1:58 - loss: 0.3676 - acc: 0.853 - ETA: 1:53 - loss: 0.3670 - acc: 0.853 - ETA: 1:49 - loss: 0.3676 - acc: 0.853 - ETA: 1:44 - loss: 0.3667 - acc: 0.853 - ETA: 1:39 - loss: 0.3671 - acc: 0.853 - ETA: 1:34 - loss: 0.3672 - acc: 0.853 - ETA: 1:30 - loss: 0.3669 - acc: 0.853 - ETA: 1:25 - loss: 0.3657 - acc: 0.854 - ETA: 1:20 - loss: 0.3657 - acc: 0.854 - ETA: 1:16 - loss: 0.3653 - acc: 0.854 - ETA: 1:11 - loss: 0.3670 - acc: 0.852 - ETA: 1:06 - loss: 0.3661 - acc: 0.852 - ETA: 1:01 - loss: 0.3665 - acc: 0.852 - ETA: 57s - loss: 0.3658 - acc: 0.853 - ETA: 52s - loss: 0.3661 - acc: 0.85 - ETA: 47s - loss: 0.3661 - acc: 0.85 - ETA: 42s - loss: 0.3654 - acc: 0.85 - ETA: 38s - loss: 0.3659 - acc: 0.85 - ETA: 33s - loss: 0.3665 - acc: 0.85 - ETA: 28s - loss: 0.3670 - acc: 0.85 - ETA: 23s - loss: 0.3686 - acc: 0.85 - ETA: 18s - loss: 0.3663 - acc: 0.85 - ETA: 14s - loss: 0.3671 - acc: 0.85 - ETA: 9s - loss: 0.3675 - acc: 0.8518 - ETA: 4s - loss: 0.3678 - acc: 0.8519\n",
      "Epoch 00060: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.3683 - acc: 0.8516 - val_loss: 0.6374 - val_acc: 0.6900\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3183 - acc: 0.885 - ETA: 6:06 - loss: 0.3735 - acc: 0.849 - ETA: 6:01 - loss: 0.3732 - acc: 0.847 - ETA: 5:56 - loss: 0.3821 - acc: 0.838 - ETA: 5:51 - loss: 0.3656 - acc: 0.850 - ETA: 5:47 - loss: 0.3686 - acc: 0.852 - ETA: 5:42 - loss: 0.3679 - acc: 0.852 - ETA: 5:37 - loss: 0.3645 - acc: 0.856 - ETA: 5:32 - loss: 0.3623 - acc: 0.858 - ETA: 5:28 - loss: 0.3615 - acc: 0.858 - ETA: 5:23 - loss: 0.3605 - acc: 0.859 - ETA: 5:18 - loss: 0.3638 - acc: 0.855 - ETA: 5:13 - loss: 0.3626 - acc: 0.853 - ETA: 5:08 - loss: 0.3649 - acc: 0.854 - ETA: 5:03 - loss: 0.3612 - acc: 0.857 - ETA: 4:58 - loss: 0.3624 - acc: 0.856 - ETA: 4:54 - loss: 0.3590 - acc: 0.860 - ETA: 4:49 - loss: 0.3572 - acc: 0.860 - ETA: 4:44 - loss: 0.3565 - acc: 0.861 - ETA: 4:39 - loss: 0.3566 - acc: 0.861 - ETA: 4:34 - loss: 0.3568 - acc: 0.861 - ETA: 4:29 - loss: 0.3581 - acc: 0.858 - ETA: 4:25 - loss: 0.3583 - acc: 0.858 - ETA: 4:20 - loss: 0.3603 - acc: 0.855 - ETA: 4:15 - loss: 0.3578 - acc: 0.857 - ETA: 4:10 - loss: 0.3574 - acc: 0.858 - ETA: 4:05 - loss: 0.3576 - acc: 0.859 - ETA: 4:01 - loss: 0.3575 - acc: 0.858 - ETA: 3:56 - loss: 0.3556 - acc: 0.860 - ETA: 3:51 - loss: 0.3557 - acc: 0.859 - ETA: 3:46 - loss: 0.3535 - acc: 0.862 - ETA: 3:41 - loss: 0.3525 - acc: 0.862 - ETA: 3:36 - loss: 0.3551 - acc: 0.859 - ETA: 3:32 - loss: 0.3544 - acc: 0.859 - ETA: 3:27 - loss: 0.3550 - acc: 0.859 - ETA: 3:22 - loss: 0.3563 - acc: 0.858 - ETA: 3:17 - loss: 0.3594 - acc: 0.855 - ETA: 3:12 - loss: 0.3606 - acc: 0.853 - ETA: 3:08 - loss: 0.3610 - acc: 0.852 - ETA: 3:03 - loss: 0.3605 - acc: 0.852 - ETA: 2:58 - loss: 0.3601 - acc: 0.852 - ETA: 2:53 - loss: 0.3611 - acc: 0.851 - ETA: 2:48 - loss: 0.3617 - acc: 0.851 - ETA: 2:43 - loss: 0.3622 - acc: 0.850 - ETA: 2:39 - loss: 0.3616 - acc: 0.851 - ETA: 2:34 - loss: 0.3616 - acc: 0.850 - ETA: 2:29 - loss: 0.3608 - acc: 0.851 - ETA: 2:24 - loss: 0.3612 - acc: 0.851 - ETA: 2:19 - loss: 0.3606 - acc: 0.851 - ETA: 2:15 - loss: 0.3610 - acc: 0.851 - ETA: 2:10 - loss: 0.3621 - acc: 0.849 - ETA: 2:05 - loss: 0.3641 - acc: 0.848 - ETA: 2:00 - loss: 0.3639 - acc: 0.849 - ETA: 1:55 - loss: 0.3635 - acc: 0.849 - ETA: 1:50 - loss: 0.3637 - acc: 0.849 - ETA: 1:46 - loss: 0.3630 - acc: 0.850 - ETA: 1:41 - loss: 0.3622 - acc: 0.850 - ETA: 1:36 - loss: 0.3617 - acc: 0.851 - ETA: 1:31 - loss: 0.3622 - acc: 0.850 - ETA: 1:26 - loss: 0.3617 - acc: 0.851 - ETA: 1:21 - loss: 0.3612 - acc: 0.852 - ETA: 1:17 - loss: 0.3617 - acc: 0.852 - ETA: 1:12 - loss: 0.3627 - acc: 0.851 - ETA: 1:07 - loss: 0.3628 - acc: 0.850 - ETA: 1:02 - loss: 0.3617 - acc: 0.852 - ETA: 57s - loss: 0.3624 - acc: 0.851 - ETA: 53s - loss: 0.3626 - acc: 0.85 - ETA: 48s - loss: 0.3621 - acc: 0.85 - ETA: 43s - loss: 0.3624 - acc: 0.85 - ETA: 38s - loss: 0.3639 - acc: 0.85 - ETA: 33s - loss: 0.3643 - acc: 0.85 - ETA: 28s - loss: 0.3658 - acc: 0.84 - ETA: 24s - loss: 0.3663 - acc: 0.84 - ETA: 19s - loss: 0.3667 - acc: 0.84 - ETA: 14s - loss: 0.3664 - acc: 0.84 - ETA: 9s - loss: 0.3666 - acc: 0.8492 - ETA: 4s - loss: 0.3670 - acc: 0.8492\n",
      "Epoch 00061: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.3667 - acc: 0.8492 - val_loss: 0.6674 - val_acc: 0.6673\n",
      "Epoch 62/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3455 - acc: 0.843 - ETA: 6:06 - loss: 0.3258 - acc: 0.875 - ETA: 6:01 - loss: 0.3374 - acc: 0.864 - ETA: 5:56 - loss: 0.3727 - acc: 0.838 - ETA: 5:51 - loss: 0.3674 - acc: 0.850 - ETA: 5:46 - loss: 0.3654 - acc: 0.850 - ETA: 5:42 - loss: 0.3688 - acc: 0.852 - ETA: 5:37 - loss: 0.3737 - acc: 0.852 - ETA: 5:32 - loss: 0.3804 - acc: 0.847 - ETA: 5:27 - loss: 0.3761 - acc: 0.846 - ETA: 5:22 - loss: 0.3751 - acc: 0.844 - ETA: 5:18 - loss: 0.3721 - acc: 0.847 - ETA: 5:13 - loss: 0.3729 - acc: 0.847 - ETA: 5:08 - loss: 0.3681 - acc: 0.851 - ETA: 5:03 - loss: 0.3679 - acc: 0.851 - ETA: 4:58 - loss: 0.3669 - acc: 0.853 - ETA: 4:54 - loss: 0.3652 - acc: 0.856 - ETA: 4:49 - loss: 0.3668 - acc: 0.854 - ETA: 4:44 - loss: 0.3668 - acc: 0.854 - ETA: 4:39 - loss: 0.3667 - acc: 0.854 - ETA: 4:34 - loss: 0.3657 - acc: 0.856 - ETA: 4:29 - loss: 0.3658 - acc: 0.857 - ETA: 4:25 - loss: 0.3648 - acc: 0.858 - ETA: 4:20 - loss: 0.3643 - acc: 0.858 - ETA: 4:15 - loss: 0.3628 - acc: 0.858 - ETA: 4:10 - loss: 0.3644 - acc: 0.857 - ETA: 4:05 - loss: 0.3633 - acc: 0.857 - ETA: 4:00 - loss: 0.3626 - acc: 0.857 - ETA: 3:56 - loss: 0.3631 - acc: 0.857 - ETA: 3:51 - loss: 0.3622 - acc: 0.858 - ETA: 3:46 - loss: 0.3615 - acc: 0.858 - ETA: 3:41 - loss: 0.3631 - acc: 0.856 - ETA: 3:36 - loss: 0.3646 - acc: 0.855 - ETA: 3:32 - loss: 0.3646 - acc: 0.856 - ETA: 3:27 - loss: 0.3660 - acc: 0.856 - ETA: 3:22 - loss: 0.3676 - acc: 0.854 - ETA: 3:17 - loss: 0.3664 - acc: 0.855 - ETA: 3:12 - loss: 0.3658 - acc: 0.856 - ETA: 3:08 - loss: 0.3677 - acc: 0.854 - ETA: 3:03 - loss: 0.3666 - acc: 0.855 - ETA: 2:58 - loss: 0.3665 - acc: 0.855 - ETA: 2:53 - loss: 0.3642 - acc: 0.857 - ETA: 2:48 - loss: 0.3646 - acc: 0.857 - ETA: 2:43 - loss: 0.3643 - acc: 0.857 - ETA: 2:39 - loss: 0.3628 - acc: 0.857 - ETA: 2:34 - loss: 0.3632 - acc: 0.857 - ETA: 2:29 - loss: 0.3636 - acc: 0.857 - ETA: 2:24 - loss: 0.3631 - acc: 0.858 - ETA: 2:19 - loss: 0.3660 - acc: 0.855 - ETA: 2:14 - loss: 0.3666 - acc: 0.855 - ETA: 2:10 - loss: 0.3662 - acc: 0.855 - ETA: 2:03 - loss: 0.3706 - acc: 0.853 - ETA: 1:58 - loss: 0.3720 - acc: 0.852 - ETA: 1:53 - loss: 0.3713 - acc: 0.852 - ETA: 1:49 - loss: 0.3699 - acc: 0.853 - ETA: 1:44 - loss: 0.3692 - acc: 0.854 - ETA: 1:39 - loss: 0.3689 - acc: 0.854 - ETA: 1:34 - loss: 0.3677 - acc: 0.856 - ETA: 1:30 - loss: 0.3672 - acc: 0.856 - ETA: 1:25 - loss: 0.3680 - acc: 0.856 - ETA: 1:20 - loss: 0.3689 - acc: 0.855 - ETA: 1:16 - loss: 0.3687 - acc: 0.855 - ETA: 1:11 - loss: 0.3688 - acc: 0.856 - ETA: 1:06 - loss: 0.3685 - acc: 0.855 - ETA: 1:01 - loss: 0.3679 - acc: 0.856 - ETA: 57s - loss: 0.3680 - acc: 0.856 - ETA: 52s - loss: 0.3675 - acc: 0.85 - ETA: 47s - loss: 0.3669 - acc: 0.85 - ETA: 42s - loss: 0.3663 - acc: 0.85 - ETA: 38s - loss: 0.3655 - acc: 0.85 - ETA: 33s - loss: 0.3651 - acc: 0.85 - ETA: 28s - loss: 0.3656 - acc: 0.85 - ETA: 23s - loss: 0.3669 - acc: 0.85 - ETA: 19s - loss: 0.3679 - acc: 0.85 - ETA: 14s - loss: 0.3687 - acc: 0.85 - ETA: 9s - loss: 0.3686 - acc: 0.8557 - ETA: 4s - loss: 0.3681 - acc: 0.8559\n",
      "Epoch 00062: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3682 - acc: 0.8558 - val_loss: 0.7133 - val_acc: 0.6376\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3125 - acc: 0.906 - ETA: 6:06 - loss: 0.3466 - acc: 0.869 - ETA: 6:01 - loss: 0.3507 - acc: 0.857 - ETA: 5:56 - loss: 0.3594 - acc: 0.854 - ETA: 5:51 - loss: 0.3546 - acc: 0.864 - ETA: 5:47 - loss: 0.3620 - acc: 0.859 - ETA: 5:42 - loss: 0.3733 - acc: 0.845 - ETA: 5:37 - loss: 0.3664 - acc: 0.846 - ETA: 5:32 - loss: 0.3655 - acc: 0.849 - ETA: 5:27 - loss: 0.3691 - acc: 0.847 - ETA: 5:23 - loss: 0.3681 - acc: 0.846 - ETA: 5:18 - loss: 0.3583 - acc: 0.854 - ETA: 5:13 - loss: 0.3619 - acc: 0.850 - ETA: 5:08 - loss: 0.3619 - acc: 0.848 - ETA: 5:03 - loss: 0.3577 - acc: 0.853 - ETA: 4:58 - loss: 0.3564 - acc: 0.856 - ETA: 4:54 - loss: 0.3559 - acc: 0.859 - ETA: 4:49 - loss: 0.3575 - acc: 0.857 - ETA: 4:44 - loss: 0.3589 - acc: 0.856 - ETA: 4:39 - loss: 0.3553 - acc: 0.861 - ETA: 4:34 - loss: 0.3566 - acc: 0.858 - ETA: 4:30 - loss: 0.3572 - acc: 0.858 - ETA: 4:25 - loss: 0.3584 - acc: 0.857 - ETA: 4:20 - loss: 0.3593 - acc: 0.857 - ETA: 4:15 - loss: 0.3609 - acc: 0.856 - ETA: 4:10 - loss: 0.3624 - acc: 0.855 - ETA: 4:05 - loss: 0.3626 - acc: 0.855 - ETA: 4:01 - loss: 0.3640 - acc: 0.853 - ETA: 3:56 - loss: 0.3665 - acc: 0.852 - ETA: 3:51 - loss: 0.3666 - acc: 0.853 - ETA: 3:46 - loss: 0.3658 - acc: 0.853 - ETA: 3:41 - loss: 0.3654 - acc: 0.854 - ETA: 3:37 - loss: 0.3665 - acc: 0.853 - ETA: 3:32 - loss: 0.3657 - acc: 0.854 - ETA: 3:27 - loss: 0.3640 - acc: 0.855 - ETA: 3:22 - loss: 0.3633 - acc: 0.855 - ETA: 3:17 - loss: 0.3663 - acc: 0.853 - ETA: 3:12 - loss: 0.3658 - acc: 0.853 - ETA: 3:08 - loss: 0.3661 - acc: 0.852 - ETA: 3:03 - loss: 0.3662 - acc: 0.852 - ETA: 2:58 - loss: 0.3651 - acc: 0.852 - ETA: 2:53 - loss: 0.3668 - acc: 0.851 - ETA: 2:48 - loss: 0.3654 - acc: 0.852 - ETA: 2:43 - loss: 0.3643 - acc: 0.852 - ETA: 2:39 - loss: 0.3643 - acc: 0.852 - ETA: 2:34 - loss: 0.3652 - acc: 0.851 - ETA: 2:29 - loss: 0.3641 - acc: 0.852 - ETA: 2:24 - loss: 0.3653 - acc: 0.852 - ETA: 2:19 - loss: 0.3645 - acc: 0.852 - ETA: 2:15 - loss: 0.3639 - acc: 0.852 - ETA: 2:10 - loss: 0.3643 - acc: 0.852 - ETA: 2:05 - loss: 0.3629 - acc: 0.854 - ETA: 2:00 - loss: 0.3638 - acc: 0.853 - ETA: 1:55 - loss: 0.3629 - acc: 0.854 - ETA: 1:50 - loss: 0.3629 - acc: 0.854 - ETA: 1:46 - loss: 0.3634 - acc: 0.854 - ETA: 1:41 - loss: 0.3641 - acc: 0.853 - ETA: 1:36 - loss: 0.3650 - acc: 0.853 - ETA: 1:31 - loss: 0.3642 - acc: 0.853 - ETA: 1:26 - loss: 0.3629 - acc: 0.854 - ETA: 1:21 - loss: 0.3630 - acc: 0.855 - ETA: 1:16 - loss: 0.3644 - acc: 0.855 - ETA: 1:11 - loss: 0.3646 - acc: 0.854 - ETA: 1:06 - loss: 0.3645 - acc: 0.854 - ETA: 1:01 - loss: 0.3646 - acc: 0.854 - ETA: 57s - loss: 0.3651 - acc: 0.854 - ETA: 52s - loss: 0.3649 - acc: 0.85 - ETA: 47s - loss: 0.3651 - acc: 0.85 - ETA: 42s - loss: 0.3653 - acc: 0.85 - ETA: 38s - loss: 0.3646 - acc: 0.85 - ETA: 33s - loss: 0.3651 - acc: 0.85 - ETA: 28s - loss: 0.3644 - acc: 0.85 - ETA: 23s - loss: 0.3648 - acc: 0.85 - ETA: 19s - loss: 0.3651 - acc: 0.85 - ETA: 14s - loss: 0.3647 - acc: 0.85 - ETA: 9s - loss: 0.3651 - acc: 0.8544 - ETA: 4s - loss: 0.3661 - acc: 0.8536\n",
      "Epoch 00063: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3660 - acc: 0.8542 - val_loss: 0.6632 - val_acc: 0.6761\n",
      "Epoch 64/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3496 - acc: 0.854 - ETA: 6:06 - loss: 0.3683 - acc: 0.849 - ETA: 6:01 - loss: 0.3870 - acc: 0.836 - ETA: 5:57 - loss: 0.4066 - acc: 0.825 - ETA: 5:52 - loss: 0.4010 - acc: 0.827 - ETA: 5:47 - loss: 0.4061 - acc: 0.822 - ETA: 5:42 - loss: 0.3883 - acc: 0.839 - ETA: 5:37 - loss: 0.3848 - acc: 0.842 - ETA: 5:32 - loss: 0.3845 - acc: 0.843 - ETA: 5:28 - loss: 0.3792 - acc: 0.845 - ETA: 5:23 - loss: 0.3841 - acc: 0.840 - ETA: 5:18 - loss: 0.3784 - acc: 0.846 - ETA: 5:13 - loss: 0.3797 - acc: 0.844 - ETA: 5:08 - loss: 0.3753 - acc: 0.849 - ETA: 5:03 - loss: 0.3729 - acc: 0.851 - ETA: 4:59 - loss: 0.3722 - acc: 0.852 - ETA: 4:54 - loss: 0.3754 - acc: 0.850 - ETA: 4:49 - loss: 0.3734 - acc: 0.851 - ETA: 4:44 - loss: 0.3714 - acc: 0.850 - ETA: 4:39 - loss: 0.3693 - acc: 0.851 - ETA: 4:34 - loss: 0.3693 - acc: 0.849 - ETA: 4:30 - loss: 0.3701 - acc: 0.850 - ETA: 4:25 - loss: 0.3726 - acc: 0.848 - ETA: 4:20 - loss: 0.3749 - acc: 0.845 - ETA: 4:15 - loss: 0.3721 - acc: 0.848 - ETA: 4:10 - loss: 0.3706 - acc: 0.849 - ETA: 4:05 - loss: 0.3703 - acc: 0.848 - ETA: 4:01 - loss: 0.3693 - acc: 0.850 - ETA: 3:56 - loss: 0.3685 - acc: 0.850 - ETA: 3:51 - loss: 0.3681 - acc: 0.850 - ETA: 3:46 - loss: 0.3648 - acc: 0.853 - ETA: 3:41 - loss: 0.3652 - acc: 0.854 - ETA: 3:36 - loss: 0.3657 - acc: 0.853 - ETA: 3:32 - loss: 0.3653 - acc: 0.854 - ETA: 3:27 - loss: 0.3649 - acc: 0.855 - ETA: 3:22 - loss: 0.3652 - acc: 0.855 - ETA: 3:17 - loss: 0.3637 - acc: 0.856 - ETA: 3:12 - loss: 0.3651 - acc: 0.855 - ETA: 3:08 - loss: 0.3649 - acc: 0.855 - ETA: 2:59 - loss: 0.3628 - acc: 0.857 - ETA: 2:54 - loss: 0.3633 - acc: 0.857 - ETA: 2:50 - loss: 0.3621 - acc: 0.858 - ETA: 2:45 - loss: 0.3624 - acc: 0.858 - ETA: 2:40 - loss: 0.3625 - acc: 0.858 - ETA: 2:36 - loss: 0.3656 - acc: 0.856 - ETA: 2:31 - loss: 0.3656 - acc: 0.856 - ETA: 2:26 - loss: 0.3662 - acc: 0.856 - ETA: 2:22 - loss: 0.3653 - acc: 0.856 - ETA: 2:17 - loss: 0.3641 - acc: 0.856 - ETA: 2:12 - loss: 0.3633 - acc: 0.857 - ETA: 2:08 - loss: 0.3642 - acc: 0.856 - ETA: 2:03 - loss: 0.3628 - acc: 0.857 - ETA: 1:58 - loss: 0.3625 - acc: 0.857 - ETA: 1:53 - loss: 0.3621 - acc: 0.857 - ETA: 1:49 - loss: 0.3608 - acc: 0.859 - ETA: 1:44 - loss: 0.3594 - acc: 0.860 - ETA: 1:39 - loss: 0.3594 - acc: 0.860 - ETA: 1:35 - loss: 0.3579 - acc: 0.861 - ETA: 1:30 - loss: 0.3579 - acc: 0.860 - ETA: 1:25 - loss: 0.3578 - acc: 0.860 - ETA: 1:20 - loss: 0.3582 - acc: 0.859 - ETA: 1:16 - loss: 0.3593 - acc: 0.859 - ETA: 1:11 - loss: 0.3603 - acc: 0.858 - ETA: 1:06 - loss: 0.3607 - acc: 0.857 - ETA: 1:01 - loss: 0.3602 - acc: 0.857 - ETA: 57s - loss: 0.3596 - acc: 0.857 - ETA: 52s - loss: 0.3610 - acc: 0.85 - ETA: 47s - loss: 0.3609 - acc: 0.85 - ETA: 42s - loss: 0.3606 - acc: 0.85 - ETA: 38s - loss: 0.3613 - acc: 0.85 - ETA: 33s - loss: 0.3602 - acc: 0.85 - ETA: 28s - loss: 0.3597 - acc: 0.85 - ETA: 23s - loss: 0.3602 - acc: 0.85 - ETA: 19s - loss: 0.3612 - acc: 0.85 - ETA: 14s - loss: 0.3605 - acc: 0.85 - ETA: 9s - loss: 0.3598 - acc: 0.8565 - ETA: 4s - loss: 0.3595 - acc: 0.8570\n",
      "Epoch 00064: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3597 - acc: 0.8564 - val_loss: 0.6203 - val_acc: 0.6875\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.2903 - acc: 0.885 - ETA: 6:05 - loss: 0.3408 - acc: 0.854 - ETA: 6:01 - loss: 0.3721 - acc: 0.836 - ETA: 5:56 - loss: 0.3703 - acc: 0.838 - ETA: 5:51 - loss: 0.3623 - acc: 0.841 - ETA: 5:46 - loss: 0.3603 - acc: 0.847 - ETA: 5:42 - loss: 0.3513 - acc: 0.851 - ETA: 5:37 - loss: 0.3449 - acc: 0.860 - ETA: 5:32 - loss: 0.3460 - acc: 0.863 - ETA: 5:28 - loss: 0.3483 - acc: 0.866 - ETA: 5:23 - loss: 0.3505 - acc: 0.866 - ETA: 5:18 - loss: 0.3531 - acc: 0.862 - ETA: 5:13 - loss: 0.3538 - acc: 0.863 - ETA: 5:08 - loss: 0.3505 - acc: 0.864 - ETA: 5:03 - loss: 0.3525 - acc: 0.862 - ETA: 4:59 - loss: 0.3546 - acc: 0.861 - ETA: 4:54 - loss: 0.3534 - acc: 0.863 - ETA: 4:49 - loss: 0.3545 - acc: 0.863 - ETA: 4:44 - loss: 0.3553 - acc: 0.862 - ETA: 4:39 - loss: 0.3541 - acc: 0.863 - ETA: 4:34 - loss: 0.3575 - acc: 0.862 - ETA: 4:30 - loss: 0.3552 - acc: 0.864 - ETA: 4:25 - loss: 0.3582 - acc: 0.861 - ETA: 4:11 - loss: 0.3559 - acc: 0.860 - ETA: 4:06 - loss: 0.3553 - acc: 0.860 - ETA: 4:02 - loss: 0.3524 - acc: 0.862 - ETA: 3:58 - loss: 0.3527 - acc: 0.861 - ETA: 3:53 - loss: 0.3520 - acc: 0.863 - ETA: 3:49 - loss: 0.3507 - acc: 0.863 - ETA: 3:44 - loss: 0.3516 - acc: 0.862 - ETA: 3:40 - loss: 0.3516 - acc: 0.861 - ETA: 3:35 - loss: 0.3515 - acc: 0.862 - ETA: 3:31 - loss: 0.3525 - acc: 0.861 - ETA: 3:26 - loss: 0.3568 - acc: 0.857 - ETA: 3:22 - loss: 0.3561 - acc: 0.857 - ETA: 3:17 - loss: 0.3563 - acc: 0.857 - ETA: 3:13 - loss: 0.3586 - acc: 0.856 - ETA: 3:08 - loss: 0.3593 - acc: 0.856 - ETA: 3:03 - loss: 0.3599 - acc: 0.855 - ETA: 2:59 - loss: 0.3596 - acc: 0.855 - ETA: 2:54 - loss: 0.3611 - acc: 0.853 - ETA: 2:50 - loss: 0.3605 - acc: 0.855 - ETA: 2:45 - loss: 0.3601 - acc: 0.856 - ETA: 2:40 - loss: 0.3599 - acc: 0.856 - ETA: 2:36 - loss: 0.3611 - acc: 0.855 - ETA: 2:31 - loss: 0.3607 - acc: 0.856 - ETA: 2:26 - loss: 0.3614 - acc: 0.855 - ETA: 2:22 - loss: 0.3623 - acc: 0.855 - ETA: 2:17 - loss: 0.3625 - acc: 0.855 - ETA: 2:12 - loss: 0.3614 - acc: 0.856 - ETA: 2:08 - loss: 0.3609 - acc: 0.856 - ETA: 2:03 - loss: 0.3595 - acc: 0.856 - ETA: 1:58 - loss: 0.3595 - acc: 0.857 - ETA: 1:53 - loss: 0.3597 - acc: 0.857 - ETA: 1:49 - loss: 0.3604 - acc: 0.856 - ETA: 1:44 - loss: 0.3610 - acc: 0.856 - ETA: 1:39 - loss: 0.3621 - acc: 0.856 - ETA: 1:35 - loss: 0.3622 - acc: 0.855 - ETA: 1:30 - loss: 0.3610 - acc: 0.856 - ETA: 1:25 - loss: 0.3604 - acc: 0.856 - ETA: 1:20 - loss: 0.3606 - acc: 0.856 - ETA: 1:16 - loss: 0.3613 - acc: 0.855 - ETA: 1:11 - loss: 0.3613 - acc: 0.855 - ETA: 1:06 - loss: 0.3616 - acc: 0.855 - ETA: 1:01 - loss: 0.3622 - acc: 0.855 - ETA: 57s - loss: 0.3627 - acc: 0.854 - ETA: 52s - loss: 0.3627 - acc: 0.85 - ETA: 47s - loss: 0.3631 - acc: 0.85 - ETA: 42s - loss: 0.3637 - acc: 0.85 - ETA: 38s - loss: 0.3641 - acc: 0.85 - ETA: 33s - loss: 0.3634 - acc: 0.85 - ETA: 28s - loss: 0.3624 - acc: 0.85 - ETA: 23s - loss: 0.3625 - acc: 0.85 - ETA: 19s - loss: 0.3618 - acc: 0.85 - ETA: 14s - loss: 0.3609 - acc: 0.85 - ETA: 9s - loss: 0.3613 - acc: 0.8555 - ETA: 4s - loss: 0.3613 - acc: 0.8554\n",
      "Epoch 00065: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3614 - acc: 0.8554 - val_loss: 0.6855 - val_acc: 0.6635\n",
      "Epoch 66/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3640 - acc: 0.875 - ETA: 6:05 - loss: 0.3696 - acc: 0.849 - ETA: 6:01 - loss: 0.3908 - acc: 0.829 - ETA: 5:56 - loss: 0.3805 - acc: 0.835 - ETA: 5:51 - loss: 0.3618 - acc: 0.852 - ETA: 5:46 - loss: 0.3706 - acc: 0.842 - ETA: 5:42 - loss: 0.3695 - acc: 0.843 - ETA: 5:37 - loss: 0.3713 - acc: 0.845 - ETA: 5:32 - loss: 0.3696 - acc: 0.849 - ETA: 5:27 - loss: 0.3713 - acc: 0.846 - ETA: 5:22 - loss: 0.3648 - acc: 0.854 - ETA: 5:18 - loss: 0.3562 - acc: 0.859 - ETA: 5:13 - loss: 0.3592 - acc: 0.855 - ETA: 5:08 - loss: 0.3639 - acc: 0.850 - ETA: 5:03 - loss: 0.3650 - acc: 0.848 - ETA: 4:58 - loss: 0.3650 - acc: 0.849 - ETA: 4:53 - loss: 0.3667 - acc: 0.845 - ETA: 4:49 - loss: 0.3693 - acc: 0.843 - ETA: 4:44 - loss: 0.3647 - acc: 0.847 - ETA: 4:39 - loss: 0.3620 - acc: 0.850 - ETA: 4:34 - loss: 0.3614 - acc: 0.850 - ETA: 4:29 - loss: 0.3600 - acc: 0.852 - ETA: 4:25 - loss: 0.3626 - acc: 0.850 - ETA: 4:20 - loss: 0.3647 - acc: 0.847 - ETA: 4:06 - loss: 0.3681 - acc: 0.846 - ETA: 4:02 - loss: 0.3645 - acc: 0.849 - ETA: 3:57 - loss: 0.3638 - acc: 0.851 - ETA: 3:53 - loss: 0.3618 - acc: 0.853 - ETA: 3:49 - loss: 0.3601 - acc: 0.855 - ETA: 3:44 - loss: 0.3591 - acc: 0.856 - ETA: 3:40 - loss: 0.3598 - acc: 0.856 - ETA: 3:35 - loss: 0.3583 - acc: 0.857 - ETA: 3:31 - loss: 0.3573 - acc: 0.858 - ETA: 3:26 - loss: 0.3576 - acc: 0.858 - ETA: 3:22 - loss: 0.3563 - acc: 0.859 - ETA: 3:17 - loss: 0.3561 - acc: 0.860 - ETA: 3:13 - loss: 0.3554 - acc: 0.861 - ETA: 3:08 - loss: 0.3565 - acc: 0.859 - ETA: 3:03 - loss: 0.3559 - acc: 0.860 - ETA: 2:59 - loss: 0.3564 - acc: 0.859 - ETA: 2:54 - loss: 0.3579 - acc: 0.858 - ETA: 2:49 - loss: 0.3565 - acc: 0.859 - ETA: 2:45 - loss: 0.3550 - acc: 0.859 - ETA: 2:40 - loss: 0.3566 - acc: 0.858 - ETA: 2:36 - loss: 0.3577 - acc: 0.858 - ETA: 2:31 - loss: 0.3579 - acc: 0.858 - ETA: 2:26 - loss: 0.3589 - acc: 0.857 - ETA: 2:22 - loss: 0.3588 - acc: 0.857 - ETA: 2:17 - loss: 0.3584 - acc: 0.857 - ETA: 2:12 - loss: 0.3575 - acc: 0.858 - ETA: 2:07 - loss: 0.3565 - acc: 0.859 - ETA: 2:03 - loss: 0.3566 - acc: 0.859 - ETA: 1:58 - loss: 0.3547 - acc: 0.861 - ETA: 1:53 - loss: 0.3548 - acc: 0.860 - ETA: 1:49 - loss: 0.3534 - acc: 0.862 - ETA: 1:44 - loss: 0.3538 - acc: 0.861 - ETA: 1:39 - loss: 0.3542 - acc: 0.861 - ETA: 1:34 - loss: 0.3533 - acc: 0.862 - ETA: 1:30 - loss: 0.3531 - acc: 0.862 - ETA: 1:25 - loss: 0.3536 - acc: 0.862 - ETA: 1:20 - loss: 0.3535 - acc: 0.862 - ETA: 1:16 - loss: 0.3526 - acc: 0.863 - ETA: 1:11 - loss: 0.3525 - acc: 0.864 - ETA: 1:06 - loss: 0.3527 - acc: 0.863 - ETA: 1:01 - loss: 0.3514 - acc: 0.864 - ETA: 57s - loss: 0.3512 - acc: 0.865 - ETA: 52s - loss: 0.3516 - acc: 0.86 - ETA: 47s - loss: 0.3509 - acc: 0.86 - ETA: 42s - loss: 0.3504 - acc: 0.86 - ETA: 38s - loss: 0.3503 - acc: 0.86 - ETA: 33s - loss: 0.3509 - acc: 0.86 - ETA: 28s - loss: 0.3524 - acc: 0.86 - ETA: 23s - loss: 0.3530 - acc: 0.86 - ETA: 19s - loss: 0.3529 - acc: 0.86 - ETA: 14s - loss: 0.3522 - acc: 0.86 - ETA: 9s - loss: 0.3523 - acc: 0.8639 - ETA: 4s - loss: 0.3518 - acc: 0.8643\n",
      "Epoch 00066: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3520 - acc: 0.8642 - val_loss: 0.6651 - val_acc: 0.6717\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3933 - acc: 0.812 - ETA: 6:06 - loss: 0.3568 - acc: 0.859 - ETA: 6:01 - loss: 0.3485 - acc: 0.864 - ETA: 5:56 - loss: 0.3469 - acc: 0.859 - ETA: 5:51 - loss: 0.3464 - acc: 0.864 - ETA: 5:47 - loss: 0.3460 - acc: 0.862 - ETA: 5:42 - loss: 0.3503 - acc: 0.855 - ETA: 5:37 - loss: 0.3567 - acc: 0.851 - ETA: 5:32 - loss: 0.3594 - acc: 0.846 - ETA: 5:27 - loss: 0.3645 - acc: 0.843 - ETA: 5:23 - loss: 0.3641 - acc: 0.845 - ETA: 5:18 - loss: 0.3582 - acc: 0.850 - ETA: 5:13 - loss: 0.3634 - acc: 0.846 - ETA: 5:08 - loss: 0.3634 - acc: 0.847 - ETA: 5:03 - loss: 0.3588 - acc: 0.851 - ETA: 4:59 - loss: 0.3631 - acc: 0.848 - ETA: 4:54 - loss: 0.3598 - acc: 0.852 - ETA: 4:49 - loss: 0.3601 - acc: 0.850 - ETA: 4:44 - loss: 0.3613 - acc: 0.851 - ETA: 4:39 - loss: 0.3610 - acc: 0.852 - ETA: 4:34 - loss: 0.3594 - acc: 0.854 - ETA: 4:30 - loss: 0.3563 - acc: 0.856 - ETA: 4:25 - loss: 0.3569 - acc: 0.856 - ETA: 4:20 - loss: 0.3561 - acc: 0.857 - ETA: 4:15 - loss: 0.3574 - acc: 0.855 - ETA: 4:10 - loss: 0.3567 - acc: 0.856 - ETA: 4:06 - loss: 0.3556 - acc: 0.856 - ETA: 4:01 - loss: 0.3549 - acc: 0.857 - ETA: 3:56 - loss: 0.3543 - acc: 0.857 - ETA: 3:51 - loss: 0.3537 - acc: 0.858 - ETA: 3:46 - loss: 0.3546 - acc: 0.858 - ETA: 3:41 - loss: 0.3540 - acc: 0.858 - ETA: 3:37 - loss: 0.3527 - acc: 0.859 - ETA: 3:32 - loss: 0.3524 - acc: 0.859 - ETA: 3:27 - loss: 0.3510 - acc: 0.860 - ETA: 3:22 - loss: 0.3494 - acc: 0.862 - ETA: 3:17 - loss: 0.3486 - acc: 0.862 - ETA: 3:12 - loss: 0.3497 - acc: 0.862 - ETA: 3:08 - loss: 0.3494 - acc: 0.862 - ETA: 3:03 - loss: 0.3502 - acc: 0.862 - ETA: 2:58 - loss: 0.3493 - acc: 0.863 - ETA: 2:53 - loss: 0.3483 - acc: 0.864 - ETA: 2:48 - loss: 0.3494 - acc: 0.862 - ETA: 2:43 - loss: 0.3507 - acc: 0.861 - ETA: 2:39 - loss: 0.3518 - acc: 0.859 - ETA: 2:34 - loss: 0.3522 - acc: 0.859 - ETA: 2:29 - loss: 0.3520 - acc: 0.859 - ETA: 2:24 - loss: 0.3519 - acc: 0.859 - ETA: 2:19 - loss: 0.3525 - acc: 0.859 - ETA: 2:15 - loss: 0.3528 - acc: 0.858 - ETA: 2:10 - loss: 0.3523 - acc: 0.859 - ETA: 2:05 - loss: 0.3543 - acc: 0.858 - ETA: 2:00 - loss: 0.3548 - acc: 0.857 - ETA: 1:55 - loss: 0.3535 - acc: 0.858 - ETA: 1:50 - loss: 0.3517 - acc: 0.859 - ETA: 1:46 - loss: 0.3517 - acc: 0.860 - ETA: 1:41 - loss: 0.3516 - acc: 0.860 - ETA: 1:36 - loss: 0.3508 - acc: 0.860 - ETA: 1:31 - loss: 0.3503 - acc: 0.861 - ETA: 1:26 - loss: 0.3506 - acc: 0.861 - ETA: 1:21 - loss: 0.3510 - acc: 0.860 - ETA: 1:16 - loss: 0.3547 - acc: 0.856 - ETA: 1:11 - loss: 0.3557 - acc: 0.855 - ETA: 1:06 - loss: 0.3553 - acc: 0.855 - ETA: 1:01 - loss: 0.3540 - acc: 0.856 - ETA: 57s - loss: 0.3539 - acc: 0.856 - ETA: 52s - loss: 0.3545 - acc: 0.85 - ETA: 47s - loss: 0.3541 - acc: 0.85 - ETA: 42s - loss: 0.3538 - acc: 0.85 - ETA: 38s - loss: 0.3536 - acc: 0.85 - ETA: 33s - loss: 0.3538 - acc: 0.85 - ETA: 28s - loss: 0.3537 - acc: 0.85 - ETA: 23s - loss: 0.3542 - acc: 0.85 - ETA: 19s - loss: 0.3548 - acc: 0.85 - ETA: 14s - loss: 0.3544 - acc: 0.85 - ETA: 9s - loss: 0.3541 - acc: 0.8555 - ETA: 4s - loss: 0.3531 - acc: 0.8563\n",
      "Epoch 00067: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3527 - acc: 0.8569 - val_loss: 0.6659 - val_acc: 0.6742\n",
      "Epoch 68/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4520 - acc: 0.781 - ETA: 6:06 - loss: 0.4044 - acc: 0.817 - ETA: 6:01 - loss: 0.3811 - acc: 0.840 - ETA: 5:56 - loss: 0.3677 - acc: 0.849 - ETA: 5:51 - loss: 0.3765 - acc: 0.831 - ETA: 5:46 - loss: 0.3585 - acc: 0.850 - ETA: 5:42 - loss: 0.3605 - acc: 0.854 - ETA: 5:37 - loss: 0.3595 - acc: 0.860 - ETA: 5:32 - loss: 0.3510 - acc: 0.863 - ETA: 5:27 - loss: 0.3515 - acc: 0.865 - ETA: 5:22 - loss: 0.3520 - acc: 0.864 - ETA: 5:18 - loss: 0.3566 - acc: 0.861 - ETA: 5:13 - loss: 0.3534 - acc: 0.863 - ETA: 5:08 - loss: 0.3517 - acc: 0.866 - ETA: 5:03 - loss: 0.3500 - acc: 0.867 - ETA: 4:58 - loss: 0.3512 - acc: 0.864 - ETA: 4:54 - loss: 0.3519 - acc: 0.865 - ETA: 4:49 - loss: 0.3510 - acc: 0.865 - ETA: 4:44 - loss: 0.3528 - acc: 0.861 - ETA: 4:39 - loss: 0.3538 - acc: 0.860 - ETA: 4:34 - loss: 0.3508 - acc: 0.864 - ETA: 4:30 - loss: 0.3490 - acc: 0.866 - ETA: 4:25 - loss: 0.3481 - acc: 0.867 - ETA: 4:20 - loss: 0.3471 - acc: 0.868 - ETA: 4:15 - loss: 0.3499 - acc: 0.865 - ETA: 4:10 - loss: 0.3475 - acc: 0.867 - ETA: 4:05 - loss: 0.3466 - acc: 0.868 - ETA: 4:01 - loss: 0.3450 - acc: 0.870 - ETA: 3:56 - loss: 0.3436 - acc: 0.871 - ETA: 3:51 - loss: 0.3438 - acc: 0.871 - ETA: 3:40 - loss: 0.3474 - acc: 0.867 - ETA: 3:35 - loss: 0.3472 - acc: 0.868 - ETA: 3:31 - loss: 0.3469 - acc: 0.869 - ETA: 3:26 - loss: 0.3486 - acc: 0.866 - ETA: 3:22 - loss: 0.3465 - acc: 0.868 - ETA: 3:17 - loss: 0.3458 - acc: 0.868 - ETA: 3:13 - loss: 0.3455 - acc: 0.869 - ETA: 3:08 - loss: 0.3442 - acc: 0.870 - ETA: 3:03 - loss: 0.3457 - acc: 0.869 - ETA: 2:59 - loss: 0.3454 - acc: 0.869 - ETA: 2:54 - loss: 0.3447 - acc: 0.869 - ETA: 2:50 - loss: 0.3452 - acc: 0.870 - ETA: 2:45 - loss: 0.3441 - acc: 0.871 - ETA: 2:40 - loss: 0.3442 - acc: 0.871 - ETA: 2:36 - loss: 0.3450 - acc: 0.871 - ETA: 2:31 - loss: 0.3462 - acc: 0.869 - ETA: 2:26 - loss: 0.3468 - acc: 0.869 - ETA: 2:22 - loss: 0.3474 - acc: 0.868 - ETA: 2:17 - loss: 0.3478 - acc: 0.867 - ETA: 2:12 - loss: 0.3483 - acc: 0.867 - ETA: 2:08 - loss: 0.3478 - acc: 0.867 - ETA: 2:03 - loss: 0.3490 - acc: 0.866 - ETA: 1:58 - loss: 0.3485 - acc: 0.866 - ETA: 1:53 - loss: 0.3492 - acc: 0.866 - ETA: 1:49 - loss: 0.3515 - acc: 0.863 - ETA: 1:44 - loss: 0.3515 - acc: 0.863 - ETA: 1:39 - loss: 0.3517 - acc: 0.863 - ETA: 1:35 - loss: 0.3519 - acc: 0.863 - ETA: 1:30 - loss: 0.3518 - acc: 0.862 - ETA: 1:25 - loss: 0.3525 - acc: 0.861 - ETA: 1:20 - loss: 0.3536 - acc: 0.860 - ETA: 1:16 - loss: 0.3531 - acc: 0.860 - ETA: 1:11 - loss: 0.3544 - acc: 0.860 - ETA: 1:06 - loss: 0.3554 - acc: 0.859 - ETA: 1:01 - loss: 0.3559 - acc: 0.859 - ETA: 57s - loss: 0.3559 - acc: 0.859 - ETA: 52s - loss: 0.3558 - acc: 0.85 - ETA: 47s - loss: 0.3555 - acc: 0.86 - ETA: 42s - loss: 0.3553 - acc: 0.86 - ETA: 38s - loss: 0.3548 - acc: 0.86 - ETA: 33s - loss: 0.3541 - acc: 0.86 - ETA: 28s - loss: 0.3543 - acc: 0.86 - ETA: 23s - loss: 0.3541 - acc: 0.86 - ETA: 19s - loss: 0.3544 - acc: 0.86 - ETA: 14s - loss: 0.3539 - acc: 0.86 - ETA: 9s - loss: 0.3549 - acc: 0.8606 - ETA: 4s - loss: 0.3546 - acc: 0.8603\n",
      "Epoch 00068: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3547 - acc: 0.8598 - val_loss: 0.6875 - val_acc: 0.6566\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3981 - acc: 0.843 - ETA: 6:06 - loss: 0.4068 - acc: 0.828 - ETA: 6:01 - loss: 0.3579 - acc: 0.868 - ETA: 5:56 - loss: 0.3426 - acc: 0.877 - ETA: 5:51 - loss: 0.3319 - acc: 0.879 - ETA: 5:47 - loss: 0.3390 - acc: 0.871 - ETA: 5:42 - loss: 0.3400 - acc: 0.864 - ETA: 5:37 - loss: 0.3448 - acc: 0.862 - ETA: 5:32 - loss: 0.3416 - acc: 0.866 - ETA: 5:27 - loss: 0.3432 - acc: 0.860 - ETA: 5:23 - loss: 0.3433 - acc: 0.862 - ETA: 5:18 - loss: 0.3384 - acc: 0.867 - ETA: 5:13 - loss: 0.3401 - acc: 0.866 - ETA: 5:08 - loss: 0.3396 - acc: 0.867 - ETA: 5:03 - loss: 0.3381 - acc: 0.869 - ETA: 4:58 - loss: 0.3421 - acc: 0.866 - ETA: 4:54 - loss: 0.3400 - acc: 0.869 - ETA: 4:49 - loss: 0.3450 - acc: 0.866 - ETA: 4:44 - loss: 0.3466 - acc: 0.865 - ETA: 4:39 - loss: 0.3479 - acc: 0.862 - ETA: 4:34 - loss: 0.3514 - acc: 0.858 - ETA: 4:30 - loss: 0.3512 - acc: 0.858 - ETA: 4:25 - loss: 0.3500 - acc: 0.859 - ETA: 4:20 - loss: 0.3494 - acc: 0.859 - ETA: 4:15 - loss: 0.3474 - acc: 0.862 - ETA: 4:10 - loss: 0.3480 - acc: 0.860 - ETA: 4:05 - loss: 0.3493 - acc: 0.859 - ETA: 4:01 - loss: 0.3470 - acc: 0.861 - ETA: 3:56 - loss: 0.3483 - acc: 0.860 - ETA: 3:51 - loss: 0.3462 - acc: 0.862 - ETA: 3:46 - loss: 0.3475 - acc: 0.860 - ETA: 3:41 - loss: 0.3470 - acc: 0.861 - ETA: 3:37 - loss: 0.3462 - acc: 0.861 - ETA: 3:32 - loss: 0.3469 - acc: 0.861 - ETA: 3:27 - loss: 0.3472 - acc: 0.861 - ETA: 3:22 - loss: 0.3499 - acc: 0.859 - ETA: 3:17 - loss: 0.3494 - acc: 0.859 - ETA: 3:12 - loss: 0.3502 - acc: 0.859 - ETA: 3:08 - loss: 0.3488 - acc: 0.860 - ETA: 3:03 - loss: 0.3491 - acc: 0.860 - ETA: 2:58 - loss: 0.3490 - acc: 0.860 - ETA: 2:53 - loss: 0.3494 - acc: 0.859 - ETA: 2:48 - loss: 0.3487 - acc: 0.860 - ETA: 2:43 - loss: 0.3503 - acc: 0.858 - ETA: 2:39 - loss: 0.3502 - acc: 0.859 - ETA: 2:34 - loss: 0.3499 - acc: 0.858 - ETA: 2:26 - loss: 0.3507 - acc: 0.860 - ETA: 2:22 - loss: 0.3516 - acc: 0.858 - ETA: 2:17 - loss: 0.3518 - acc: 0.858 - ETA: 2:12 - loss: 0.3521 - acc: 0.859 - ETA: 2:07 - loss: 0.3526 - acc: 0.859 - ETA: 2:03 - loss: 0.3521 - acc: 0.859 - ETA: 1:58 - loss: 0.3513 - acc: 0.860 - ETA: 1:53 - loss: 0.3509 - acc: 0.861 - ETA: 1:49 - loss: 0.3514 - acc: 0.860 - ETA: 1:44 - loss: 0.3510 - acc: 0.861 - ETA: 1:39 - loss: 0.3508 - acc: 0.861 - ETA: 1:35 - loss: 0.3526 - acc: 0.859 - ETA: 1:30 - loss: 0.3528 - acc: 0.859 - ETA: 1:25 - loss: 0.3532 - acc: 0.858 - ETA: 1:20 - loss: 0.3528 - acc: 0.859 - ETA: 1:16 - loss: 0.3526 - acc: 0.859 - ETA: 1:11 - loss: 0.3533 - acc: 0.858 - ETA: 1:06 - loss: 0.3546 - acc: 0.858 - ETA: 1:01 - loss: 0.3540 - acc: 0.858 - ETA: 57s - loss: 0.3533 - acc: 0.859 - ETA: 52s - loss: 0.3533 - acc: 0.85 - ETA: 47s - loss: 0.3532 - acc: 0.85 - ETA: 42s - loss: 0.3531 - acc: 0.86 - ETA: 38s - loss: 0.3533 - acc: 0.85 - ETA: 33s - loss: 0.3543 - acc: 0.85 - ETA: 28s - loss: 0.3551 - acc: 0.85 - ETA: 23s - loss: 0.3549 - acc: 0.85 - ETA: 19s - loss: 0.3542 - acc: 0.86 - ETA: 14s - loss: 0.3547 - acc: 0.85 - ETA: 9s - loss: 0.3547 - acc: 0.8595 - ETA: 4s - loss: 0.3552 - acc: 0.8589\n",
      "Epoch 00069: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3565 - acc: 0.8587 - val_loss: 0.6692 - val_acc: 0.6648\n",
      "Epoch 70/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3001 - acc: 0.906 - ETA: 6:06 - loss: 0.3382 - acc: 0.875 - ETA: 6:01 - loss: 0.3561 - acc: 0.864 - ETA: 5:56 - loss: 0.3432 - acc: 0.872 - ETA: 5:51 - loss: 0.3287 - acc: 0.881 - ETA: 5:46 - loss: 0.3388 - acc: 0.873 - ETA: 5:42 - loss: 0.3404 - acc: 0.864 - ETA: 5:37 - loss: 0.3416 - acc: 0.871 - ETA: 5:32 - loss: 0.3372 - acc: 0.873 - ETA: 5:27 - loss: 0.3369 - acc: 0.876 - ETA: 5:22 - loss: 0.3330 - acc: 0.878 - ETA: 5:18 - loss: 0.3322 - acc: 0.881 - ETA: 5:13 - loss: 0.3302 - acc: 0.883 - ETA: 5:08 - loss: 0.3353 - acc: 0.877 - ETA: 5:03 - loss: 0.3357 - acc: 0.879 - ETA: 4:58 - loss: 0.3340 - acc: 0.878 - ETA: 4:54 - loss: 0.3349 - acc: 0.877 - ETA: 4:49 - loss: 0.3364 - acc: 0.876 - ETA: 4:44 - loss: 0.3352 - acc: 0.876 - ETA: 4:39 - loss: 0.3352 - acc: 0.876 - ETA: 4:34 - loss: 0.3355 - acc: 0.874 - ETA: 4:29 - loss: 0.3367 - acc: 0.871 - ETA: 4:25 - loss: 0.3362 - acc: 0.872 - ETA: 4:20 - loss: 0.3377 - acc: 0.872 - ETA: 4:15 - loss: 0.3368 - acc: 0.873 - ETA: 4:10 - loss: 0.3354 - acc: 0.874 - ETA: 4:05 - loss: 0.3343 - acc: 0.874 - ETA: 4:01 - loss: 0.3335 - acc: 0.874 - ETA: 3:56 - loss: 0.3342 - acc: 0.874 - ETA: 3:51 - loss: 0.3323 - acc: 0.875 - ETA: 3:46 - loss: 0.3327 - acc: 0.876 - ETA: 3:41 - loss: 0.3333 - acc: 0.874 - ETA: 3:37 - loss: 0.3344 - acc: 0.874 - ETA: 3:32 - loss: 0.3362 - acc: 0.872 - ETA: 3:27 - loss: 0.3373 - acc: 0.870 - ETA: 3:22 - loss: 0.3388 - acc: 0.868 - ETA: 3:17 - loss: 0.3368 - acc: 0.871 - ETA: 3:12 - loss: 0.3367 - acc: 0.871 - ETA: 3:08 - loss: 0.3346 - acc: 0.873 - ETA: 3:03 - loss: 0.3337 - acc: 0.874 - ETA: 2:58 - loss: 0.3342 - acc: 0.874 - ETA: 2:53 - loss: 0.3335 - acc: 0.874 - ETA: 2:48 - loss: 0.3352 - acc: 0.873 - ETA: 2:43 - loss: 0.3372 - acc: 0.870 - ETA: 2:39 - loss: 0.3381 - acc: 0.869 - ETA: 2:34 - loss: 0.3371 - acc: 0.870 - ETA: 2:29 - loss: 0.3365 - acc: 0.870 - ETA: 2:24 - loss: 0.3363 - acc: 0.871 - ETA: 2:19 - loss: 0.3371 - acc: 0.870 - ETA: 2:15 - loss: 0.3372 - acc: 0.871 - ETA: 2:10 - loss: 0.3386 - acc: 0.871 - ETA: 2:05 - loss: 0.3380 - acc: 0.871 - ETA: 2:00 - loss: 0.3393 - acc: 0.870 - ETA: 1:55 - loss: 0.3392 - acc: 0.870 - ETA: 1:50 - loss: 0.3392 - acc: 0.869 - ETA: 1:46 - loss: 0.3392 - acc: 0.870 - ETA: 1:41 - loss: 0.3404 - acc: 0.868 - ETA: 1:35 - loss: 0.3442 - acc: 0.865 - ETA: 1:30 - loss: 0.3453 - acc: 0.864 - ETA: 1:25 - loss: 0.3454 - acc: 0.864 - ETA: 1:20 - loss: 0.3446 - acc: 0.865 - ETA: 1:16 - loss: 0.3447 - acc: 0.865 - ETA: 1:11 - loss: 0.3450 - acc: 0.864 - ETA: 1:06 - loss: 0.3443 - acc: 0.865 - ETA: 1:01 - loss: 0.3443 - acc: 0.864 - ETA: 57s - loss: 0.3448 - acc: 0.864 - ETA: 52s - loss: 0.3439 - acc: 0.86 - ETA: 47s - loss: 0.3444 - acc: 0.86 - ETA: 42s - loss: 0.3443 - acc: 0.86 - ETA: 38s - loss: 0.3436 - acc: 0.86 - ETA: 33s - loss: 0.3438 - acc: 0.86 - ETA: 28s - loss: 0.3432 - acc: 0.86 - ETA: 23s - loss: 0.3448 - acc: 0.86 - ETA: 19s - loss: 0.3445 - acc: 0.86 - ETA: 14s - loss: 0.3450 - acc: 0.86 - ETA: 9s - loss: 0.3458 - acc: 0.8647 - ETA: 4s - loss: 0.3454 - acc: 0.8650\n",
      "Epoch 00070: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3452 - acc: 0.8651 - val_loss: 0.6805 - val_acc: 0.6730\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4194 - acc: 0.802 - ETA: 6:06 - loss: 0.3924 - acc: 0.843 - ETA: 6:01 - loss: 0.3817 - acc: 0.850 - ETA: 5:57 - loss: 0.3588 - acc: 0.864 - ETA: 4:51 - loss: 0.3969 - acc: 0.841 - ETA: 4:57 - loss: 0.3864 - acc: 0.842 - ETA: 5:00 - loss: 0.3759 - acc: 0.848 - ETA: 5:01 - loss: 0.3722 - acc: 0.851 - ETA: 5:01 - loss: 0.3618 - acc: 0.857 - ETA: 4:59 - loss: 0.3707 - acc: 0.849 - ETA: 4:57 - loss: 0.3727 - acc: 0.849 - ETA: 4:55 - loss: 0.3803 - acc: 0.841 - ETA: 4:52 - loss: 0.3776 - acc: 0.844 - ETA: 4:49 - loss: 0.3755 - acc: 0.846 - ETA: 4:46 - loss: 0.3731 - acc: 0.849 - ETA: 4:43 - loss: 0.3686 - acc: 0.852 - ETA: 4:39 - loss: 0.3662 - acc: 0.852 - ETA: 4:35 - loss: 0.3618 - acc: 0.856 - ETA: 4:31 - loss: 0.3593 - acc: 0.858 - ETA: 4:27 - loss: 0.3561 - acc: 0.860 - ETA: 4:23 - loss: 0.3597 - acc: 0.857 - ETA: 4:19 - loss: 0.3599 - acc: 0.857 - ETA: 4:15 - loss: 0.3610 - acc: 0.856 - ETA: 4:11 - loss: 0.3623 - acc: 0.854 - ETA: 4:06 - loss: 0.3624 - acc: 0.853 - ETA: 4:02 - loss: 0.3603 - acc: 0.854 - ETA: 3:58 - loss: 0.3621 - acc: 0.850 - ETA: 3:53 - loss: 0.3607 - acc: 0.851 - ETA: 3:49 - loss: 0.3633 - acc: 0.848 - ETA: 3:44 - loss: 0.3609 - acc: 0.849 - ETA: 3:40 - loss: 0.3611 - acc: 0.849 - ETA: 3:35 - loss: 0.3604 - acc: 0.850 - ETA: 3:31 - loss: 0.3606 - acc: 0.850 - ETA: 3:26 - loss: 0.3596 - acc: 0.850 - ETA: 3:22 - loss: 0.3584 - acc: 0.851 - ETA: 3:17 - loss: 0.3598 - acc: 0.850 - ETA: 3:13 - loss: 0.3589 - acc: 0.851 - ETA: 3:08 - loss: 0.3580 - acc: 0.852 - ETA: 3:03 - loss: 0.3563 - acc: 0.853 - ETA: 2:59 - loss: 0.3570 - acc: 0.853 - ETA: 2:54 - loss: 0.3560 - acc: 0.854 - ETA: 2:50 - loss: 0.3562 - acc: 0.855 - ETA: 2:45 - loss: 0.3572 - acc: 0.854 - ETA: 2:40 - loss: 0.3561 - acc: 0.854 - ETA: 2:36 - loss: 0.3566 - acc: 0.853 - ETA: 2:31 - loss: 0.3552 - acc: 0.855 - ETA: 2:26 - loss: 0.3563 - acc: 0.853 - ETA: 2:22 - loss: 0.3559 - acc: 0.854 - ETA: 2:17 - loss: 0.3556 - acc: 0.854 - ETA: 2:12 - loss: 0.3549 - acc: 0.855 - ETA: 2:08 - loss: 0.3531 - acc: 0.856 - ETA: 2:03 - loss: 0.3530 - acc: 0.856 - ETA: 1:58 - loss: 0.3525 - acc: 0.857 - ETA: 1:53 - loss: 0.3515 - acc: 0.857 - ETA: 1:49 - loss: 0.3515 - acc: 0.858 - ETA: 1:44 - loss: 0.3524 - acc: 0.857 - ETA: 1:39 - loss: 0.3519 - acc: 0.857 - ETA: 1:35 - loss: 0.3520 - acc: 0.857 - ETA: 1:30 - loss: 0.3518 - acc: 0.857 - ETA: 1:25 - loss: 0.3529 - acc: 0.856 - ETA: 1:20 - loss: 0.3529 - acc: 0.856 - ETA: 1:16 - loss: 0.3538 - acc: 0.855 - ETA: 1:11 - loss: 0.3540 - acc: 0.855 - ETA: 1:06 - loss: 0.3546 - acc: 0.855 - ETA: 1:01 - loss: 0.3535 - acc: 0.855 - ETA: 57s - loss: 0.3535 - acc: 0.855 - ETA: 52s - loss: 0.3539 - acc: 0.85 - ETA: 47s - loss: 0.3541 - acc: 0.85 - ETA: 42s - loss: 0.3545 - acc: 0.85 - ETA: 38s - loss: 0.3542 - acc: 0.85 - ETA: 33s - loss: 0.3527 - acc: 0.85 - ETA: 28s - loss: 0.3530 - acc: 0.85 - ETA: 23s - loss: 0.3525 - acc: 0.85 - ETA: 18s - loss: 0.3520 - acc: 0.85 - ETA: 14s - loss: 0.3507 - acc: 0.85 - ETA: 9s - loss: 0.3509 - acc: 0.8579 - ETA: 4s - loss: 0.3504 - acc: 0.8585\n",
      "Epoch 00071: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.3498 - acc: 0.8588 - val_loss: 0.6678 - val_acc: 0.6648\n",
      "Epoch 72/100\n",
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.3010 - acc: 0.875 - ETA: 6:05 - loss: 0.3017 - acc: 0.885 - ETA: 6:00 - loss: 0.3134 - acc: 0.875 - ETA: 5:56 - loss: 0.3100 - acc: 0.875 - ETA: 5:51 - loss: 0.3236 - acc: 0.870 - ETA: 5:46 - loss: 0.3236 - acc: 0.878 - ETA: 5:42 - loss: 0.3304 - acc: 0.876 - ETA: 5:37 - loss: 0.3378 - acc: 0.873 - ETA: 5:32 - loss: 0.3424 - acc: 0.871 - ETA: 5:27 - loss: 0.3483 - acc: 0.866 - ETA: 5:22 - loss: 0.3530 - acc: 0.858 - ETA: 5:18 - loss: 0.3517 - acc: 0.861 - ETA: 5:13 - loss: 0.3516 - acc: 0.859 - ETA: 5:08 - loss: 0.3477 - acc: 0.862 - ETA: 5:03 - loss: 0.3441 - acc: 0.867 - ETA: 4:58 - loss: 0.3516 - acc: 0.858 - ETA: 4:54 - loss: 0.3527 - acc: 0.859 - ETA: 4:49 - loss: 0.3534 - acc: 0.858 - ETA: 4:44 - loss: 0.3517 - acc: 0.860 - ETA: 4:39 - loss: 0.3563 - acc: 0.858 - ETA: 4:34 - loss: 0.3555 - acc: 0.859 - ETA: 4:29 - loss: 0.3532 - acc: 0.861 - ETA: 4:25 - loss: 0.3541 - acc: 0.861 - ETA: 4:20 - loss: 0.3505 - acc: 0.863 - ETA: 4:15 - loss: 0.3508 - acc: 0.863 - ETA: 4:10 - loss: 0.3504 - acc: 0.862 - ETA: 4:05 - loss: 0.3508 - acc: 0.862 - ETA: 4:01 - loss: 0.3506 - acc: 0.861 - ETA: 3:56 - loss: 0.3483 - acc: 0.862 - ETA: 3:51 - loss: 0.3495 - acc: 0.861 - ETA: 3:46 - loss: 0.3487 - acc: 0.861 - ETA: 3:41 - loss: 0.3474 - acc: 0.863 - ETA: 3:36 - loss: 0.3480 - acc: 0.862 - ETA: 3:32 - loss: 0.3473 - acc: 0.863 - ETA: 3:27 - loss: 0.3469 - acc: 0.862 - ETA: 3:22 - loss: 0.3469 - acc: 0.862 - ETA: 3:17 - loss: 0.3461 - acc: 0.864 - ETA: 3:12 - loss: 0.3444 - acc: 0.865 - ETA: 3:08 - loss: 0.3442 - acc: 0.865 - ETA: 3:03 - loss: 0.3459 - acc: 0.863 - ETA: 2:58 - loss: 0.3449 - acc: 0.864 - ETA: 2:53 - loss: 0.3458 - acc: 0.863 - ETA: 2:48 - loss: 0.3444 - acc: 0.864 - ETA: 2:43 - loss: 0.3453 - acc: 0.864 - ETA: 2:39 - loss: 0.3465 - acc: 0.863 - ETA: 2:34 - loss: 0.3481 - acc: 0.861 - ETA: 2:29 - loss: 0.3478 - acc: 0.862 - ETA: 2:24 - loss: 0.3477 - acc: 0.863 - ETA: 2:19 - loss: 0.3475 - acc: 0.863 - ETA: 2:15 - loss: 0.3473 - acc: 0.863 - ETA: 2:10 - loss: 0.3472 - acc: 0.862 - ETA: 2:05 - loss: 0.3480 - acc: 0.862 - ETA: 2:00 - loss: 0.3476 - acc: 0.862 - ETA: 1:55 - loss: 0.3469 - acc: 0.862 - ETA: 1:50 - loss: 0.3469 - acc: 0.862 - ETA: 1:46 - loss: 0.3463 - acc: 0.862 - ETA: 1:41 - loss: 0.3469 - acc: 0.861 - ETA: 1:36 - loss: 0.3466 - acc: 0.861 - ETA: 1:31 - loss: 0.3465 - acc: 0.862 - ETA: 1:26 - loss: 0.3452 - acc: 0.863 - ETA: 1:21 - loss: 0.3454 - acc: 0.863 - ETA: 1:17 - loss: 0.3450 - acc: 0.863 - ETA: 1:12 - loss: 0.3449 - acc: 0.863 - ETA: 1:07 - loss: 0.3447 - acc: 0.863 - ETA: 1:02 - loss: 0.3447 - acc: 0.863 - ETA: 57s - loss: 0.3449 - acc: 0.864 - ETA: 53s - loss: 0.3450 - acc: 0.86 - ETA: 48s - loss: 0.3439 - acc: 0.86 - ETA: 43s - loss: 0.3440 - acc: 0.86 - ETA: 38s - loss: 0.3434 - acc: 0.86 - ETA: 33s - loss: 0.3428 - acc: 0.86 - ETA: 28s - loss: 0.3428 - acc: 0.86 - ETA: 24s - loss: 0.3434 - acc: 0.86 - ETA: 19s - loss: 0.3447 - acc: 0.86 - ETA: 14s - loss: 0.3447 - acc: 0.86 - ETA: 9s - loss: 0.3462 - acc: 0.8627 - ETA: 4s - loss: 0.3457 - acc: 0.8628\n",
      "Epoch 00072: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.3458 - acc: 0.8628 - val_loss: 0.6754 - val_acc: 0.6660\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3611 - acc: 0.864 - ETA: 6:06 - loss: 0.3376 - acc: 0.880 - ETA: 6:01 - loss: 0.3305 - acc: 0.864 - ETA: 5:56 - loss: 0.3418 - acc: 0.862 - ETA: 5:51 - loss: 0.3282 - acc: 0.872 - ETA: 5:47 - loss: 0.3397 - acc: 0.861 - ETA: 5:42 - loss: 0.3529 - acc: 0.849 - ETA: 5:37 - loss: 0.3513 - acc: 0.852 - ETA: 5:32 - loss: 0.3524 - acc: 0.851 - ETA: 5:27 - loss: 0.3498 - acc: 0.853 - ETA: 5:22 - loss: 0.3491 - acc: 0.854 - ETA: 5:18 - loss: 0.3462 - acc: 0.856 - ETA: 5:13 - loss: 0.3430 - acc: 0.859 - ETA: 5:08 - loss: 0.3420 - acc: 0.861 - ETA: 5:03 - loss: 0.3391 - acc: 0.865 - ETA: 4:58 - loss: 0.3344 - acc: 0.871 - ETA: 4:54 - loss: 0.3330 - acc: 0.874 - ETA: 4:49 - loss: 0.3312 - acc: 0.875 - ETA: 4:44 - loss: 0.3304 - acc: 0.877 - ETA: 4:39 - loss: 0.3307 - acc: 0.876 - ETA: 4:34 - loss: 0.3319 - acc: 0.877 - ETA: 4:30 - loss: 0.3338 - acc: 0.874 - ETA: 4:25 - loss: 0.3329 - acc: 0.875 - ETA: 4:20 - loss: 0.3332 - acc: 0.875 - ETA: 4:15 - loss: 0.3371 - acc: 0.872 - ETA: 4:10 - loss: 0.3365 - acc: 0.873 - ETA: 4:05 - loss: 0.3367 - acc: 0.871 - ETA: 4:01 - loss: 0.3389 - acc: 0.869 - ETA: 3:56 - loss: 0.3391 - acc: 0.867 - ETA: 3:51 - loss: 0.3396 - acc: 0.867 - ETA: 3:46 - loss: 0.3420 - acc: 0.866 - ETA: 3:41 - loss: 0.3374 - acc: 0.870 - ETA: 3:36 - loss: 0.3356 - acc: 0.870 - ETA: 3:32 - loss: 0.3355 - acc: 0.871 - ETA: 3:27 - loss: 0.3385 - acc: 0.868 - ETA: 3:22 - loss: 0.3393 - acc: 0.867 - ETA: 3:17 - loss: 0.3400 - acc: 0.866 - ETA: 3:12 - loss: 0.3417 - acc: 0.864 - ETA: 3:08 - loss: 0.3411 - acc: 0.865 - ETA: 3:03 - loss: 0.3415 - acc: 0.863 - ETA: 2:58 - loss: 0.3421 - acc: 0.862 - ETA: 2:53 - loss: 0.3422 - acc: 0.862 - ETA: 2:48 - loss: 0.3409 - acc: 0.863 - ETA: 2:43 - loss: 0.3409 - acc: 0.863 - ETA: 2:39 - loss: 0.3413 - acc: 0.862 - ETA: 2:34 - loss: 0.3402 - acc: 0.863 - ETA: 2:29 - loss: 0.3396 - acc: 0.863 - ETA: 2:22 - loss: 0.3383 - acc: 0.864 - ETA: 2:17 - loss: 0.3365 - acc: 0.866 - ETA: 2:12 - loss: 0.3361 - acc: 0.866 - ETA: 2:08 - loss: 0.3345 - acc: 0.867 - ETA: 2:03 - loss: 0.3333 - acc: 0.868 - ETA: 1:58 - loss: 0.3333 - acc: 0.868 - ETA: 1:53 - loss: 0.3328 - acc: 0.868 - ETA: 1:49 - loss: 0.3360 - acc: 0.866 - ETA: 1:44 - loss: 0.3367 - acc: 0.866 - ETA: 1:39 - loss: 0.3363 - acc: 0.866 - ETA: 1:35 - loss: 0.3362 - acc: 0.866 - ETA: 1:30 - loss: 0.3368 - acc: 0.866 - ETA: 1:25 - loss: 0.3369 - acc: 0.865 - ETA: 1:20 - loss: 0.3372 - acc: 0.864 - ETA: 1:16 - loss: 0.3385 - acc: 0.863 - ETA: 1:11 - loss: 0.3393 - acc: 0.862 - ETA: 1:06 - loss: 0.3391 - acc: 0.863 - ETA: 1:01 - loss: 0.3383 - acc: 0.864 - ETA: 57s - loss: 0.3370 - acc: 0.865 - ETA: 52s - loss: 0.3374 - acc: 0.86 - ETA: 47s - loss: 0.3371 - acc: 0.86 - ETA: 42s - loss: 0.3377 - acc: 0.86 - ETA: 38s - loss: 0.3369 - acc: 0.86 - ETA: 33s - loss: 0.3378 - acc: 0.86 - ETA: 28s - loss: 0.3370 - acc: 0.86 - ETA: 23s - loss: 0.3371 - acc: 0.86 - ETA: 19s - loss: 0.3378 - acc: 0.86 - ETA: 14s - loss: 0.3392 - acc: 0.86 - ETA: 9s - loss: 0.3393 - acc: 0.8650 - ETA: 4s - loss: 0.3391 - acc: 0.8647\n",
      "Epoch 00073: val_loss did not improve\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.3398 - acc: 0.8650 - val_loss: 0.6785 - val_acc: 0.6624\n",
      "Epoch 74/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3465 - acc: 0.895 - ETA: 6:05 - loss: 0.3279 - acc: 0.880 - ETA: 6:01 - loss: 0.3374 - acc: 0.864 - ETA: 5:56 - loss: 0.3366 - acc: 0.859 - ETA: 5:51 - loss: 0.3213 - acc: 0.872 - ETA: 5:46 - loss: 0.3198 - acc: 0.876 - ETA: 5:42 - loss: 0.3321 - acc: 0.863 - ETA: 5:37 - loss: 0.3338 - acc: 0.863 - ETA: 5:32 - loss: 0.3311 - acc: 0.863 - ETA: 5:27 - loss: 0.3282 - acc: 0.864 - ETA: 5:22 - loss: 0.3370 - acc: 0.856 - ETA: 5:18 - loss: 0.3366 - acc: 0.855 - ETA: 5:13 - loss: 0.3289 - acc: 0.861 - ETA: 5:08 - loss: 0.3233 - acc: 0.868 - ETA: 5:03 - loss: 0.3203 - acc: 0.871 - ETA: 4:58 - loss: 0.3209 - acc: 0.871 - ETA: 4:54 - loss: 0.3251 - acc: 0.871 - ETA: 4:49 - loss: 0.3246 - acc: 0.873 - ETA: 4:44 - loss: 0.3236 - acc: 0.875 - ETA: 4:39 - loss: 0.3246 - acc: 0.873 - ETA: 4:34 - loss: 0.3240 - acc: 0.874 - ETA: 4:30 - loss: 0.3225 - acc: 0.875 - ETA: 4:25 - loss: 0.3240 - acc: 0.874 - ETA: 4:20 - loss: 0.3237 - acc: 0.874 - ETA: 4:15 - loss: 0.3259 - acc: 0.873 - ETA: 4:10 - loss: 0.3262 - acc: 0.874 - ETA: 4:05 - loss: 0.3273 - acc: 0.874 - ETA: 4:01 - loss: 0.3263 - acc: 0.875 - ETA: 3:56 - loss: 0.3269 - acc: 0.874 - ETA: 3:51 - loss: 0.3244 - acc: 0.877 - ETA: 3:46 - loss: 0.3243 - acc: 0.877 - ETA: 3:41 - loss: 0.3232 - acc: 0.877 - ETA: 3:36 - loss: 0.3238 - acc: 0.876 - ETA: 3:32 - loss: 0.3258 - acc: 0.875 - ETA: 3:27 - loss: 0.3261 - acc: 0.876 - ETA: 3:22 - loss: 0.3272 - acc: 0.876 - ETA: 3:17 - loss: 0.3280 - acc: 0.875 - ETA: 3:12 - loss: 0.3308 - acc: 0.872 - ETA: 3:08 - loss: 0.3305 - acc: 0.872 - ETA: 3:03 - loss: 0.3355 - acc: 0.868 - ETA: 2:58 - loss: 0.3346 - acc: 0.868 - ETA: 2:53 - loss: 0.3345 - acc: 0.868 - ETA: 2:48 - loss: 0.3338 - acc: 0.868 - ETA: 2:43 - loss: 0.3334 - acc: 0.868 - ETA: 2:39 - loss: 0.3328 - acc: 0.869 - ETA: 2:34 - loss: 0.3337 - acc: 0.869 - ETA: 2:29 - loss: 0.3346 - acc: 0.868 - ETA: 2:24 - loss: 0.3341 - acc: 0.869 - ETA: 2:19 - loss: 0.3327 - acc: 0.869 - ETA: 2:15 - loss: 0.3335 - acc: 0.868 - ETA: 2:10 - loss: 0.3343 - acc: 0.868 - ETA: 2:05 - loss: 0.3350 - acc: 0.868 - ETA: 2:00 - loss: 0.3341 - acc: 0.869 - ETA: 1:55 - loss: 0.3348 - acc: 0.868 - ETA: 1:50 - loss: 0.3350 - acc: 0.868 - ETA: 1:46 - loss: 0.3349 - acc: 0.867 - ETA: 1:41 - loss: 0.3341 - acc: 0.868 - ETA: 1:36 - loss: 0.3339 - acc: 0.867 - ETA: 1:31 - loss: 0.3334 - acc: 0.868 - ETA: 1:26 - loss: 0.3340 - acc: 0.867 - ETA: 1:21 - loss: 0.3337 - acc: 0.868 - ETA: 1:17 - loss: 0.3326 - acc: 0.869 - ETA: 1:11 - loss: 0.3316 - acc: 0.870 - ETA: 1:06 - loss: 0.3309 - acc: 0.870 - ETA: 1:01 - loss: 0.3315 - acc: 0.869 - ETA: 57s - loss: 0.3334 - acc: 0.868 - ETA: 52s - loss: 0.3341 - acc: 0.86 - ETA: 47s - loss: 0.3343 - acc: 0.86 - ETA: 42s - loss: 0.3338 - acc: 0.86 - ETA: 38s - loss: 0.3331 - acc: 0.86 - ETA: 33s - loss: 0.3337 - acc: 0.86 - ETA: 28s - loss: 0.3337 - acc: 0.86 - ETA: 23s - loss: 0.3339 - acc: 0.86 - ETA: 19s - loss: 0.3346 - acc: 0.86 - ETA: 14s - loss: 0.3342 - acc: 0.86 - ETA: 9s - loss: 0.3334 - acc: 0.8677 - ETA: 4s - loss: 0.3332 - acc: 0.8680\n",
      "Epoch 00074: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3333 - acc: 0.8678 - val_loss: 0.6629 - val_acc: 0.6673\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3791 - acc: 0.812 - ETA: 6:05 - loss: 0.3612 - acc: 0.828 - ETA: 6:01 - loss: 0.3500 - acc: 0.850 - ETA: 5:56 - loss: 0.3366 - acc: 0.872 - ETA: 5:51 - loss: 0.3414 - acc: 0.875 - ETA: 5:47 - loss: 0.3335 - acc: 0.878 - ETA: 5:42 - loss: 0.3276 - acc: 0.878 - ETA: 5:37 - loss: 0.3300 - acc: 0.877 - ETA: 5:32 - loss: 0.3290 - acc: 0.878 - ETA: 5:27 - loss: 0.3286 - acc: 0.876 - ETA: 5:23 - loss: 0.3277 - acc: 0.875 - ETA: 5:18 - loss: 0.3373 - acc: 0.870 - ETA: 5:13 - loss: 0.3373 - acc: 0.871 - ETA: 5:08 - loss: 0.3369 - acc: 0.874 - ETA: 5:03 - loss: 0.3343 - acc: 0.877 - ETA: 4:59 - loss: 0.3283 - acc: 0.882 - ETA: 4:54 - loss: 0.3309 - acc: 0.881 - ETA: 4:49 - loss: 0.3288 - acc: 0.881 - ETA: 4:44 - loss: 0.3305 - acc: 0.881 - ETA: 4:39 - loss: 0.3300 - acc: 0.880 - ETA: 4:34 - loss: 0.3300 - acc: 0.881 - ETA: 4:30 - loss: 0.3344 - acc: 0.877 - ETA: 4:25 - loss: 0.3359 - acc: 0.875 - ETA: 4:20 - loss: 0.3355 - acc: 0.875 - ETA: 4:15 - loss: 0.3366 - acc: 0.873 - ETA: 4:10 - loss: 0.3360 - acc: 0.873 - ETA: 4:05 - loss: 0.3346 - acc: 0.873 - ETA: 4:01 - loss: 0.3316 - acc: 0.876 - ETA: 3:56 - loss: 0.3325 - acc: 0.875 - ETA: 3:51 - loss: 0.3320 - acc: 0.875 - ETA: 3:46 - loss: 0.3319 - acc: 0.875 - ETA: 3:35 - loss: 0.3413 - acc: 0.868 - ETA: 3:31 - loss: 0.3385 - acc: 0.870 - ETA: 3:26 - loss: 0.3387 - acc: 0.870 - ETA: 3:22 - loss: 0.3401 - acc: 0.869 - ETA: 3:17 - loss: 0.3401 - acc: 0.869 - ETA: 3:13 - loss: 0.3410 - acc: 0.868 - ETA: 3:08 - loss: 0.3404 - acc: 0.869 - ETA: 3:03 - loss: 0.3400 - acc: 0.869 - ETA: 2:59 - loss: 0.3386 - acc: 0.870 - ETA: 2:54 - loss: 0.3396 - acc: 0.869 - ETA: 2:50 - loss: 0.3400 - acc: 0.868 - ETA: 2:45 - loss: 0.3411 - acc: 0.867 - ETA: 2:40 - loss: 0.3423 - acc: 0.866 - ETA: 2:36 - loss: 0.3433 - acc: 0.866 - ETA: 2:31 - loss: 0.3444 - acc: 0.865 - ETA: 2:26 - loss: 0.3433 - acc: 0.866 - ETA: 2:22 - loss: 0.3431 - acc: 0.866 - ETA: 2:17 - loss: 0.3432 - acc: 0.865 - ETA: 2:12 - loss: 0.3424 - acc: 0.866 - ETA: 2:08 - loss: 0.3425 - acc: 0.865 - ETA: 2:03 - loss: 0.3422 - acc: 0.865 - ETA: 1:58 - loss: 0.3423 - acc: 0.865 - ETA: 1:53 - loss: 0.3421 - acc: 0.865 - ETA: 1:49 - loss: 0.3426 - acc: 0.865 - ETA: 1:44 - loss: 0.3427 - acc: 0.864 - ETA: 1:39 - loss: 0.3420 - acc: 0.864 - ETA: 1:35 - loss: 0.3417 - acc: 0.865 - ETA: 1:30 - loss: 0.3413 - acc: 0.866 - ETA: 1:25 - loss: 0.3410 - acc: 0.866 - ETA: 1:20 - loss: 0.3409 - acc: 0.867 - ETA: 1:16 - loss: 0.3404 - acc: 0.867 - ETA: 1:11 - loss: 0.3405 - acc: 0.868 - ETA: 1:06 - loss: 0.3395 - acc: 0.868 - ETA: 1:01 - loss: 0.3398 - acc: 0.868 - ETA: 57s - loss: 0.3400 - acc: 0.867 - ETA: 52s - loss: 0.3395 - acc: 0.86 - ETA: 47s - loss: 0.3402 - acc: 0.86 - ETA: 42s - loss: 0.3401 - acc: 0.86 - ETA: 38s - loss: 0.3404 - acc: 0.86 - ETA: 33s - loss: 0.3405 - acc: 0.86 - ETA: 28s - loss: 0.3396 - acc: 0.86 - ETA: 23s - loss: 0.3389 - acc: 0.86 - ETA: 19s - loss: 0.3388 - acc: 0.86 - ETA: 14s - loss: 0.3382 - acc: 0.86 - ETA: 9s - loss: 0.3376 - acc: 0.8688 - ETA: 4s - loss: 0.3371 - acc: 0.8693\n",
      "Epoch 00075: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3376 - acc: 0.8688 - val_loss: 0.7033 - val_acc: 0.6496\n",
      "Epoch 76/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3881 - acc: 0.833 - ETA: 3:28 - loss: 0.3280 - acc: 0.916 - ETA: 4:17 - loss: 0.3211 - acc: 0.913 - ETA: 4:39 - loss: 0.3203 - acc: 0.911 - ETA: 4:51 - loss: 0.3245 - acc: 0.902 - ETA: 4:57 - loss: 0.3184 - acc: 0.904 - ETA: 5:00 - loss: 0.3168 - acc: 0.900 - ETA: 5:01 - loss: 0.3170 - acc: 0.897 - ETA: 5:00 - loss: 0.3129 - acc: 0.901 - ETA: 4:59 - loss: 0.3147 - acc: 0.900 - ETA: 4:57 - loss: 0.3222 - acc: 0.894 - ETA: 4:55 - loss: 0.3167 - acc: 0.899 - ETA: 4:52 - loss: 0.3231 - acc: 0.896 - ETA: 4:49 - loss: 0.3261 - acc: 0.898 - ETA: 4:46 - loss: 0.3305 - acc: 0.891 - ETA: 4:42 - loss: 0.3331 - acc: 0.886 - ETA: 4:39 - loss: 0.3345 - acc: 0.884 - ETA: 4:35 - loss: 0.3350 - acc: 0.884 - ETA: 4:31 - loss: 0.3278 - acc: 0.888 - ETA: 4:27 - loss: 0.3240 - acc: 0.891 - ETA: 4:23 - loss: 0.3254 - acc: 0.889 - ETA: 4:19 - loss: 0.3225 - acc: 0.891 - ETA: 4:15 - loss: 0.3225 - acc: 0.889 - ETA: 4:10 - loss: 0.3218 - acc: 0.890 - ETA: 4:06 - loss: 0.3226 - acc: 0.888 - ETA: 4:02 - loss: 0.3200 - acc: 0.890 - ETA: 3:58 - loss: 0.3197 - acc: 0.888 - ETA: 3:53 - loss: 0.3186 - acc: 0.891 - ETA: 3:49 - loss: 0.3179 - acc: 0.890 - ETA: 3:44 - loss: 0.3167 - acc: 0.891 - ETA: 3:40 - loss: 0.3149 - acc: 0.893 - ETA: 3:35 - loss: 0.3145 - acc: 0.893 - ETA: 3:31 - loss: 0.3144 - acc: 0.891 - ETA: 3:26 - loss: 0.3148 - acc: 0.890 - ETA: 3:22 - loss: 0.3160 - acc: 0.890 - ETA: 3:17 - loss: 0.3174 - acc: 0.888 - ETA: 3:13 - loss: 0.3172 - acc: 0.888 - ETA: 3:08 - loss: 0.3171 - acc: 0.888 - ETA: 3:03 - loss: 0.3183 - acc: 0.887 - ETA: 2:59 - loss: 0.3189 - acc: 0.886 - ETA: 2:54 - loss: 0.3209 - acc: 0.884 - ETA: 2:50 - loss: 0.3192 - acc: 0.885 - ETA: 2:45 - loss: 0.3205 - acc: 0.884 - ETA: 2:40 - loss: 0.3207 - acc: 0.884 - ETA: 2:36 - loss: 0.3208 - acc: 0.883 - ETA: 2:31 - loss: 0.3204 - acc: 0.883 - ETA: 2:26 - loss: 0.3193 - acc: 0.884 - ETA: 2:22 - loss: 0.3218 - acc: 0.883 - ETA: 2:17 - loss: 0.3220 - acc: 0.882 - ETA: 2:12 - loss: 0.3221 - acc: 0.882 - ETA: 2:08 - loss: 0.3236 - acc: 0.880 - ETA: 2:03 - loss: 0.3241 - acc: 0.879 - ETA: 1:58 - loss: 0.3247 - acc: 0.878 - ETA: 1:53 - loss: 0.3246 - acc: 0.878 - ETA: 1:49 - loss: 0.3233 - acc: 0.878 - ETA: 1:44 - loss: 0.3227 - acc: 0.878 - ETA: 1:39 - loss: 0.3247 - acc: 0.878 - ETA: 1:35 - loss: 0.3232 - acc: 0.879 - ETA: 1:30 - loss: 0.3247 - acc: 0.877 - ETA: 1:25 - loss: 0.3253 - acc: 0.877 - ETA: 1:20 - loss: 0.3257 - acc: 0.876 - ETA: 1:16 - loss: 0.3258 - acc: 0.876 - ETA: 1:11 - loss: 0.3253 - acc: 0.876 - ETA: 1:06 - loss: 0.3252 - acc: 0.877 - ETA: 1:01 - loss: 0.3256 - acc: 0.877 - ETA: 57s - loss: 0.3247 - acc: 0.877 - ETA: 52s - loss: 0.3238 - acc: 0.87 - ETA: 47s - loss: 0.3243 - acc: 0.87 - ETA: 42s - loss: 0.3247 - acc: 0.87 - ETA: 38s - loss: 0.3240 - acc: 0.87 - ETA: 33s - loss: 0.3238 - acc: 0.87 - ETA: 28s - loss: 0.3224 - acc: 0.87 - ETA: 23s - loss: 0.3232 - acc: 0.87 - ETA: 19s - loss: 0.3236 - acc: 0.87 - ETA: 14s - loss: 0.3235 - acc: 0.87 - ETA: 9s - loss: 0.3241 - acc: 0.8762 - ETA: 4s - loss: 0.3246 - acc: 0.8758\n",
      "Epoch 00076: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3254 - acc: 0.8752 - val_loss: 0.6484 - val_acc: 0.6717\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.2578 - acc: 0.958 - ETA: 6:05 - loss: 0.2990 - acc: 0.921 - ETA: 6:01 - loss: 0.3135 - acc: 0.902 - ETA: 5:56 - loss: 0.3081 - acc: 0.890 - ETA: 5:51 - loss: 0.3056 - acc: 0.895 - ETA: 5:46 - loss: 0.3027 - acc: 0.895 - ETA: 5:42 - loss: 0.3042 - acc: 0.895 - ETA: 5:37 - loss: 0.3109 - acc: 0.891 - ETA: 5:32 - loss: 0.3102 - acc: 0.888 - ETA: 5:27 - loss: 0.3162 - acc: 0.883 - ETA: 5:22 - loss: 0.3162 - acc: 0.883 - ETA: 5:18 - loss: 0.3183 - acc: 0.882 - ETA: 5:13 - loss: 0.3240 - acc: 0.877 - ETA: 5:08 - loss: 0.3256 - acc: 0.876 - ETA: 5:03 - loss: 0.3301 - acc: 0.876 - ETA: 4:58 - loss: 0.3290 - acc: 0.880 - ETA: 4:54 - loss: 0.3300 - acc: 0.879 - ETA: 4:49 - loss: 0.3300 - acc: 0.880 - ETA: 4:44 - loss: 0.3274 - acc: 0.884 - ETA: 4:39 - loss: 0.3282 - acc: 0.883 - ETA: 4:34 - loss: 0.3281 - acc: 0.882 - ETA: 4:29 - loss: 0.3311 - acc: 0.879 - ETA: 4:25 - loss: 0.3317 - acc: 0.878 - ETA: 4:20 - loss: 0.3323 - acc: 0.878 - ETA: 4:15 - loss: 0.3338 - acc: 0.875 - ETA: 4:10 - loss: 0.3321 - acc: 0.876 - ETA: 4:05 - loss: 0.3323 - acc: 0.875 - ETA: 4:01 - loss: 0.3326 - acc: 0.875 - ETA: 3:56 - loss: 0.3323 - acc: 0.875 - ETA: 3:51 - loss: 0.3307 - acc: 0.876 - ETA: 3:46 - loss: 0.3342 - acc: 0.874 - ETA: 3:41 - loss: 0.3357 - acc: 0.872 - ETA: 3:36 - loss: 0.3349 - acc: 0.873 - ETA: 3:32 - loss: 0.3350 - acc: 0.873 - ETA: 3:27 - loss: 0.3345 - acc: 0.873 - ETA: 3:22 - loss: 0.3333 - acc: 0.874 - ETA: 3:17 - loss: 0.3329 - acc: 0.874 - ETA: 3:12 - loss: 0.3340 - acc: 0.874 - ETA: 3:08 - loss: 0.3329 - acc: 0.874 - ETA: 3:03 - loss: 0.3317 - acc: 0.875 - ETA: 2:58 - loss: 0.3313 - acc: 0.876 - ETA: 2:53 - loss: 0.3315 - acc: 0.876 - ETA: 2:48 - loss: 0.3333 - acc: 0.874 - ETA: 2:43 - loss: 0.3326 - acc: 0.875 - ETA: 2:39 - loss: 0.3333 - acc: 0.874 - ETA: 2:34 - loss: 0.3329 - acc: 0.875 - ETA: 2:29 - loss: 0.3314 - acc: 0.876 - ETA: 2:24 - loss: 0.3309 - acc: 0.877 - ETA: 2:19 - loss: 0.3314 - acc: 0.876 - ETA: 2:14 - loss: 0.3315 - acc: 0.877 - ETA: 2:10 - loss: 0.3320 - acc: 0.876 - ETA: 2:05 - loss: 0.3311 - acc: 0.876 - ETA: 2:00 - loss: 0.3309 - acc: 0.876 - ETA: 1:55 - loss: 0.3310 - acc: 0.876 - ETA: 1:50 - loss: 0.3304 - acc: 0.876 - ETA: 1:46 - loss: 0.3298 - acc: 0.876 - ETA: 1:41 - loss: 0.3298 - acc: 0.875 - ETA: 1:36 - loss: 0.3300 - acc: 0.875 - ETA: 1:31 - loss: 0.3292 - acc: 0.876 - ETA: 1:26 - loss: 0.3307 - acc: 0.875 - ETA: 1:21 - loss: 0.3313 - acc: 0.874 - ETA: 1:17 - loss: 0.3301 - acc: 0.875 - ETA: 1:12 - loss: 0.3309 - acc: 0.875 - ETA: 1:07 - loss: 0.3319 - acc: 0.873 - ETA: 1:02 - loss: 0.3328 - acc: 0.873 - ETA: 57s - loss: 0.3323 - acc: 0.873 - ETA: 53s - loss: 0.3315 - acc: 0.87 - ETA: 48s - loss: 0.3321 - acc: 0.87 - ETA: 43s - loss: 0.3326 - acc: 0.87 - ETA: 38s - loss: 0.3318 - acc: 0.87 - ETA: 33s - loss: 0.3320 - acc: 0.87 - ETA: 28s - loss: 0.3325 - acc: 0.87 - ETA: 23s - loss: 0.3316 - acc: 0.87 - ETA: 19s - loss: 0.3318 - acc: 0.87 - ETA: 14s - loss: 0.3330 - acc: 0.87 - ETA: 9s - loss: 0.3324 - acc: 0.8735 - ETA: 4s - loss: 0.3323 - acc: 0.8730\n",
      "Epoch 00077: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3323 - acc: 0.8734 - val_loss: 0.6769 - val_acc: 0.6742\n",
      "Epoch 78/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3807 - acc: 0.833 - ETA: 6:06 - loss: 0.3639 - acc: 0.854 - ETA: 6:01 - loss: 0.3629 - acc: 0.850 - ETA: 5:56 - loss: 0.3456 - acc: 0.864 - ETA: 5:52 - loss: 0.3351 - acc: 0.870 - ETA: 5:47 - loss: 0.3263 - acc: 0.876 - ETA: 5:42 - loss: 0.3321 - acc: 0.869 - ETA: 5:37 - loss: 0.3290 - acc: 0.869 - ETA: 5:32 - loss: 0.3324 - acc: 0.866 - ETA: 5:27 - loss: 0.3333 - acc: 0.869 - ETA: 5:22 - loss: 0.3316 - acc: 0.871 - ETA: 5:18 - loss: 0.3272 - acc: 0.875 - ETA: 5:13 - loss: 0.3262 - acc: 0.873 - ETA: 5:08 - loss: 0.3317 - acc: 0.865 - ETA: 5:03 - loss: 0.3324 - acc: 0.863 - ETA: 4:58 - loss: 0.3290 - acc: 0.867 - ETA: 4:54 - loss: 0.3271 - acc: 0.871 - ETA: 4:49 - loss: 0.3277 - acc: 0.870 - ETA: 4:44 - loss: 0.3272 - acc: 0.871 - ETA: 4:39 - loss: 0.3286 - acc: 0.869 - ETA: 4:34 - loss: 0.3301 - acc: 0.868 - ETA: 4:30 - loss: 0.3318 - acc: 0.867 - ETA: 4:25 - loss: 0.3307 - acc: 0.867 - ETA: 4:20 - loss: 0.3301 - acc: 0.868 - ETA: 4:15 - loss: 0.3298 - acc: 0.869 - ETA: 4:10 - loss: 0.3276 - acc: 0.870 - ETA: 4:05 - loss: 0.3246 - acc: 0.873 - ETA: 4:01 - loss: 0.3250 - acc: 0.873 - ETA: 3:56 - loss: 0.3246 - acc: 0.873 - ETA: 3:51 - loss: 0.3244 - acc: 0.873 - ETA: 3:46 - loss: 0.3234 - acc: 0.874 - ETA: 3:41 - loss: 0.3247 - acc: 0.872 - ETA: 3:36 - loss: 0.3245 - acc: 0.872 - ETA: 3:32 - loss: 0.3252 - acc: 0.871 - ETA: 3:27 - loss: 0.3294 - acc: 0.869 - ETA: 3:22 - loss: 0.3288 - acc: 0.870 - ETA: 3:17 - loss: 0.3278 - acc: 0.871 - ETA: 3:12 - loss: 0.3266 - acc: 0.872 - ETA: 3:08 - loss: 0.3274 - acc: 0.872 - ETA: 3:03 - loss: 0.3273 - acc: 0.872 - ETA: 2:58 - loss: 0.3290 - acc: 0.871 - ETA: 2:53 - loss: 0.3297 - acc: 0.870 - ETA: 2:48 - loss: 0.3294 - acc: 0.871 - ETA: 2:43 - loss: 0.3286 - acc: 0.872 - ETA: 2:39 - loss: 0.3284 - acc: 0.872 - ETA: 2:31 - loss: 0.3305 - acc: 0.869 - ETA: 2:26 - loss: 0.3302 - acc: 0.870 - ETA: 2:22 - loss: 0.3303 - acc: 0.869 - ETA: 2:17 - loss: 0.3333 - acc: 0.867 - ETA: 2:12 - loss: 0.3336 - acc: 0.868 - ETA: 2:07 - loss: 0.3339 - acc: 0.868 - ETA: 2:03 - loss: 0.3340 - acc: 0.868 - ETA: 1:58 - loss: 0.3338 - acc: 0.868 - ETA: 1:53 - loss: 0.3344 - acc: 0.867 - ETA: 1:49 - loss: 0.3341 - acc: 0.868 - ETA: 1:44 - loss: 0.3340 - acc: 0.868 - ETA: 1:39 - loss: 0.3335 - acc: 0.868 - ETA: 1:35 - loss: 0.3329 - acc: 0.869 - ETA: 1:30 - loss: 0.3321 - acc: 0.869 - ETA: 1:25 - loss: 0.3317 - acc: 0.870 - ETA: 1:20 - loss: 0.3308 - acc: 0.870 - ETA: 1:16 - loss: 0.3309 - acc: 0.870 - ETA: 1:11 - loss: 0.3308 - acc: 0.870 - ETA: 1:06 - loss: 0.3305 - acc: 0.870 - ETA: 1:01 - loss: 0.3310 - acc: 0.870 - ETA: 57s - loss: 0.3312 - acc: 0.870 - ETA: 52s - loss: 0.3321 - acc: 0.86 - ETA: 47s - loss: 0.3323 - acc: 0.86 - ETA: 42s - loss: 0.3323 - acc: 0.86 - ETA: 38s - loss: 0.3312 - acc: 0.86 - ETA: 33s - loss: 0.3306 - acc: 0.86 - ETA: 28s - loss: 0.3306 - acc: 0.87 - ETA: 23s - loss: 0.3304 - acc: 0.87 - ETA: 19s - loss: 0.3311 - acc: 0.87 - ETA: 14s - loss: 0.3307 - acc: 0.87 - ETA: 9s - loss: 0.3316 - acc: 0.8692 - ETA: 4s - loss: 0.3320 - acc: 0.8690\n",
      "Epoch 00078: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3310 - acc: 0.8702 - val_loss: 0.7020 - val_acc: 0.6452\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.4019 - acc: 0.843 - ETA: 6:06 - loss: 0.4157 - acc: 0.807 - ETA: 6:01 - loss: 0.3852 - acc: 0.840 - ETA: 5:56 - loss: 0.3790 - acc: 0.838 - ETA: 5:51 - loss: 0.3711 - acc: 0.839 - ETA: 5:46 - loss: 0.3553 - acc: 0.852 - ETA: 5:42 - loss: 0.3597 - acc: 0.848 - ETA: 5:37 - loss: 0.3568 - acc: 0.855 - ETA: 5:32 - loss: 0.3526 - acc: 0.860 - ETA: 5:27 - loss: 0.3487 - acc: 0.861 - ETA: 5:23 - loss: 0.3407 - acc: 0.866 - ETA: 5:18 - loss: 0.3410 - acc: 0.865 - ETA: 5:13 - loss: 0.3415 - acc: 0.863 - ETA: 5:08 - loss: 0.3440 - acc: 0.859 - ETA: 5:03 - loss: 0.3416 - acc: 0.859 - ETA: 4:59 - loss: 0.3394 - acc: 0.862 - ETA: 4:54 - loss: 0.3411 - acc: 0.860 - ETA: 4:49 - loss: 0.3434 - acc: 0.857 - ETA: 4:44 - loss: 0.3439 - acc: 0.857 - ETA: 4:39 - loss: 0.3437 - acc: 0.858 - ETA: 4:34 - loss: 0.3417 - acc: 0.860 - ETA: 4:30 - loss: 0.3395 - acc: 0.860 - ETA: 4:25 - loss: 0.3394 - acc: 0.861 - ETA: 4:20 - loss: 0.3401 - acc: 0.860 - ETA: 4:15 - loss: 0.3358 - acc: 0.864 - ETA: 4:10 - loss: 0.3333 - acc: 0.866 - ETA: 4:05 - loss: 0.3331 - acc: 0.866 - ETA: 4:01 - loss: 0.3350 - acc: 0.864 - ETA: 3:56 - loss: 0.3343 - acc: 0.864 - ETA: 3:51 - loss: 0.3328 - acc: 0.866 - ETA: 3:46 - loss: 0.3296 - acc: 0.869 - ETA: 3:41 - loss: 0.3296 - acc: 0.869 - ETA: 3:37 - loss: 0.3295 - acc: 0.869 - ETA: 3:32 - loss: 0.3286 - acc: 0.868 - ETA: 3:27 - loss: 0.3288 - acc: 0.869 - ETA: 3:22 - loss: 0.3294 - acc: 0.870 - ETA: 3:17 - loss: 0.3291 - acc: 0.870 - ETA: 3:12 - loss: 0.3287 - acc: 0.870 - ETA: 3:08 - loss: 0.3298 - acc: 0.869 - ETA: 3:03 - loss: 0.3292 - acc: 0.870 - ETA: 2:58 - loss: 0.3274 - acc: 0.871 - ETA: 2:53 - loss: 0.3267 - acc: 0.872 - ETA: 2:48 - loss: 0.3264 - acc: 0.872 - ETA: 2:43 - loss: 0.3267 - acc: 0.872 - ETA: 2:39 - loss: 0.3266 - acc: 0.873 - ETA: 2:34 - loss: 0.3259 - acc: 0.873 - ETA: 2:29 - loss: 0.3262 - acc: 0.873 - ETA: 2:24 - loss: 0.3258 - acc: 0.874 - ETA: 2:19 - loss: 0.3252 - acc: 0.874 - ETA: 2:15 - loss: 0.3253 - acc: 0.875 - ETA: 2:10 - loss: 0.3253 - acc: 0.875 - ETA: 2:05 - loss: 0.3265 - acc: 0.873 - ETA: 2:00 - loss: 0.3273 - acc: 0.873 - ETA: 1:53 - loss: 0.3264 - acc: 0.872 - ETA: 1:49 - loss: 0.3261 - acc: 0.872 - ETA: 1:44 - loss: 0.3251 - acc: 0.873 - ETA: 1:39 - loss: 0.3256 - acc: 0.872 - ETA: 1:35 - loss: 0.3256 - acc: 0.872 - ETA: 1:30 - loss: 0.3254 - acc: 0.872 - ETA: 1:25 - loss: 0.3253 - acc: 0.873 - ETA: 1:20 - loss: 0.3263 - acc: 0.872 - ETA: 1:16 - loss: 0.3266 - acc: 0.871 - ETA: 1:11 - loss: 0.3270 - acc: 0.871 - ETA: 1:06 - loss: 0.3264 - acc: 0.872 - ETA: 1:01 - loss: 0.3275 - acc: 0.871 - ETA: 57s - loss: 0.3267 - acc: 0.872 - ETA: 52s - loss: 0.3268 - acc: 0.87 - ETA: 47s - loss: 0.3269 - acc: 0.87 - ETA: 42s - loss: 0.3271 - acc: 0.87 - ETA: 38s - loss: 0.3263 - acc: 0.87 - ETA: 33s - loss: 0.3266 - acc: 0.87 - ETA: 28s - loss: 0.3265 - acc: 0.87 - ETA: 23s - loss: 0.3259 - acc: 0.87 - ETA: 19s - loss: 0.3256 - acc: 0.87 - ETA: 14s - loss: 0.3263 - acc: 0.87 - ETA: 9s - loss: 0.3260 - acc: 0.8728 - ETA: 4s - loss: 0.3258 - acc: 0.8727\n",
      "Epoch 00079: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3253 - acc: 0.8731 - val_loss: 0.6676 - val_acc: 0.6629\n",
      "Epoch 80/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3203 - acc: 0.885 - ETA: 6:06 - loss: 0.2967 - acc: 0.901 - ETA: 6:01 - loss: 0.3144 - acc: 0.881 - ETA: 5:56 - loss: 0.3194 - acc: 0.872 - ETA: 5:52 - loss: 0.3187 - acc: 0.870 - ETA: 4:57 - loss: 0.3427 - acc: 0.850 - ETA: 5:00 - loss: 0.3507 - acc: 0.839 - ETA: 5:01 - loss: 0.3428 - acc: 0.847 - ETA: 5:00 - loss: 0.3342 - acc: 0.856 - ETA: 4:59 - loss: 0.3320 - acc: 0.861 - ETA: 4:57 - loss: 0.3229 - acc: 0.868 - ETA: 4:55 - loss: 0.3222 - acc: 0.870 - ETA: 4:52 - loss: 0.3265 - acc: 0.864 - ETA: 4:49 - loss: 0.3303 - acc: 0.859 - ETA: 4:46 - loss: 0.3287 - acc: 0.860 - ETA: 4:42 - loss: 0.3241 - acc: 0.863 - ETA: 4:39 - loss: 0.3282 - acc: 0.862 - ETA: 4:35 - loss: 0.3252 - acc: 0.865 - ETA: 4:31 - loss: 0.3234 - acc: 0.867 - ETA: 4:27 - loss: 0.3244 - acc: 0.867 - ETA: 4:23 - loss: 0.3288 - acc: 0.863 - ETA: 4:19 - loss: 0.3259 - acc: 0.863 - ETA: 4:15 - loss: 0.3238 - acc: 0.865 - ETA: 4:11 - loss: 0.3215 - acc: 0.867 - ETA: 4:06 - loss: 0.3224 - acc: 0.867 - ETA: 4:02 - loss: 0.3228 - acc: 0.867 - ETA: 3:58 - loss: 0.3239 - acc: 0.866 - ETA: 3:53 - loss: 0.3211 - acc: 0.869 - ETA: 3:49 - loss: 0.3220 - acc: 0.868 - ETA: 3:44 - loss: 0.3239 - acc: 0.867 - ETA: 3:40 - loss: 0.3238 - acc: 0.867 - ETA: 3:35 - loss: 0.3232 - acc: 0.868 - ETA: 3:31 - loss: 0.3229 - acc: 0.868 - ETA: 3:26 - loss: 0.3221 - acc: 0.869 - ETA: 3:22 - loss: 0.3212 - acc: 0.870 - ETA: 3:17 - loss: 0.3207 - acc: 0.870 - ETA: 3:13 - loss: 0.3226 - acc: 0.868 - ETA: 3:08 - loss: 0.3215 - acc: 0.869 - ETA: 3:03 - loss: 0.3227 - acc: 0.869 - ETA: 2:59 - loss: 0.3238 - acc: 0.869 - ETA: 2:54 - loss: 0.3245 - acc: 0.868 - ETA: 2:50 - loss: 0.3262 - acc: 0.868 - ETA: 2:45 - loss: 0.3259 - acc: 0.867 - ETA: 2:40 - loss: 0.3257 - acc: 0.867 - ETA: 2:36 - loss: 0.3262 - acc: 0.866 - ETA: 2:31 - loss: 0.3267 - acc: 0.865 - ETA: 2:26 - loss: 0.3273 - acc: 0.865 - ETA: 2:22 - loss: 0.3259 - acc: 0.867 - ETA: 2:17 - loss: 0.3273 - acc: 0.865 - ETA: 2:12 - loss: 0.3266 - acc: 0.865 - ETA: 2:08 - loss: 0.3260 - acc: 0.866 - ETA: 2:03 - loss: 0.3251 - acc: 0.866 - ETA: 1:58 - loss: 0.3248 - acc: 0.866 - ETA: 1:53 - loss: 0.3256 - acc: 0.865 - ETA: 1:49 - loss: 0.3254 - acc: 0.867 - ETA: 1:44 - loss: 0.3257 - acc: 0.867 - ETA: 1:39 - loss: 0.3260 - acc: 0.867 - ETA: 1:35 - loss: 0.3272 - acc: 0.866 - ETA: 1:30 - loss: 0.3260 - acc: 0.868 - ETA: 1:25 - loss: 0.3261 - acc: 0.868 - ETA: 1:20 - loss: 0.3284 - acc: 0.866 - ETA: 1:16 - loss: 0.3285 - acc: 0.866 - ETA: 1:11 - loss: 0.3282 - acc: 0.866 - ETA: 1:06 - loss: 0.3277 - acc: 0.866 - ETA: 1:01 - loss: 0.3287 - acc: 0.866 - ETA: 57s - loss: 0.3279 - acc: 0.866 - ETA: 52s - loss: 0.3280 - acc: 0.86 - ETA: 47s - loss: 0.3281 - acc: 0.86 - ETA: 42s - loss: 0.3281 - acc: 0.86 - ETA: 38s - loss: 0.3282 - acc: 0.86 - ETA: 33s - loss: 0.3277 - acc: 0.86 - ETA: 28s - loss: 0.3274 - acc: 0.86 - ETA: 23s - loss: 0.3277 - acc: 0.86 - ETA: 19s - loss: 0.3267 - acc: 0.86 - ETA: 14s - loss: 0.3274 - acc: 0.86 - ETA: 9s - loss: 0.3276 - acc: 0.8687 - ETA: 4s - loss: 0.3270 - acc: 0.8695\n",
      "Epoch 00080: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3268 - acc: 0.8697 - val_loss: 0.6905 - val_acc: 0.6604\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3398 - acc: 0.875 - ETA: 6:06 - loss: 0.3373 - acc: 0.880 - ETA: 6:01 - loss: 0.3238 - acc: 0.885 - ETA: 5:56 - loss: 0.3396 - acc: 0.872 - ETA: 5:51 - loss: 0.3265 - acc: 0.883 - ETA: 5:46 - loss: 0.3360 - acc: 0.871 - ETA: 5:42 - loss: 0.3293 - acc: 0.876 - ETA: 5:37 - loss: 0.3298 - acc: 0.875 - ETA: 5:32 - loss: 0.3316 - acc: 0.875 - ETA: 5:27 - loss: 0.3270 - acc: 0.876 - ETA: 5:22 - loss: 0.3314 - acc: 0.873 - ETA: 5:18 - loss: 0.3346 - acc: 0.866 - ETA: 5:13 - loss: 0.3396 - acc: 0.864 - ETA: 5:08 - loss: 0.3369 - acc: 0.866 - ETA: 5:03 - loss: 0.3340 - acc: 0.868 - ETA: 4:58 - loss: 0.3380 - acc: 0.865 - ETA: 4:54 - loss: 0.3347 - acc: 0.868 - ETA: 4:49 - loss: 0.3384 - acc: 0.864 - ETA: 4:44 - loss: 0.3388 - acc: 0.862 - ETA: 4:39 - loss: 0.3354 - acc: 0.866 - ETA: 4:34 - loss: 0.3329 - acc: 0.869 - ETA: 4:29 - loss: 0.3316 - acc: 0.870 - ETA: 4:25 - loss: 0.3327 - acc: 0.870 - ETA: 4:20 - loss: 0.3341 - acc: 0.868 - ETA: 4:06 - loss: 0.3369 - acc: 0.870 - ETA: 4:02 - loss: 0.3347 - acc: 0.870 - ETA: 3:57 - loss: 0.3326 - acc: 0.872 - ETA: 3:53 - loss: 0.3316 - acc: 0.871 - ETA: 3:49 - loss: 0.3335 - acc: 0.871 - ETA: 3:44 - loss: 0.3342 - acc: 0.869 - ETA: 3:40 - loss: 0.3356 - acc: 0.867 - ETA: 3:35 - loss: 0.3366 - acc: 0.868 - ETA: 3:31 - loss: 0.3331 - acc: 0.871 - ETA: 3:26 - loss: 0.3349 - acc: 0.869 - ETA: 3:22 - loss: 0.3337 - acc: 0.870 - ETA: 3:17 - loss: 0.3347 - acc: 0.868 - ETA: 3:13 - loss: 0.3325 - acc: 0.870 - ETA: 3:08 - loss: 0.3317 - acc: 0.871 - ETA: 3:03 - loss: 0.3319 - acc: 0.870 - ETA: 2:59 - loss: 0.3313 - acc: 0.869 - ETA: 2:54 - loss: 0.3320 - acc: 0.869 - ETA: 2:50 - loss: 0.3316 - acc: 0.870 - ETA: 2:45 - loss: 0.3336 - acc: 0.867 - ETA: 2:40 - loss: 0.3317 - acc: 0.869 - ETA: 2:36 - loss: 0.3328 - acc: 0.868 - ETA: 2:31 - loss: 0.3306 - acc: 0.870 - ETA: 2:26 - loss: 0.3297 - acc: 0.870 - ETA: 2:22 - loss: 0.3287 - acc: 0.872 - ETA: 2:17 - loss: 0.3289 - acc: 0.872 - ETA: 2:12 - loss: 0.3281 - acc: 0.872 - ETA: 2:07 - loss: 0.3281 - acc: 0.872 - ETA: 2:03 - loss: 0.3287 - acc: 0.871 - ETA: 1:58 - loss: 0.3294 - acc: 0.871 - ETA: 1:53 - loss: 0.3310 - acc: 0.869 - ETA: 1:49 - loss: 0.3322 - acc: 0.868 - ETA: 1:44 - loss: 0.3320 - acc: 0.868 - ETA: 1:39 - loss: 0.3325 - acc: 0.868 - ETA: 1:34 - loss: 0.3333 - acc: 0.868 - ETA: 1:30 - loss: 0.3334 - acc: 0.868 - ETA: 1:25 - loss: 0.3324 - acc: 0.869 - ETA: 1:20 - loss: 0.3329 - acc: 0.869 - ETA: 1:16 - loss: 0.3324 - acc: 0.869 - ETA: 1:11 - loss: 0.3326 - acc: 0.869 - ETA: 1:06 - loss: 0.3324 - acc: 0.869 - ETA: 1:01 - loss: 0.3326 - acc: 0.868 - ETA: 57s - loss: 0.3328 - acc: 0.868 - ETA: 52s - loss: 0.3344 - acc: 0.86 - ETA: 47s - loss: 0.3332 - acc: 0.86 - ETA: 42s - loss: 0.3336 - acc: 0.86 - ETA: 38s - loss: 0.3335 - acc: 0.86 - ETA: 33s - loss: 0.3335 - acc: 0.86 - ETA: 28s - loss: 0.3327 - acc: 0.86 - ETA: 23s - loss: 0.3321 - acc: 0.87 - ETA: 19s - loss: 0.3322 - acc: 0.86 - ETA: 14s - loss: 0.3332 - acc: 0.86 - ETA: 9s - loss: 0.3326 - acc: 0.8694 - ETA: 4s - loss: 0.3330 - acc: 0.8695\n",
      "Epoch 00081: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3334 - acc: 0.8691 - val_loss: 0.6645 - val_acc: 0.6616\n",
      "Epoch 82/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.2734 - acc: 0.937 - ETA: 6:06 - loss: 0.2966 - acc: 0.906 - ETA: 6:01 - loss: 0.3093 - acc: 0.888 - ETA: 5:56 - loss: 0.3039 - acc: 0.888 - ETA: 5:52 - loss: 0.3158 - acc: 0.887 - ETA: 5:47 - loss: 0.3240 - acc: 0.880 - ETA: 5:42 - loss: 0.3195 - acc: 0.879 - ETA: 5:37 - loss: 0.3162 - acc: 0.881 - ETA: 5:32 - loss: 0.3165 - acc: 0.881 - ETA: 5:27 - loss: 0.3114 - acc: 0.884 - ETA: 5:23 - loss: 0.3146 - acc: 0.877 - ETA: 5:18 - loss: 0.3182 - acc: 0.874 - ETA: 5:13 - loss: 0.3204 - acc: 0.874 - ETA: 5:08 - loss: 0.3210 - acc: 0.872 - ETA: 5:03 - loss: 0.3204 - acc: 0.871 - ETA: 4:59 - loss: 0.3263 - acc: 0.868 - ETA: 4:54 - loss: 0.3308 - acc: 0.864 - ETA: 4:35 - loss: 0.3409 - acc: 0.862 - ETA: 4:31 - loss: 0.3396 - acc: 0.864 - ETA: 4:27 - loss: 0.3355 - acc: 0.868 - ETA: 4:23 - loss: 0.3381 - acc: 0.866 - ETA: 4:19 - loss: 0.3394 - acc: 0.866 - ETA: 4:15 - loss: 0.3416 - acc: 0.864 - ETA: 4:11 - loss: 0.3390 - acc: 0.866 - ETA: 4:06 - loss: 0.3376 - acc: 0.866 - ETA: 4:02 - loss: 0.3369 - acc: 0.867 - ETA: 3:58 - loss: 0.3362 - acc: 0.868 - ETA: 3:53 - loss: 0.3373 - acc: 0.866 - ETA: 3:49 - loss: 0.3346 - acc: 0.869 - ETA: 3:44 - loss: 0.3350 - acc: 0.869 - ETA: 3:40 - loss: 0.3329 - acc: 0.871 - ETA: 3:35 - loss: 0.3325 - acc: 0.872 - ETA: 3:31 - loss: 0.3324 - acc: 0.873 - ETA: 3:26 - loss: 0.3314 - acc: 0.874 - ETA: 3:22 - loss: 0.3329 - acc: 0.874 - ETA: 3:17 - loss: 0.3330 - acc: 0.872 - ETA: 3:13 - loss: 0.3354 - acc: 0.871 - ETA: 3:08 - loss: 0.3351 - acc: 0.871 - ETA: 3:03 - loss: 0.3350 - acc: 0.871 - ETA: 2:59 - loss: 0.3360 - acc: 0.871 - ETA: 2:54 - loss: 0.3342 - acc: 0.872 - ETA: 2:50 - loss: 0.3328 - acc: 0.873 - ETA: 2:45 - loss: 0.3323 - acc: 0.873 - ETA: 2:40 - loss: 0.3307 - acc: 0.874 - ETA: 2:36 - loss: 0.3298 - acc: 0.875 - ETA: 2:31 - loss: 0.3295 - acc: 0.875 - ETA: 2:26 - loss: 0.3298 - acc: 0.875 - ETA: 2:22 - loss: 0.3300 - acc: 0.875 - ETA: 2:17 - loss: 0.3289 - acc: 0.876 - ETA: 2:12 - loss: 0.3275 - acc: 0.876 - ETA: 2:08 - loss: 0.3269 - acc: 0.876 - ETA: 2:03 - loss: 0.3261 - acc: 0.877 - ETA: 1:58 - loss: 0.3274 - acc: 0.876 - ETA: 1:53 - loss: 0.3272 - acc: 0.875 - ETA: 1:49 - loss: 0.3282 - acc: 0.875 - ETA: 1:44 - loss: 0.3293 - acc: 0.874 - ETA: 1:39 - loss: 0.3314 - acc: 0.873 - ETA: 1:35 - loss: 0.3305 - acc: 0.874 - ETA: 1:30 - loss: 0.3310 - acc: 0.873 - ETA: 1:25 - loss: 0.3303 - acc: 0.874 - ETA: 1:20 - loss: 0.3298 - acc: 0.874 - ETA: 1:16 - loss: 0.3305 - acc: 0.873 - ETA: 1:11 - loss: 0.3295 - acc: 0.874 - ETA: 1:06 - loss: 0.3302 - acc: 0.873 - ETA: 1:01 - loss: 0.3304 - acc: 0.874 - ETA: 57s - loss: 0.3296 - acc: 0.874 - ETA: 52s - loss: 0.3287 - acc: 0.87 - ETA: 47s - loss: 0.3288 - acc: 0.87 - ETA: 42s - loss: 0.3282 - acc: 0.87 - ETA: 38s - loss: 0.3278 - acc: 0.87 - ETA: 33s - loss: 0.3267 - acc: 0.87 - ETA: 28s - loss: 0.3272 - acc: 0.87 - ETA: 23s - loss: 0.3273 - acc: 0.87 - ETA: 19s - loss: 0.3264 - acc: 0.87 - ETA: 14s - loss: 0.3266 - acc: 0.87 - ETA: 9s - loss: 0.3267 - acc: 0.8754 - ETA: 4s - loss: 0.3254 - acc: 0.8761\n",
      "Epoch 00082: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3250 - acc: 0.8767 - val_loss: 0.6740 - val_acc: 0.6610\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:09 - loss: 0.3653 - acc: 0.864 - ETA: 6:05 - loss: 0.3649 - acc: 0.838 - ETA: 6:01 - loss: 0.3433 - acc: 0.850 - ETA: 5:56 - loss: 0.3350 - acc: 0.856 - ETA: 5:51 - loss: 0.3486 - acc: 0.858 - ETA: 5:47 - loss: 0.3385 - acc: 0.866 - ETA: 5:42 - loss: 0.3358 - acc: 0.875 - ETA: 5:37 - loss: 0.3402 - acc: 0.868 - ETA: 5:32 - loss: 0.3303 - acc: 0.875 - ETA: 5:27 - loss: 0.3340 - acc: 0.872 - ETA: 5:23 - loss: 0.3296 - acc: 0.875 - ETA: 5:18 - loss: 0.3269 - acc: 0.877 - ETA: 5:13 - loss: 0.3237 - acc: 0.879 - ETA: 5:08 - loss: 0.3252 - acc: 0.876 - ETA: 5:03 - loss: 0.3283 - acc: 0.872 - ETA: 4:58 - loss: 0.3268 - acc: 0.871 - ETA: 4:54 - loss: 0.3319 - acc: 0.868 - ETA: 4:49 - loss: 0.3326 - acc: 0.868 - ETA: 4:44 - loss: 0.3318 - acc: 0.867 - ETA: 4:39 - loss: 0.3319 - acc: 0.867 - ETA: 4:34 - loss: 0.3322 - acc: 0.868 - ETA: 4:29 - loss: 0.3307 - acc: 0.869 - ETA: 4:25 - loss: 0.3327 - acc: 0.865 - ETA: 4:20 - loss: 0.3324 - acc: 0.866 - ETA: 4:15 - loss: 0.3319 - acc: 0.867 - ETA: 4:10 - loss: 0.3328 - acc: 0.865 - ETA: 4:05 - loss: 0.3300 - acc: 0.868 - ETA: 4:01 - loss: 0.3298 - acc: 0.869 - ETA: 3:56 - loss: 0.3302 - acc: 0.867 - ETA: 3:51 - loss: 0.3296 - acc: 0.868 - ETA: 3:46 - loss: 0.3277 - acc: 0.870 - ETA: 3:41 - loss: 0.3284 - acc: 0.870 - ETA: 3:31 - loss: 0.3266 - acc: 0.871 - ETA: 3:26 - loss: 0.3299 - acc: 0.867 - ETA: 3:22 - loss: 0.3293 - acc: 0.868 - ETA: 3:17 - loss: 0.3283 - acc: 0.868 - ETA: 3:13 - loss: 0.3289 - acc: 0.867 - ETA: 3:08 - loss: 0.3285 - acc: 0.867 - ETA: 3:03 - loss: 0.3268 - acc: 0.869 - ETA: 2:59 - loss: 0.3278 - acc: 0.868 - ETA: 2:54 - loss: 0.3274 - acc: 0.868 - ETA: 2:50 - loss: 0.3277 - acc: 0.867 - ETA: 2:45 - loss: 0.3261 - acc: 0.868 - ETA: 2:40 - loss: 0.3270 - acc: 0.867 - ETA: 2:36 - loss: 0.3267 - acc: 0.868 - ETA: 2:31 - loss: 0.3271 - acc: 0.867 - ETA: 2:26 - loss: 0.3285 - acc: 0.866 - ETA: 2:22 - loss: 0.3264 - acc: 0.868 - ETA: 2:17 - loss: 0.3270 - acc: 0.868 - ETA: 2:12 - loss: 0.3280 - acc: 0.867 - ETA: 2:07 - loss: 0.3282 - acc: 0.868 - ETA: 2:03 - loss: 0.3296 - acc: 0.866 - ETA: 1:58 - loss: 0.3301 - acc: 0.866 - ETA: 1:53 - loss: 0.3297 - acc: 0.865 - ETA: 1:49 - loss: 0.3288 - acc: 0.866 - ETA: 1:44 - loss: 0.3281 - acc: 0.867 - ETA: 1:39 - loss: 0.3278 - acc: 0.867 - ETA: 1:35 - loss: 0.3274 - acc: 0.867 - ETA: 1:30 - loss: 0.3271 - acc: 0.867 - ETA: 1:25 - loss: 0.3267 - acc: 0.867 - ETA: 1:20 - loss: 0.3266 - acc: 0.867 - ETA: 1:16 - loss: 0.3259 - acc: 0.868 - ETA: 1:11 - loss: 0.3258 - acc: 0.868 - ETA: 1:06 - loss: 0.3260 - acc: 0.869 - ETA: 1:01 - loss: 0.3259 - acc: 0.869 - ETA: 57s - loss: 0.3257 - acc: 0.869 - ETA: 52s - loss: 0.3250 - acc: 0.87 - ETA: 47s - loss: 0.3252 - acc: 0.87 - ETA: 42s - loss: 0.3269 - acc: 0.87 - ETA: 38s - loss: 0.3277 - acc: 0.86 - ETA: 33s - loss: 0.3282 - acc: 0.86 - ETA: 28s - loss: 0.3283 - acc: 0.86 - ETA: 23s - loss: 0.3290 - acc: 0.86 - ETA: 19s - loss: 0.3290 - acc: 0.86 - ETA: 14s - loss: 0.3294 - acc: 0.86 - ETA: 9s - loss: 0.3282 - acc: 0.8681 - ETA: 4s - loss: 0.3273 - acc: 0.8689\n",
      "Epoch 00083: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3263 - acc: 0.8698 - val_loss: 0.7022 - val_acc: 0.6572\n",
      "Epoch 84/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.2441 - acc: 0.958 - ETA: 6:06 - loss: 0.3371 - acc: 0.875 - ETA: 6:01 - loss: 0.3216 - acc: 0.878 - ETA: 5:56 - loss: 0.3430 - acc: 0.849 - ETA: 5:51 - loss: 0.3263 - acc: 0.862 - ETA: 5:46 - loss: 0.3306 - acc: 0.862 - ETA: 5:42 - loss: 0.3339 - acc: 0.861 - ETA: 5:37 - loss: 0.3307 - acc: 0.865 - ETA: 5:32 - loss: 0.3221 - acc: 0.875 - ETA: 5:27 - loss: 0.3227 - acc: 0.875 - ETA: 5:23 - loss: 0.3230 - acc: 0.876 - ETA: 5:18 - loss: 0.3262 - acc: 0.870 - ETA: 5:13 - loss: 0.3237 - acc: 0.873 - ETA: 5:08 - loss: 0.3211 - acc: 0.876 - ETA: 5:03 - loss: 0.3196 - acc: 0.877 - ETA: 4:59 - loss: 0.3159 - acc: 0.880 - ETA: 4:54 - loss: 0.3165 - acc: 0.880 - ETA: 4:49 - loss: 0.3162 - acc: 0.881 - ETA: 4:44 - loss: 0.3158 - acc: 0.882 - ETA: 4:39 - loss: 0.3141 - acc: 0.883 - ETA: 4:34 - loss: 0.3145 - acc: 0.882 - ETA: 4:30 - loss: 0.3137 - acc: 0.884 - ETA: 4:25 - loss: 0.3151 - acc: 0.882 - ETA: 4:20 - loss: 0.3150 - acc: 0.881 - ETA: 4:15 - loss: 0.3127 - acc: 0.882 - ETA: 4:10 - loss: 0.3093 - acc: 0.884 - ETA: 4:06 - loss: 0.3090 - acc: 0.886 - ETA: 4:01 - loss: 0.3072 - acc: 0.887 - ETA: 3:56 - loss: 0.3069 - acc: 0.887 - ETA: 3:51 - loss: 0.3065 - acc: 0.887 - ETA: 3:46 - loss: 0.3102 - acc: 0.885 - ETA: 3:41 - loss: 0.3112 - acc: 0.884 - ETA: 3:37 - loss: 0.3129 - acc: 0.883 - ETA: 3:32 - loss: 0.3140 - acc: 0.883 - ETA: 3:27 - loss: 0.3136 - acc: 0.883 - ETA: 3:22 - loss: 0.3159 - acc: 0.882 - ETA: 3:17 - loss: 0.3170 - acc: 0.880 - ETA: 3:12 - loss: 0.3191 - acc: 0.878 - ETA: 3:08 - loss: 0.3195 - acc: 0.876 - ETA: 2:59 - loss: 0.3205 - acc: 0.877 - ETA: 2:54 - loss: 0.3196 - acc: 0.878 - ETA: 2:50 - loss: 0.3185 - acc: 0.878 - ETA: 2:45 - loss: 0.3170 - acc: 0.880 - ETA: 2:40 - loss: 0.3181 - acc: 0.879 - ETA: 2:36 - loss: 0.3185 - acc: 0.878 - ETA: 2:31 - loss: 0.3201 - acc: 0.877 - ETA: 2:26 - loss: 0.3182 - acc: 0.879 - ETA: 2:22 - loss: 0.3178 - acc: 0.880 - ETA: 2:17 - loss: 0.3188 - acc: 0.879 - ETA: 2:12 - loss: 0.3202 - acc: 0.878 - ETA: 2:08 - loss: 0.3214 - acc: 0.877 - ETA: 2:03 - loss: 0.3231 - acc: 0.875 - ETA: 1:58 - loss: 0.3236 - acc: 0.875 - ETA: 1:53 - loss: 0.3243 - acc: 0.875 - ETA: 1:49 - loss: 0.3250 - acc: 0.874 - ETA: 1:44 - loss: 0.3248 - acc: 0.874 - ETA: 1:39 - loss: 0.3255 - acc: 0.873 - ETA: 1:35 - loss: 0.3249 - acc: 0.873 - ETA: 1:30 - loss: 0.3251 - acc: 0.873 - ETA: 1:25 - loss: 0.3239 - acc: 0.874 - ETA: 1:20 - loss: 0.3240 - acc: 0.874 - ETA: 1:16 - loss: 0.3235 - acc: 0.874 - ETA: 1:11 - loss: 0.3235 - acc: 0.874 - ETA: 1:06 - loss: 0.3228 - acc: 0.874 - ETA: 1:01 - loss: 0.3228 - acc: 0.874 - ETA: 57s - loss: 0.3223 - acc: 0.874 - ETA: 52s - loss: 0.3227 - acc: 0.87 - ETA: 47s - loss: 0.3229 - acc: 0.87 - ETA: 42s - loss: 0.3234 - acc: 0.87 - ETA: 38s - loss: 0.3232 - acc: 0.87 - ETA: 33s - loss: 0.3241 - acc: 0.87 - ETA: 28s - loss: 0.3250 - acc: 0.87 - ETA: 23s - loss: 0.3250 - acc: 0.87 - ETA: 19s - loss: 0.3247 - acc: 0.87 - ETA: 14s - loss: 0.3252 - acc: 0.87 - ETA: 9s - loss: 0.3244 - acc: 0.8723 - ETA: 4s - loss: 0.3242 - acc: 0.8727\n",
      "Epoch 00084: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3239 - acc: 0.8730 - val_loss: 0.6561 - val_acc: 0.6768\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3214 - acc: 0.885 - ETA: 6:05 - loss: 0.3054 - acc: 0.901 - ETA: 6:01 - loss: 0.3260 - acc: 0.878 - ETA: 5:56 - loss: 0.3169 - acc: 0.888 - ETA: 5:51 - loss: 0.3108 - acc: 0.891 - ETA: 5:46 - loss: 0.3149 - acc: 0.894 - ETA: 5:42 - loss: 0.3136 - acc: 0.889 - ETA: 5:37 - loss: 0.3180 - acc: 0.888 - ETA: 5:32 - loss: 0.3131 - acc: 0.887 - ETA: 5:27 - loss: 0.3101 - acc: 0.889 - ETA: 5:22 - loss: 0.3088 - acc: 0.887 - ETA: 5:17 - loss: 0.3074 - acc: 0.891 - ETA: 5:13 - loss: 0.3091 - acc: 0.889 - ETA: 5:08 - loss: 0.3117 - acc: 0.885 - ETA: 5:03 - loss: 0.3159 - acc: 0.881 - ETA: 4:58 - loss: 0.3221 - acc: 0.875 - ETA: 4:53 - loss: 0.3208 - acc: 0.877 - ETA: 4:49 - loss: 0.3221 - acc: 0.876 - ETA: 4:44 - loss: 0.3228 - acc: 0.875 - ETA: 4:39 - loss: 0.3282 - acc: 0.871 - ETA: 4:34 - loss: 0.3267 - acc: 0.872 - ETA: 4:29 - loss: 0.3274 - acc: 0.871 - ETA: 4:25 - loss: 0.3245 - acc: 0.875 - ETA: 4:20 - loss: 0.3250 - acc: 0.875 - ETA: 4:15 - loss: 0.3263 - acc: 0.874 - ETA: 4:10 - loss: 0.3261 - acc: 0.876 - ETA: 4:05 - loss: 0.3256 - acc: 0.876 - ETA: 4:01 - loss: 0.3291 - acc: 0.873 - ETA: 3:56 - loss: 0.3266 - acc: 0.875 - ETA: 3:51 - loss: 0.3233 - acc: 0.878 - ETA: 3:46 - loss: 0.3247 - acc: 0.877 - ETA: 3:41 - loss: 0.3240 - acc: 0.877 - ETA: 3:36 - loss: 0.3244 - acc: 0.878 - ETA: 3:32 - loss: 0.3235 - acc: 0.879 - ETA: 3:27 - loss: 0.3232 - acc: 0.878 - ETA: 3:22 - loss: 0.3232 - acc: 0.878 - ETA: 3:17 - loss: 0.3242 - acc: 0.877 - ETA: 3:12 - loss: 0.3243 - acc: 0.877 - ETA: 3:08 - loss: 0.3246 - acc: 0.876 - ETA: 3:03 - loss: 0.3236 - acc: 0.877 - ETA: 2:58 - loss: 0.3232 - acc: 0.877 - ETA: 2:53 - loss: 0.3219 - acc: 0.878 - ETA: 2:48 - loss: 0.3226 - acc: 0.878 - ETA: 2:43 - loss: 0.3214 - acc: 0.879 - ETA: 2:39 - loss: 0.3210 - acc: 0.879 - ETA: 2:34 - loss: 0.3215 - acc: 0.879 - ETA: 2:29 - loss: 0.3207 - acc: 0.879 - ETA: 2:24 - loss: 0.3200 - acc: 0.880 - ETA: 2:19 - loss: 0.3187 - acc: 0.881 - ETA: 2:15 - loss: 0.3199 - acc: 0.880 - ETA: 2:10 - loss: 0.3185 - acc: 0.881 - ETA: 2:05 - loss: 0.3180 - acc: 0.882 - ETA: 2:00 - loss: 0.3183 - acc: 0.882 - ETA: 1:55 - loss: 0.3173 - acc: 0.882 - ETA: 1:50 - loss: 0.3180 - acc: 0.881 - ETA: 1:46 - loss: 0.3183 - acc: 0.881 - ETA: 1:41 - loss: 0.3176 - acc: 0.882 - ETA: 1:36 - loss: 0.3165 - acc: 0.883 - ETA: 1:31 - loss: 0.3162 - acc: 0.883 - ETA: 1:26 - loss: 0.3164 - acc: 0.882 - ETA: 1:21 - loss: 0.3170 - acc: 0.882 - ETA: 1:17 - loss: 0.3178 - acc: 0.881 - ETA: 1:12 - loss: 0.3185 - acc: 0.881 - ETA: 1:07 - loss: 0.3183 - acc: 0.880 - ETA: 1:02 - loss: 0.3191 - acc: 0.879 - ETA: 57s - loss: 0.3192 - acc: 0.880 - ETA: 53s - loss: 0.3206 - acc: 0.87 - ETA: 48s - loss: 0.3195 - acc: 0.87 - ETA: 43s - loss: 0.3187 - acc: 0.87 - ETA: 38s - loss: 0.3179 - acc: 0.88 - ETA: 33s - loss: 0.3170 - acc: 0.88 - ETA: 28s - loss: 0.3169 - acc: 0.88 - ETA: 24s - loss: 0.3161 - acc: 0.88 - ETA: 19s - loss: 0.3153 - acc: 0.88 - ETA: 14s - loss: 0.3151 - acc: 0.88 - ETA: 9s - loss: 0.3157 - acc: 0.8823 - ETA: 4s - loss: 0.3151 - acc: 0.8828\n",
      "Epoch 00085: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.3145 - acc: 0.8831 - val_loss: 0.6946 - val_acc: 0.6547\n",
      "Epoch 86/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.2881 - acc: 0.895 - ETA: 6:07 - loss: 0.3176 - acc: 0.880 - ETA: 4:18 - loss: 0.2675 - acc: 0.920 - ETA: 4:40 - loss: 0.2873 - acc: 0.901 - ETA: 4:51 - loss: 0.3011 - acc: 0.891 - ETA: 4:57 - loss: 0.3019 - acc: 0.895 - ETA: 5:00 - loss: 0.3042 - acc: 0.895 - ETA: 5:01 - loss: 0.3029 - acc: 0.893 - ETA: 5:00 - loss: 0.2978 - acc: 0.899 - ETA: 4:59 - loss: 0.3003 - acc: 0.896 - ETA: 4:57 - loss: 0.3047 - acc: 0.891 - ETA: 4:55 - loss: 0.3134 - acc: 0.883 - ETA: 4:52 - loss: 0.3131 - acc: 0.882 - ETA: 4:49 - loss: 0.3121 - acc: 0.881 - ETA: 4:46 - loss: 0.3115 - acc: 0.881 - ETA: 4:42 - loss: 0.3114 - acc: 0.880 - ETA: 4:39 - loss: 0.3105 - acc: 0.879 - ETA: 4:35 - loss: 0.3157 - acc: 0.877 - ETA: 4:31 - loss: 0.3139 - acc: 0.879 - ETA: 4:27 - loss: 0.3188 - acc: 0.875 - ETA: 4:23 - loss: 0.3189 - acc: 0.876 - ETA: 4:19 - loss: 0.3225 - acc: 0.873 - ETA: 4:15 - loss: 0.3225 - acc: 0.875 - ETA: 4:10 - loss: 0.3211 - acc: 0.876 - ETA: 4:06 - loss: 0.3195 - acc: 0.877 - ETA: 4:02 - loss: 0.3191 - acc: 0.879 - ETA: 3:58 - loss: 0.3207 - acc: 0.877 - ETA: 3:53 - loss: 0.3196 - acc: 0.876 - ETA: 3:49 - loss: 0.3199 - acc: 0.876 - ETA: 3:44 - loss: 0.3195 - acc: 0.877 - ETA: 3:40 - loss: 0.3201 - acc: 0.877 - ETA: 3:35 - loss: 0.3202 - acc: 0.877 - ETA: 3:31 - loss: 0.3201 - acc: 0.877 - ETA: 3:26 - loss: 0.3195 - acc: 0.878 - ETA: 3:22 - loss: 0.3221 - acc: 0.876 - ETA: 3:17 - loss: 0.3234 - acc: 0.875 - ETA: 3:13 - loss: 0.3246 - acc: 0.875 - ETA: 3:08 - loss: 0.3236 - acc: 0.876 - ETA: 3:03 - loss: 0.3224 - acc: 0.877 - ETA: 2:59 - loss: 0.3244 - acc: 0.875 - ETA: 2:54 - loss: 0.3230 - acc: 0.876 - ETA: 2:50 - loss: 0.3235 - acc: 0.875 - ETA: 2:45 - loss: 0.3224 - acc: 0.875 - ETA: 2:40 - loss: 0.3223 - acc: 0.876 - ETA: 2:36 - loss: 0.3219 - acc: 0.876 - ETA: 2:31 - loss: 0.3216 - acc: 0.876 - ETA: 2:26 - loss: 0.3228 - acc: 0.874 - ETA: 2:22 - loss: 0.3231 - acc: 0.874 - ETA: 2:17 - loss: 0.3216 - acc: 0.876 - ETA: 2:12 - loss: 0.3204 - acc: 0.876 - ETA: 2:07 - loss: 0.3197 - acc: 0.877 - ETA: 2:03 - loss: 0.3199 - acc: 0.878 - ETA: 1:58 - loss: 0.3205 - acc: 0.878 - ETA: 1:53 - loss: 0.3198 - acc: 0.879 - ETA: 1:49 - loss: 0.3207 - acc: 0.878 - ETA: 1:44 - loss: 0.3201 - acc: 0.879 - ETA: 1:39 - loss: 0.3193 - acc: 0.880 - ETA: 1:34 - loss: 0.3186 - acc: 0.880 - ETA: 1:30 - loss: 0.3192 - acc: 0.879 - ETA: 1:25 - loss: 0.3208 - acc: 0.878 - ETA: 1:20 - loss: 0.3204 - acc: 0.878 - ETA: 1:16 - loss: 0.3198 - acc: 0.879 - ETA: 1:11 - loss: 0.3194 - acc: 0.880 - ETA: 1:06 - loss: 0.3194 - acc: 0.880 - ETA: 1:01 - loss: 0.3202 - acc: 0.880 - ETA: 56s - loss: 0.3209 - acc: 0.878 - ETA: 51s - loss: 0.3199 - acc: 0.87 - ETA: 46s - loss: 0.3202 - acc: 0.87 - ETA: 42s - loss: 0.3202 - acc: 0.87 - ETA: 37s - loss: 0.3192 - acc: 0.87 - ETA: 32s - loss: 0.3191 - acc: 0.87 - ETA: 28s - loss: 0.3187 - acc: 0.87 - ETA: 23s - loss: 0.3197 - acc: 0.87 - ETA: 18s - loss: 0.3195 - acc: 0.87 - ETA: 14s - loss: 0.3191 - acc: 0.87 - ETA: 9s - loss: 0.3188 - acc: 0.8777 - ETA: 4s - loss: 0.3183 - acc: 0.8781\n",
      "Epoch 00086: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.3181 - acc: 0.8782 - val_loss: 0.6698 - val_acc: 0.6686\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3288 - acc: 0.875 - ETA: 6:05 - loss: 0.3279 - acc: 0.880 - ETA: 6:01 - loss: 0.3228 - acc: 0.878 - ETA: 5:56 - loss: 0.3149 - acc: 0.888 - ETA: 5:51 - loss: 0.3232 - acc: 0.881 - ETA: 5:46 - loss: 0.3301 - acc: 0.873 - ETA: 5:41 - loss: 0.3252 - acc: 0.876 - ETA: 5:37 - loss: 0.3212 - acc: 0.881 - ETA: 5:32 - loss: 0.3171 - acc: 0.884 - ETA: 5:27 - loss: 0.3127 - acc: 0.889 - ETA: 5:22 - loss: 0.3149 - acc: 0.884 - ETA: 5:18 - loss: 0.3084 - acc: 0.888 - ETA: 5:13 - loss: 0.3104 - acc: 0.887 - ETA: 5:08 - loss: 0.3145 - acc: 0.881 - ETA: 5:03 - loss: 0.3153 - acc: 0.881 - ETA: 4:58 - loss: 0.3189 - acc: 0.881 - ETA: 4:54 - loss: 0.3192 - acc: 0.879 - ETA: 4:49 - loss: 0.3150 - acc: 0.882 - ETA: 4:44 - loss: 0.3187 - acc: 0.879 - ETA: 4:39 - loss: 0.3163 - acc: 0.881 - ETA: 4:34 - loss: 0.3156 - acc: 0.883 - ETA: 4:29 - loss: 0.3165 - acc: 0.883 - ETA: 4:25 - loss: 0.3150 - acc: 0.882 - ETA: 4:20 - loss: 0.3154 - acc: 0.882 - ETA: 4:15 - loss: 0.3147 - acc: 0.882 - ETA: 4:10 - loss: 0.3162 - acc: 0.881 - ETA: 4:05 - loss: 0.3137 - acc: 0.884 - ETA: 4:01 - loss: 0.3133 - acc: 0.884 - ETA: 3:56 - loss: 0.3183 - acc: 0.881 - ETA: 3:51 - loss: 0.3169 - acc: 0.880 - ETA: 3:46 - loss: 0.3158 - acc: 0.880 - ETA: 3:41 - loss: 0.3140 - acc: 0.882 - ETA: 3:36 - loss: 0.3153 - acc: 0.880 - ETA: 3:32 - loss: 0.3159 - acc: 0.879 - ETA: 3:27 - loss: 0.3148 - acc: 0.880 - ETA: 3:22 - loss: 0.3157 - acc: 0.879 - ETA: 3:17 - loss: 0.3161 - acc: 0.879 - ETA: 3:12 - loss: 0.3147 - acc: 0.880 - ETA: 3:08 - loss: 0.3136 - acc: 0.881 - ETA: 3:03 - loss: 0.3132 - acc: 0.881 - ETA: 2:58 - loss: 0.3138 - acc: 0.880 - ETA: 2:53 - loss: 0.3135 - acc: 0.880 - ETA: 2:48 - loss: 0.3142 - acc: 0.879 - ETA: 2:43 - loss: 0.3128 - acc: 0.880 - ETA: 2:39 - loss: 0.3130 - acc: 0.880 - ETA: 2:34 - loss: 0.3119 - acc: 0.881 - ETA: 2:29 - loss: 0.3110 - acc: 0.882 - ETA: 2:24 - loss: 0.3102 - acc: 0.883 - ETA: 2:19 - loss: 0.3091 - acc: 0.883 - ETA: 2:12 - loss: 0.3138 - acc: 0.879 - ETA: 2:07 - loss: 0.3149 - acc: 0.878 - ETA: 2:03 - loss: 0.3140 - acc: 0.879 - ETA: 1:58 - loss: 0.3146 - acc: 0.878 - ETA: 1:53 - loss: 0.3146 - acc: 0.879 - ETA: 1:49 - loss: 0.3155 - acc: 0.878 - ETA: 1:44 - loss: 0.3152 - acc: 0.878 - ETA: 1:39 - loss: 0.3162 - acc: 0.878 - ETA: 1:34 - loss: 0.3146 - acc: 0.880 - ETA: 1:30 - loss: 0.3147 - acc: 0.879 - ETA: 1:25 - loss: 0.3144 - acc: 0.880 - ETA: 1:20 - loss: 0.3140 - acc: 0.880 - ETA: 1:16 - loss: 0.3149 - acc: 0.880 - ETA: 1:11 - loss: 0.3134 - acc: 0.881 - ETA: 1:06 - loss: 0.3129 - acc: 0.881 - ETA: 1:01 - loss: 0.3130 - acc: 0.880 - ETA: 57s - loss: 0.3127 - acc: 0.881 - ETA: 52s - loss: 0.3126 - acc: 0.88 - ETA: 47s - loss: 0.3136 - acc: 0.88 - ETA: 42s - loss: 0.3126 - acc: 0.88 - ETA: 38s - loss: 0.3128 - acc: 0.88 - ETA: 33s - loss: 0.3132 - acc: 0.88 - ETA: 28s - loss: 0.3122 - acc: 0.88 - ETA: 23s - loss: 0.3116 - acc: 0.88 - ETA: 19s - loss: 0.3113 - acc: 0.88 - ETA: 14s - loss: 0.3123 - acc: 0.88 - ETA: 9s - loss: 0.3126 - acc: 0.8812 - ETA: 4s - loss: 0.3126 - acc: 0.8812\n",
      "Epoch 00087: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3123 - acc: 0.8818 - val_loss: 0.7126 - val_acc: 0.6484\n",
      "Epoch 88/100\n",
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.2713 - acc: 0.916 - ETA: 6:06 - loss: 0.2982 - acc: 0.885 - ETA: 6:01 - loss: 0.2813 - acc: 0.899 - ETA: 5:56 - loss: 0.2891 - acc: 0.885 - ETA: 5:51 - loss: 0.2927 - acc: 0.889 - ETA: 5:46 - loss: 0.3045 - acc: 0.881 - ETA: 5:42 - loss: 0.3148 - acc: 0.879 - ETA: 5:37 - loss: 0.3183 - acc: 0.876 - ETA: 5:32 - loss: 0.3228 - acc: 0.869 - ETA: 5:27 - loss: 0.3205 - acc: 0.869 - ETA: 5:22 - loss: 0.3225 - acc: 0.869 - ETA: 5:18 - loss: 0.3171 - acc: 0.875 - ETA: 5:13 - loss: 0.3190 - acc: 0.874 - ETA: 5:08 - loss: 0.3181 - acc: 0.875 - ETA: 5:03 - loss: 0.3188 - acc: 0.876 - ETA: 4:58 - loss: 0.3216 - acc: 0.876 - ETA: 4:53 - loss: 0.3234 - acc: 0.875 - ETA: 4:49 - loss: 0.3207 - acc: 0.876 - ETA: 4:44 - loss: 0.3177 - acc: 0.878 - ETA: 4:39 - loss: 0.3172 - acc: 0.876 - ETA: 4:34 - loss: 0.3165 - acc: 0.877 - ETA: 4:29 - loss: 0.3153 - acc: 0.878 - ETA: 4:25 - loss: 0.3166 - acc: 0.878 - ETA: 4:20 - loss: 0.3161 - acc: 0.876 - ETA: 4:15 - loss: 0.3176 - acc: 0.875 - ETA: 4:10 - loss: 0.3168 - acc: 0.874 - ETA: 4:05 - loss: 0.3163 - acc: 0.876 - ETA: 4:00 - loss: 0.3206 - acc: 0.873 - ETA: 3:56 - loss: 0.3222 - acc: 0.871 - ETA: 3:51 - loss: 0.3221 - acc: 0.871 - ETA: 3:46 - loss: 0.3222 - acc: 0.870 - ETA: 3:41 - loss: 0.3220 - acc: 0.870 - ETA: 3:36 - loss: 0.3217 - acc: 0.871 - ETA: 3:32 - loss: 0.3198 - acc: 0.873 - ETA: 3:27 - loss: 0.3181 - acc: 0.874 - ETA: 3:22 - loss: 0.3200 - acc: 0.873 - ETA: 3:17 - loss: 0.3196 - acc: 0.872 - ETA: 3:12 - loss: 0.3189 - acc: 0.873 - ETA: 3:08 - loss: 0.3187 - acc: 0.874 - ETA: 3:03 - loss: 0.3180 - acc: 0.875 - ETA: 2:58 - loss: 0.3177 - acc: 0.875 - ETA: 2:53 - loss: 0.3175 - acc: 0.875 - ETA: 2:48 - loss: 0.3175 - acc: 0.876 - ETA: 2:43 - loss: 0.3161 - acc: 0.878 - ETA: 2:39 - loss: 0.3164 - acc: 0.878 - ETA: 2:34 - loss: 0.3164 - acc: 0.878 - ETA: 2:29 - loss: 0.3179 - acc: 0.877 - ETA: 2:24 - loss: 0.3191 - acc: 0.876 - ETA: 2:19 - loss: 0.3198 - acc: 0.876 - ETA: 2:14 - loss: 0.3198 - acc: 0.876 - ETA: 2:10 - loss: 0.3201 - acc: 0.876 - ETA: 2:05 - loss: 0.3177 - acc: 0.877 - ETA: 2:00 - loss: 0.3174 - acc: 0.878 - ETA: 1:55 - loss: 0.3176 - acc: 0.877 - ETA: 1:50 - loss: 0.3173 - acc: 0.878 - ETA: 1:46 - loss: 0.3165 - acc: 0.878 - ETA: 1:41 - loss: 0.3175 - acc: 0.877 - ETA: 1:36 - loss: 0.3160 - acc: 0.878 - ETA: 1:31 - loss: 0.3156 - acc: 0.878 - ETA: 1:26 - loss: 0.3149 - acc: 0.879 - ETA: 1:21 - loss: 0.3158 - acc: 0.878 - ETA: 1:17 - loss: 0.3157 - acc: 0.878 - ETA: 1:12 - loss: 0.3150 - acc: 0.878 - ETA: 1:07 - loss: 0.3152 - acc: 0.878 - ETA: 1:02 - loss: 0.3144 - acc: 0.878 - ETA: 57s - loss: 0.3139 - acc: 0.879 - ETA: 53s - loss: 0.3137 - acc: 0.88 - ETA: 48s - loss: 0.3131 - acc: 0.88 - ETA: 43s - loss: 0.3122 - acc: 0.88 - ETA: 38s - loss: 0.3134 - acc: 0.88 - ETA: 33s - loss: 0.3123 - acc: 0.88 - ETA: 28s - loss: 0.3132 - acc: 0.88 - ETA: 23s - loss: 0.3140 - acc: 0.88 - ETA: 19s - loss: 0.3157 - acc: 0.87 - ETA: 14s - loss: 0.3156 - acc: 0.87 - ETA: 9s - loss: 0.3155 - acc: 0.8792 - ETA: 4s - loss: 0.3153 - acc: 0.8792\n",
      "Epoch 00088: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3150 - acc: 0.8793 - val_loss: 0.6725 - val_acc: 0.6610\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3223 - acc: 0.875 - ETA: 6:05 - loss: 0.3427 - acc: 0.854 - ETA: 6:01 - loss: 0.3421 - acc: 0.854 - ETA: 5:56 - loss: 0.3498 - acc: 0.843 - ETA: 5:51 - loss: 0.3268 - acc: 0.862 - ETA: 5:46 - loss: 0.3201 - acc: 0.873 - ETA: 5:42 - loss: 0.3152 - acc: 0.876 - ETA: 5:37 - loss: 0.3185 - acc: 0.876 - ETA: 5:32 - loss: 0.3139 - acc: 0.873 - ETA: 5:28 - loss: 0.3144 - acc: 0.875 - ETA: 5:23 - loss: 0.3159 - acc: 0.875 - ETA: 5:18 - loss: 0.3191 - acc: 0.871 - ETA: 5:13 - loss: 0.3164 - acc: 0.875 - ETA: 5:08 - loss: 0.3177 - acc: 0.873 - ETA: 5:03 - loss: 0.3180 - acc: 0.875 - ETA: 4:59 - loss: 0.3163 - acc: 0.876 - ETA: 4:54 - loss: 0.3218 - acc: 0.872 - ETA: 4:49 - loss: 0.3207 - acc: 0.873 - ETA: 4:44 - loss: 0.3183 - acc: 0.872 - ETA: 4:39 - loss: 0.3167 - acc: 0.872 - ETA: 4:34 - loss: 0.3145 - acc: 0.875 - ETA: 4:30 - loss: 0.3132 - acc: 0.875 - ETA: 4:25 - loss: 0.3126 - acc: 0.876 - ETA: 4:20 - loss: 0.3153 - acc: 0.875 - ETA: 4:15 - loss: 0.3191 - acc: 0.872 - ETA: 4:10 - loss: 0.3219 - acc: 0.871 - ETA: 4:05 - loss: 0.3223 - acc: 0.871 - ETA: 4:01 - loss: 0.3232 - acc: 0.872 - ETA: 3:56 - loss: 0.3248 - acc: 0.870 - ETA: 3:51 - loss: 0.3253 - acc: 0.868 - ETA: 3:46 - loss: 0.3275 - acc: 0.866 - ETA: 3:41 - loss: 0.3273 - acc: 0.867 - ETA: 3:37 - loss: 0.3250 - acc: 0.869 - ETA: 3:32 - loss: 0.3218 - acc: 0.872 - ETA: 3:27 - loss: 0.3205 - acc: 0.872 - ETA: 3:22 - loss: 0.3227 - acc: 0.871 - ETA: 3:17 - loss: 0.3201 - acc: 0.873 - ETA: 3:12 - loss: 0.3219 - acc: 0.873 - ETA: 3:08 - loss: 0.3224 - acc: 0.872 - ETA: 3:03 - loss: 0.3235 - acc: 0.872 - ETA: 2:58 - loss: 0.3242 - acc: 0.871 - ETA: 2:53 - loss: 0.3241 - acc: 0.872 - ETA: 2:48 - loss: 0.3231 - acc: 0.872 - ETA: 2:40 - loss: 0.3269 - acc: 0.868 - ETA: 2:36 - loss: 0.3277 - acc: 0.867 - ETA: 2:31 - loss: 0.3281 - acc: 0.868 - ETA: 2:26 - loss: 0.3280 - acc: 0.868 - ETA: 2:22 - loss: 0.3281 - acc: 0.868 - ETA: 2:17 - loss: 0.3269 - acc: 0.870 - ETA: 2:12 - loss: 0.3259 - acc: 0.870 - ETA: 2:08 - loss: 0.3259 - acc: 0.870 - ETA: 2:03 - loss: 0.3260 - acc: 0.870 - ETA: 1:58 - loss: 0.3253 - acc: 0.871 - ETA: 1:53 - loss: 0.3241 - acc: 0.872 - ETA: 1:49 - loss: 0.3228 - acc: 0.873 - ETA: 1:44 - loss: 0.3225 - acc: 0.873 - ETA: 1:39 - loss: 0.3217 - acc: 0.874 - ETA: 1:35 - loss: 0.3223 - acc: 0.874 - ETA: 1:30 - loss: 0.3209 - acc: 0.875 - ETA: 1:25 - loss: 0.3210 - acc: 0.874 - ETA: 1:20 - loss: 0.3208 - acc: 0.875 - ETA: 1:16 - loss: 0.3206 - acc: 0.875 - ETA: 1:11 - loss: 0.3205 - acc: 0.875 - ETA: 1:06 - loss: 0.3194 - acc: 0.876 - ETA: 1:01 - loss: 0.3194 - acc: 0.876 - ETA: 57s - loss: 0.3186 - acc: 0.877 - ETA: 52s - loss: 0.3192 - acc: 0.87 - ETA: 47s - loss: 0.3190 - acc: 0.87 - ETA: 42s - loss: 0.3194 - acc: 0.87 - ETA: 38s - loss: 0.3196 - acc: 0.87 - ETA: 33s - loss: 0.3191 - acc: 0.87 - ETA: 28s - loss: 0.3184 - acc: 0.87 - ETA: 23s - loss: 0.3201 - acc: 0.87 - ETA: 19s - loss: 0.3205 - acc: 0.87 - ETA: 14s - loss: 0.3224 - acc: 0.87 - ETA: 9s - loss: 0.3233 - acc: 0.8725 - ETA: 4s - loss: 0.3238 - acc: 0.8722\n",
      "Epoch 00089: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3226 - acc: 0.8728 - val_loss: 0.6852 - val_acc: 0.6654\n",
      "Epoch 90/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3197 - acc: 0.843 - ETA: 6:06 - loss: 0.3070 - acc: 0.875 - ETA: 6:01 - loss: 0.3119 - acc: 0.878 - ETA: 5:56 - loss: 0.3165 - acc: 0.867 - ETA: 5:51 - loss: 0.3109 - acc: 0.877 - ETA: 5:46 - loss: 0.3177 - acc: 0.869 - ETA: 5:42 - loss: 0.3243 - acc: 0.866 - ETA: 5:37 - loss: 0.3235 - acc: 0.864 - ETA: 5:32 - loss: 0.3202 - acc: 0.868 - ETA: 5:27 - loss: 0.3230 - acc: 0.864 - ETA: 5:22 - loss: 0.3198 - acc: 0.870 - ETA: 5:18 - loss: 0.3224 - acc: 0.869 - ETA: 5:13 - loss: 0.3177 - acc: 0.875 - ETA: 5:08 - loss: 0.3119 - acc: 0.882 - ETA: 5:03 - loss: 0.3119 - acc: 0.884 - ETA: 4:58 - loss: 0.3081 - acc: 0.887 - ETA: 4:53 - loss: 0.3079 - acc: 0.884 - ETA: 4:49 - loss: 0.3055 - acc: 0.886 - ETA: 4:44 - loss: 0.3034 - acc: 0.887 - ETA: 4:39 - loss: 0.3013 - acc: 0.889 - ETA: 4:34 - loss: 0.3019 - acc: 0.889 - ETA: 4:29 - loss: 0.3008 - acc: 0.889 - ETA: 4:25 - loss: 0.2988 - acc: 0.890 - ETA: 4:20 - loss: 0.2963 - acc: 0.891 - ETA: 4:15 - loss: 0.2969 - acc: 0.890 - ETA: 4:10 - loss: 0.2979 - acc: 0.888 - ETA: 4:05 - loss: 0.2971 - acc: 0.890 - ETA: 4:01 - loss: 0.2980 - acc: 0.889 - ETA: 3:56 - loss: 0.2987 - acc: 0.887 - ETA: 3:51 - loss: 0.3009 - acc: 0.886 - ETA: 3:46 - loss: 0.3007 - acc: 0.885 - ETA: 3:41 - loss: 0.2998 - acc: 0.887 - ETA: 3:36 - loss: 0.3007 - acc: 0.886 - ETA: 3:32 - loss: 0.2998 - acc: 0.886 - ETA: 3:27 - loss: 0.3000 - acc: 0.886 - ETA: 3:22 - loss: 0.3021 - acc: 0.884 - ETA: 3:17 - loss: 0.3027 - acc: 0.884 - ETA: 3:12 - loss: 0.3040 - acc: 0.883 - ETA: 3:08 - loss: 0.3041 - acc: 0.883 - ETA: 3:03 - loss: 0.3059 - acc: 0.882 - ETA: 2:58 - loss: 0.3074 - acc: 0.880 - ETA: 2:53 - loss: 0.3089 - acc: 0.878 - ETA: 2:48 - loss: 0.3091 - acc: 0.878 - ETA: 2:43 - loss: 0.3086 - acc: 0.879 - ETA: 2:39 - loss: 0.3084 - acc: 0.880 - ETA: 2:34 - loss: 0.3089 - acc: 0.879 - ETA: 2:29 - loss: 0.3091 - acc: 0.879 - ETA: 2:24 - loss: 0.3131 - acc: 0.876 - ETA: 2:19 - loss: 0.3120 - acc: 0.877 - ETA: 2:14 - loss: 0.3111 - acc: 0.877 - ETA: 2:10 - loss: 0.3113 - acc: 0.877 - ETA: 2:05 - loss: 0.3108 - acc: 0.877 - ETA: 2:00 - loss: 0.3099 - acc: 0.878 - ETA: 1:55 - loss: 0.3087 - acc: 0.879 - ETA: 1:50 - loss: 0.3082 - acc: 0.880 - ETA: 1:46 - loss: 0.3096 - acc: 0.879 - ETA: 1:41 - loss: 0.3109 - acc: 0.878 - ETA: 1:36 - loss: 0.3108 - acc: 0.878 - ETA: 1:31 - loss: 0.3107 - acc: 0.879 - ETA: 1:26 - loss: 0.3103 - acc: 0.879 - ETA: 1:21 - loss: 0.3099 - acc: 0.879 - ETA: 1:17 - loss: 0.3089 - acc: 0.880 - ETA: 1:12 - loss: 0.3084 - acc: 0.880 - ETA: 1:07 - loss: 0.3077 - acc: 0.881 - ETA: 1:02 - loss: 0.3075 - acc: 0.881 - ETA: 57s - loss: 0.3073 - acc: 0.882 - ETA: 53s - loss: 0.3068 - acc: 0.88 - ETA: 48s - loss: 0.3069 - acc: 0.88 - ETA: 43s - loss: 0.3058 - acc: 0.88 - ETA: 38s - loss: 0.3064 - acc: 0.88 - ETA: 33s - loss: 0.3061 - acc: 0.88 - ETA: 28s - loss: 0.3059 - acc: 0.88 - ETA: 24s - loss: 0.3057 - acc: 0.88 - ETA: 19s - loss: 0.3057 - acc: 0.88 - ETA: 14s - loss: 0.3057 - acc: 0.88 - ETA: 9s - loss: 0.3058 - acc: 0.8829 - ETA: 4s - loss: 0.3067 - acc: 0.8824\n",
      "Epoch 00090: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.3062 - acc: 0.8830 - val_loss: 0.6764 - val_acc: 0.6597\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3387 - acc: 0.864 - ETA: 6:06 - loss: 0.2896 - acc: 0.890 - ETA: 6:01 - loss: 0.3260 - acc: 0.857 - ETA: 5:56 - loss: 0.3105 - acc: 0.877 - ETA: 5:51 - loss: 0.3206 - acc: 0.875 - ETA: 4:56 - loss: 0.3724 - acc: 0.812 - ETA: 4:59 - loss: 0.3593 - acc: 0.825 - ETA: 5:00 - loss: 0.3504 - acc: 0.835 - ETA: 5:00 - loss: 0.3438 - acc: 0.842 - ETA: 4:59 - loss: 0.3407 - acc: 0.845 - ETA: 4:57 - loss: 0.3326 - acc: 0.856 - ETA: 4:54 - loss: 0.3264 - acc: 0.861 - ETA: 4:52 - loss: 0.3241 - acc: 0.862 - ETA: 4:49 - loss: 0.3283 - acc: 0.863 - ETA: 4:46 - loss: 0.3271 - acc: 0.865 - ETA: 4:42 - loss: 0.3296 - acc: 0.861 - ETA: 4:38 - loss: 0.3288 - acc: 0.859 - ETA: 4:35 - loss: 0.3248 - acc: 0.862 - ETA: 4:31 - loss: 0.3264 - acc: 0.858 - ETA: 4:27 - loss: 0.3250 - acc: 0.859 - ETA: 4:23 - loss: 0.3244 - acc: 0.860 - ETA: 4:19 - loss: 0.3234 - acc: 0.862 - ETA: 4:15 - loss: 0.3237 - acc: 0.863 - ETA: 4:10 - loss: 0.3225 - acc: 0.864 - ETA: 4:06 - loss: 0.3199 - acc: 0.868 - ETA: 4:02 - loss: 0.3186 - acc: 0.869 - ETA: 3:57 - loss: 0.3183 - acc: 0.870 - ETA: 3:53 - loss: 0.3155 - acc: 0.872 - ETA: 3:49 - loss: 0.3157 - acc: 0.872 - ETA: 3:44 - loss: 0.3179 - acc: 0.871 - ETA: 3:40 - loss: 0.3204 - acc: 0.869 - ETA: 3:35 - loss: 0.3195 - acc: 0.869 - ETA: 3:31 - loss: 0.3202 - acc: 0.869 - ETA: 3:26 - loss: 0.3206 - acc: 0.870 - ETA: 3:22 - loss: 0.3202 - acc: 0.871 - ETA: 3:17 - loss: 0.3205 - acc: 0.871 - ETA: 3:13 - loss: 0.3191 - acc: 0.872 - ETA: 3:08 - loss: 0.3209 - acc: 0.871 - ETA: 3:03 - loss: 0.3202 - acc: 0.871 - ETA: 2:59 - loss: 0.3200 - acc: 0.871 - ETA: 2:54 - loss: 0.3202 - acc: 0.870 - ETA: 2:50 - loss: 0.3180 - acc: 0.872 - ETA: 2:45 - loss: 0.3169 - acc: 0.872 - ETA: 2:40 - loss: 0.3173 - acc: 0.871 - ETA: 2:36 - loss: 0.3169 - acc: 0.871 - ETA: 2:31 - loss: 0.3181 - acc: 0.871 - ETA: 2:26 - loss: 0.3168 - acc: 0.872 - ETA: 2:22 - loss: 0.3157 - acc: 0.873 - ETA: 2:17 - loss: 0.3139 - acc: 0.875 - ETA: 2:12 - loss: 0.3138 - acc: 0.875 - ETA: 2:08 - loss: 0.3138 - acc: 0.875 - ETA: 2:03 - loss: 0.3133 - acc: 0.875 - ETA: 1:58 - loss: 0.3132 - acc: 0.875 - ETA: 1:53 - loss: 0.3157 - acc: 0.873 - ETA: 1:49 - loss: 0.3148 - acc: 0.874 - ETA: 1:44 - loss: 0.3144 - acc: 0.874 - ETA: 1:39 - loss: 0.3149 - acc: 0.874 - ETA: 1:35 - loss: 0.3160 - acc: 0.873 - ETA: 1:30 - loss: 0.3151 - acc: 0.874 - ETA: 1:25 - loss: 0.3152 - acc: 0.874 - ETA: 1:20 - loss: 0.3158 - acc: 0.873 - ETA: 1:16 - loss: 0.3162 - acc: 0.873 - ETA: 1:11 - loss: 0.3172 - acc: 0.873 - ETA: 1:06 - loss: 0.3175 - acc: 0.873 - ETA: 1:01 - loss: 0.3178 - acc: 0.872 - ETA: 57s - loss: 0.3180 - acc: 0.872 - ETA: 52s - loss: 0.3179 - acc: 0.87 - ETA: 47s - loss: 0.3177 - acc: 0.87 - ETA: 42s - loss: 0.3196 - acc: 0.87 - ETA: 38s - loss: 0.3202 - acc: 0.87 - ETA: 33s - loss: 0.3194 - acc: 0.87 - ETA: 28s - loss: 0.3196 - acc: 0.87 - ETA: 23s - loss: 0.3194 - acc: 0.87 - ETA: 19s - loss: 0.3193 - acc: 0.87 - ETA: 14s - loss: 0.3195 - acc: 0.87 - ETA: 9s - loss: 0.3201 - acc: 0.8701 - ETA: 4s - loss: 0.3185 - acc: 0.8711\n",
      "Epoch 00091: val_loss did not improve\n",
      "78/78 [==============================] - 463s 6s/step - loss: 0.3180 - acc: 0.8717 - val_loss: 0.6822 - val_acc: 0.6642\n",
      "Epoch 92/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.2915 - acc: 0.906 - ETA: 6:06 - loss: 0.2699 - acc: 0.921 - ETA: 6:01 - loss: 0.2840 - acc: 0.909 - ETA: 5:56 - loss: 0.2812 - acc: 0.908 - ETA: 5:51 - loss: 0.2998 - acc: 0.889 - ETA: 5:47 - loss: 0.2961 - acc: 0.890 - ETA: 5:42 - loss: 0.2956 - acc: 0.886 - ETA: 5:37 - loss: 0.2979 - acc: 0.888 - ETA: 5:32 - loss: 0.2987 - acc: 0.888 - ETA: 5:27 - loss: 0.3053 - acc: 0.885 - ETA: 5:22 - loss: 0.3054 - acc: 0.888 - ETA: 4:55 - loss: 0.3176 - acc: 0.869 - ETA: 4:53 - loss: 0.3231 - acc: 0.867 - ETA: 4:50 - loss: 0.3277 - acc: 0.864 - ETA: 4:46 - loss: 0.3329 - acc: 0.860 - ETA: 4:43 - loss: 0.3308 - acc: 0.860 - ETA: 4:39 - loss: 0.3363 - acc: 0.859 - ETA: 4:35 - loss: 0.3329 - acc: 0.862 - ETA: 4:31 - loss: 0.3300 - acc: 0.864 - ETA: 4:27 - loss: 0.3268 - acc: 0.866 - ETA: 4:23 - loss: 0.3220 - acc: 0.871 - ETA: 4:19 - loss: 0.3226 - acc: 0.870 - ETA: 4:15 - loss: 0.3235 - acc: 0.870 - ETA: 4:11 - loss: 0.3217 - acc: 0.871 - ETA: 4:07 - loss: 0.3200 - acc: 0.873 - ETA: 4:02 - loss: 0.3195 - acc: 0.873 - ETA: 3:58 - loss: 0.3186 - acc: 0.874 - ETA: 3:53 - loss: 0.3170 - acc: 0.875 - ETA: 3:49 - loss: 0.3177 - acc: 0.874 - ETA: 3:45 - loss: 0.3160 - acc: 0.875 - ETA: 3:40 - loss: 0.3179 - acc: 0.876 - ETA: 3:36 - loss: 0.3175 - acc: 0.876 - ETA: 3:31 - loss: 0.3198 - acc: 0.873 - ETA: 3:26 - loss: 0.3202 - acc: 0.873 - ETA: 3:22 - loss: 0.3201 - acc: 0.873 - ETA: 3:17 - loss: 0.3199 - acc: 0.873 - ETA: 3:13 - loss: 0.3200 - acc: 0.873 - ETA: 3:08 - loss: 0.3195 - acc: 0.874 - ETA: 3:04 - loss: 0.3179 - acc: 0.876 - ETA: 2:59 - loss: 0.3180 - acc: 0.875 - ETA: 2:54 - loss: 0.3174 - acc: 0.875 - ETA: 2:50 - loss: 0.3166 - acc: 0.876 - ETA: 2:42 - loss: 0.3168 - acc: 0.875 - ETA: 2:37 - loss: 0.3155 - acc: 0.876 - ETA: 2:33 - loss: 0.3144 - acc: 0.877 - ETA: 2:28 - loss: 0.3151 - acc: 0.876 - ETA: 2:24 - loss: 0.3139 - acc: 0.877 - ETA: 2:19 - loss: 0.3131 - acc: 0.878 - ETA: 2:15 - loss: 0.3124 - acc: 0.879 - ETA: 2:10 - loss: 0.3105 - acc: 0.881 - ETA: 2:05 - loss: 0.3101 - acc: 0.881 - ETA: 2:01 - loss: 0.3092 - acc: 0.881 - ETA: 1:56 - loss: 0.3087 - acc: 0.881 - ETA: 1:52 - loss: 0.3102 - acc: 0.880 - ETA: 1:47 - loss: 0.3104 - acc: 0.880 - ETA: 1:42 - loss: 0.3091 - acc: 0.881 - ETA: 1:38 - loss: 0.3102 - acc: 0.880 - ETA: 1:33 - loss: 0.3097 - acc: 0.880 - ETA: 1:29 - loss: 0.3098 - acc: 0.880 - ETA: 1:24 - loss: 0.3089 - acc: 0.881 - ETA: 1:19 - loss: 0.3086 - acc: 0.882 - ETA: 1:15 - loss: 0.3085 - acc: 0.882 - ETA: 1:10 - loss: 0.3090 - acc: 0.881 - ETA: 1:05 - loss: 0.3081 - acc: 0.882 - ETA: 1:01 - loss: 0.3090 - acc: 0.881 - ETA: 56s - loss: 0.3100 - acc: 0.880 - ETA: 51s - loss: 0.3091 - acc: 0.88 - ETA: 47s - loss: 0.3094 - acc: 0.88 - ETA: 42s - loss: 0.3096 - acc: 0.88 - ETA: 37s - loss: 0.3096 - acc: 0.88 - ETA: 32s - loss: 0.3095 - acc: 0.88 - ETA: 28s - loss: 0.3098 - acc: 0.88 - ETA: 23s - loss: 0.3087 - acc: 0.88 - ETA: 18s - loss: 0.3098 - acc: 0.88 - ETA: 14s - loss: 0.3096 - acc: 0.88 - ETA: 9s - loss: 0.3106 - acc: 0.8805 - ETA: 4s - loss: 0.3097 - acc: 0.8811\n",
      "Epoch 00092: val_loss did not improve\n",
      "78/78 [==============================] - 456s 6s/step - loss: 0.3102 - acc: 0.8805 - val_loss: 0.6797 - val_acc: 0.6585\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.2134 - acc: 0.947 - ETA: 6:06 - loss: 0.2330 - acc: 0.937 - ETA: 6:01 - loss: 0.2738 - acc: 0.906 - ETA: 5:56 - loss: 0.2773 - acc: 0.901 - ETA: 5:52 - loss: 0.2863 - acc: 0.897 - ETA: 5:47 - loss: 0.2999 - acc: 0.888 - ETA: 5:42 - loss: 0.2959 - acc: 0.891 - ETA: 5:37 - loss: 0.3045 - acc: 0.884 - ETA: 5:32 - loss: 0.3083 - acc: 0.880 - ETA: 5:27 - loss: 0.3059 - acc: 0.877 - ETA: 5:22 - loss: 0.3027 - acc: 0.876 - ETA: 5:18 - loss: 0.2991 - acc: 0.881 - ETA: 5:13 - loss: 0.2967 - acc: 0.884 - ETA: 5:08 - loss: 0.2993 - acc: 0.881 - ETA: 5:03 - loss: 0.2982 - acc: 0.881 - ETA: 4:59 - loss: 0.3004 - acc: 0.882 - ETA: 4:54 - loss: 0.2958 - acc: 0.884 - ETA: 4:35 - loss: 0.3127 - acc: 0.868 - ETA: 4:31 - loss: 0.3145 - acc: 0.869 - ETA: 4:27 - loss: 0.3119 - acc: 0.871 - ETA: 4:23 - loss: 0.3100 - acc: 0.874 - ETA: 4:19 - loss: 0.3087 - acc: 0.875 - ETA: 4:15 - loss: 0.3078 - acc: 0.877 - ETA: 4:11 - loss: 0.3061 - acc: 0.877 - ETA: 4:06 - loss: 0.3069 - acc: 0.878 - ETA: 4:02 - loss: 0.3087 - acc: 0.878 - ETA: 3:58 - loss: 0.3076 - acc: 0.879 - ETA: 3:53 - loss: 0.3059 - acc: 0.880 - ETA: 3:49 - loss: 0.3045 - acc: 0.882 - ETA: 3:44 - loss: 0.3020 - acc: 0.884 - ETA: 3:40 - loss: 0.3041 - acc: 0.883 - ETA: 3:35 - loss: 0.3033 - acc: 0.883 - ETA: 3:31 - loss: 0.3036 - acc: 0.882 - ETA: 3:26 - loss: 0.3049 - acc: 0.881 - ETA: 3:22 - loss: 0.3056 - acc: 0.881 - ETA: 3:17 - loss: 0.3069 - acc: 0.879 - ETA: 3:13 - loss: 0.3068 - acc: 0.879 - ETA: 3:08 - loss: 0.3076 - acc: 0.878 - ETA: 3:03 - loss: 0.3070 - acc: 0.879 - ETA: 2:59 - loss: 0.3078 - acc: 0.878 - ETA: 2:54 - loss: 0.3083 - acc: 0.878 - ETA: 2:50 - loss: 0.3074 - acc: 0.878 - ETA: 2:45 - loss: 0.3079 - acc: 0.878 - ETA: 2:40 - loss: 0.3089 - acc: 0.878 - ETA: 2:36 - loss: 0.3119 - acc: 0.875 - ETA: 2:31 - loss: 0.3108 - acc: 0.875 - ETA: 2:26 - loss: 0.3103 - acc: 0.876 - ETA: 2:22 - loss: 0.3099 - acc: 0.876 - ETA: 2:17 - loss: 0.3083 - acc: 0.877 - ETA: 2:12 - loss: 0.3078 - acc: 0.878 - ETA: 2:08 - loss: 0.3067 - acc: 0.879 - ETA: 2:03 - loss: 0.3079 - acc: 0.878 - ETA: 1:58 - loss: 0.3089 - acc: 0.876 - ETA: 1:53 - loss: 0.3082 - acc: 0.877 - ETA: 1:49 - loss: 0.3071 - acc: 0.878 - ETA: 1:44 - loss: 0.3067 - acc: 0.879 - ETA: 1:39 - loss: 0.3072 - acc: 0.878 - ETA: 1:35 - loss: 0.3059 - acc: 0.879 - ETA: 1:30 - loss: 0.3067 - acc: 0.879 - ETA: 1:25 - loss: 0.3052 - acc: 0.880 - ETA: 1:20 - loss: 0.3053 - acc: 0.880 - ETA: 1:16 - loss: 0.3051 - acc: 0.880 - ETA: 1:11 - loss: 0.3057 - acc: 0.880 - ETA: 1:06 - loss: 0.3054 - acc: 0.880 - ETA: 1:01 - loss: 0.3062 - acc: 0.880 - ETA: 57s - loss: 0.3059 - acc: 0.880 - ETA: 52s - loss: 0.3069 - acc: 0.87 - ETA: 47s - loss: 0.3063 - acc: 0.88 - ETA: 42s - loss: 0.3057 - acc: 0.88 - ETA: 38s - loss: 0.3057 - acc: 0.88 - ETA: 33s - loss: 0.3045 - acc: 0.88 - ETA: 28s - loss: 0.3044 - acc: 0.88 - ETA: 23s - loss: 0.3048 - acc: 0.88 - ETA: 19s - loss: 0.3042 - acc: 0.88 - ETA: 14s - loss: 0.3046 - acc: 0.88 - ETA: 9s - loss: 0.3045 - acc: 0.8823 - ETA: 4s - loss: 0.3050 - acc: 0.8818\n",
      "Epoch 00093: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3042 - acc: 0.8827 - val_loss: 0.6720 - val_acc: 0.6698\n",
      "Epoch 94/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3185 - acc: 0.906 - ETA: 6:05 - loss: 0.2926 - acc: 0.927 - ETA: 6:00 - loss: 0.3115 - acc: 0.899 - ETA: 5:56 - loss: 0.3196 - acc: 0.895 - ETA: 5:51 - loss: 0.3125 - acc: 0.897 - ETA: 5:46 - loss: 0.3022 - acc: 0.906 - ETA: 5:42 - loss: 0.3042 - acc: 0.900 - ETA: 5:37 - loss: 0.2937 - acc: 0.907 - ETA: 5:32 - loss: 0.2974 - acc: 0.901 - ETA: 5:27 - loss: 0.2992 - acc: 0.895 - ETA: 5:22 - loss: 0.3026 - acc: 0.893 - ETA: 5:17 - loss: 0.3077 - acc: 0.886 - ETA: 5:13 - loss: 0.3140 - acc: 0.880 - ETA: 5:08 - loss: 0.3165 - acc: 0.879 - ETA: 5:03 - loss: 0.3121 - acc: 0.882 - ETA: 4:58 - loss: 0.3118 - acc: 0.882 - ETA: 4:53 - loss: 0.3170 - acc: 0.882 - ETA: 4:49 - loss: 0.3110 - acc: 0.887 - ETA: 4:44 - loss: 0.3089 - acc: 0.887 - ETA: 4:39 - loss: 0.3046 - acc: 0.891 - ETA: 4:34 - loss: 0.3024 - acc: 0.894 - ETA: 4:29 - loss: 0.3031 - acc: 0.895 - ETA: 4:25 - loss: 0.3025 - acc: 0.895 - ETA: 4:20 - loss: 0.3005 - acc: 0.896 - ETA: 4:15 - loss: 0.3013 - acc: 0.897 - ETA: 4:10 - loss: 0.3018 - acc: 0.896 - ETA: 4:05 - loss: 0.3031 - acc: 0.895 - ETA: 4:01 - loss: 0.3040 - acc: 0.894 - ETA: 3:56 - loss: 0.3051 - acc: 0.893 - ETA: 3:51 - loss: 0.3048 - acc: 0.894 - ETA: 3:46 - loss: 0.3056 - acc: 0.893 - ETA: 3:41 - loss: 0.3058 - acc: 0.893 - ETA: 3:36 - loss: 0.3034 - acc: 0.895 - ETA: 3:32 - loss: 0.3033 - acc: 0.894 - ETA: 3:27 - loss: 0.3047 - acc: 0.892 - ETA: 3:22 - loss: 0.3045 - acc: 0.892 - ETA: 3:17 - loss: 0.3043 - acc: 0.893 - ETA: 3:12 - loss: 0.3044 - acc: 0.892 - ETA: 3:07 - loss: 0.3039 - acc: 0.892 - ETA: 3:03 - loss: 0.3047 - acc: 0.892 - ETA: 2:58 - loss: 0.3047 - acc: 0.892 - ETA: 2:53 - loss: 0.3078 - acc: 0.890 - ETA: 2:48 - loss: 0.3103 - acc: 0.889 - ETA: 2:43 - loss: 0.3105 - acc: 0.888 - ETA: 2:39 - loss: 0.3094 - acc: 0.889 - ETA: 2:34 - loss: 0.3105 - acc: 0.889 - ETA: 2:29 - loss: 0.3103 - acc: 0.889 - ETA: 2:24 - loss: 0.3102 - acc: 0.888 - ETA: 2:19 - loss: 0.3090 - acc: 0.888 - ETA: 2:14 - loss: 0.3102 - acc: 0.887 - ETA: 2:10 - loss: 0.3095 - acc: 0.887 - ETA: 2:05 - loss: 0.3089 - acc: 0.888 - ETA: 2:00 - loss: 0.3086 - acc: 0.888 - ETA: 1:55 - loss: 0.3081 - acc: 0.888 - ETA: 1:50 - loss: 0.3072 - acc: 0.889 - ETA: 1:46 - loss: 0.3072 - acc: 0.889 - ETA: 1:41 - loss: 0.3065 - acc: 0.890 - ETA: 1:34 - loss: 0.3090 - acc: 0.887 - ETA: 1:30 - loss: 0.3083 - acc: 0.887 - ETA: 1:25 - loss: 0.3083 - acc: 0.887 - ETA: 1:20 - loss: 0.3065 - acc: 0.888 - ETA: 1:16 - loss: 0.3059 - acc: 0.888 - ETA: 1:11 - loss: 0.3054 - acc: 0.888 - ETA: 1:06 - loss: 0.3050 - acc: 0.888 - ETA: 1:01 - loss: 0.3042 - acc: 0.888 - ETA: 57s - loss: 0.3049 - acc: 0.887 - ETA: 52s - loss: 0.3044 - acc: 0.88 - ETA: 47s - loss: 0.3042 - acc: 0.88 - ETA: 42s - loss: 0.3060 - acc: 0.88 - ETA: 38s - loss: 0.3056 - acc: 0.88 - ETA: 33s - loss: 0.3060 - acc: 0.88 - ETA: 28s - loss: 0.3053 - acc: 0.88 - ETA: 23s - loss: 0.3047 - acc: 0.88 - ETA: 19s - loss: 0.3041 - acc: 0.88 - ETA: 14s - loss: 0.3050 - acc: 0.88 - ETA: 9s - loss: 0.3049 - acc: 0.8880 - ETA: 4s - loss: 0.3041 - acc: 0.8888\n",
      "Epoch 00094: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3031 - acc: 0.8893 - val_loss: 0.6923 - val_acc: 0.6578\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3215 - acc: 0.864 - ETA: 6:06 - loss: 0.3111 - acc: 0.885 - ETA: 6:01 - loss: 0.3149 - acc: 0.871 - ETA: 5:57 - loss: 0.3189 - acc: 0.872 - ETA: 5:52 - loss: 0.3082 - acc: 0.883 - ETA: 5:47 - loss: 0.3182 - acc: 0.869 - ETA: 5:42 - loss: 0.3114 - acc: 0.876 - ETA: 5:37 - loss: 0.3184 - acc: 0.868 - ETA: 5:33 - loss: 0.3203 - acc: 0.863 - ETA: 5:28 - loss: 0.3248 - acc: 0.858 - ETA: 5:23 - loss: 0.3216 - acc: 0.861 - ETA: 5:18 - loss: 0.3210 - acc: 0.862 - ETA: 5:13 - loss: 0.3193 - acc: 0.863 - ETA: 5:08 - loss: 0.3198 - acc: 0.867 - ETA: 5:03 - loss: 0.3182 - acc: 0.870 - ETA: 4:59 - loss: 0.3139 - acc: 0.875 - ETA: 4:54 - loss: 0.3117 - acc: 0.876 - ETA: 4:49 - loss: 0.3119 - acc: 0.877 - ETA: 4:44 - loss: 0.3085 - acc: 0.880 - ETA: 4:39 - loss: 0.3084 - acc: 0.879 - ETA: 4:34 - loss: 0.3094 - acc: 0.878 - ETA: 4:30 - loss: 0.3089 - acc: 0.879 - ETA: 4:25 - loss: 0.3088 - acc: 0.878 - ETA: 4:20 - loss: 0.3113 - acc: 0.876 - ETA: 4:15 - loss: 0.3096 - acc: 0.877 - ETA: 4:10 - loss: 0.3108 - acc: 0.877 - ETA: 4:05 - loss: 0.3094 - acc: 0.878 - ETA: 4:01 - loss: 0.3111 - acc: 0.878 - ETA: 3:56 - loss: 0.3116 - acc: 0.878 - ETA: 3:51 - loss: 0.3107 - acc: 0.879 - ETA: 3:46 - loss: 0.3091 - acc: 0.881 - ETA: 3:41 - loss: 0.3075 - acc: 0.881 - ETA: 3:37 - loss: 0.3076 - acc: 0.882 - ETA: 3:32 - loss: 0.3060 - acc: 0.882 - ETA: 3:27 - loss: 0.3074 - acc: 0.882 - ETA: 3:22 - loss: 0.3069 - acc: 0.881 - ETA: 3:17 - loss: 0.3078 - acc: 0.881 - ETA: 3:12 - loss: 0.3055 - acc: 0.883 - ETA: 3:08 - loss: 0.3053 - acc: 0.883 - ETA: 3:03 - loss: 0.3037 - acc: 0.885 - ETA: 2:58 - loss: 0.3057 - acc: 0.882 - ETA: 2:53 - loss: 0.3061 - acc: 0.882 - ETA: 2:48 - loss: 0.3049 - acc: 0.883 - ETA: 2:43 - loss: 0.3059 - acc: 0.883 - ETA: 2:39 - loss: 0.3053 - acc: 0.883 - ETA: 2:34 - loss: 0.3053 - acc: 0.882 - ETA: 2:29 - loss: 0.3053 - acc: 0.882 - ETA: 2:24 - loss: 0.3067 - acc: 0.881 - ETA: 2:19 - loss: 0.3067 - acc: 0.881 - ETA: 2:15 - loss: 0.3069 - acc: 0.880 - ETA: 2:10 - loss: 0.3067 - acc: 0.881 - ETA: 2:05 - loss: 0.3079 - acc: 0.879 - ETA: 2:00 - loss: 0.3074 - acc: 0.878 - ETA: 1:55 - loss: 0.3082 - acc: 0.878 - ETA: 1:50 - loss: 0.3091 - acc: 0.877 - ETA: 1:46 - loss: 0.3095 - acc: 0.877 - ETA: 1:41 - loss: 0.3098 - acc: 0.877 - ETA: 1:36 - loss: 0.3100 - acc: 0.878 - ETA: 1:31 - loss: 0.3110 - acc: 0.877 - ETA: 1:26 - loss: 0.3104 - acc: 0.877 - ETA: 1:21 - loss: 0.3093 - acc: 0.878 - ETA: 1:17 - loss: 0.3099 - acc: 0.877 - ETA: 1:12 - loss: 0.3098 - acc: 0.877 - ETA: 1:07 - loss: 0.3091 - acc: 0.878 - ETA: 1:02 - loss: 0.3093 - acc: 0.878 - ETA: 57s - loss: 0.3090 - acc: 0.878 - ETA: 53s - loss: 0.3091 - acc: 0.87 - ETA: 48s - loss: 0.3081 - acc: 0.87 - ETA: 43s - loss: 0.3082 - acc: 0.87 - ETA: 38s - loss: 0.3080 - acc: 0.87 - ETA: 33s - loss: 0.3077 - acc: 0.88 - ETA: 28s - loss: 0.3068 - acc: 0.88 - ETA: 23s - loss: 0.3100 - acc: 0.87 - ETA: 19s - loss: 0.3109 - acc: 0.87 - ETA: 14s - loss: 0.3102 - acc: 0.87 - ETA: 9s - loss: 0.3098 - acc: 0.8776 - ETA: 4s - loss: 0.3107 - acc: 0.8773\n",
      "Epoch 00095: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3107 - acc: 0.8774 - val_loss: 0.7034 - val_acc: 0.6566\n",
      "Epoch 96/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.2287 - acc: 0.937 - ETA: 6:06 - loss: 0.2582 - acc: 0.906 - ETA: 6:02 - loss: 0.2671 - acc: 0.906 - ETA: 5:57 - loss: 0.2651 - acc: 0.903 - ETA: 5:52 - loss: 0.2699 - acc: 0.910 - ETA: 5:47 - loss: 0.2695 - acc: 0.914 - ETA: 5:42 - loss: 0.2712 - acc: 0.912 - ETA: 5:37 - loss: 0.2838 - acc: 0.902 - ETA: 5:32 - loss: 0.2883 - acc: 0.899 - ETA: 5:28 - loss: 0.2859 - acc: 0.900 - ETA: 5:23 - loss: 0.2895 - acc: 0.897 - ETA: 5:18 - loss: 0.2926 - acc: 0.896 - ETA: 5:13 - loss: 0.2895 - acc: 0.898 - ETA: 5:08 - loss: 0.2904 - acc: 0.900 - ETA: 5:03 - loss: 0.2882 - acc: 0.902 - ETA: 4:59 - loss: 0.2940 - acc: 0.899 - ETA: 4:54 - loss: 0.2943 - acc: 0.900 - ETA: 4:49 - loss: 0.2939 - acc: 0.899 - ETA: 4:44 - loss: 0.2959 - acc: 0.898 - ETA: 4:39 - loss: 0.2948 - acc: 0.899 - ETA: 4:34 - loss: 0.2950 - acc: 0.899 - ETA: 4:30 - loss: 0.2949 - acc: 0.899 - ETA: 4:25 - loss: 0.2943 - acc: 0.899 - ETA: 4:11 - loss: 0.2957 - acc: 0.896 - ETA: 4:06 - loss: 0.2956 - acc: 0.897 - ETA: 4:02 - loss: 0.2968 - acc: 0.896 - ETA: 3:58 - loss: 0.2970 - acc: 0.896 - ETA: 3:53 - loss: 0.2991 - acc: 0.894 - ETA: 3:49 - loss: 0.3010 - acc: 0.893 - ETA: 3:44 - loss: 0.2994 - acc: 0.893 - ETA: 3:40 - loss: 0.2960 - acc: 0.896 - ETA: 3:35 - loss: 0.2961 - acc: 0.895 - ETA: 3:31 - loss: 0.2958 - acc: 0.895 - ETA: 3:26 - loss: 0.2962 - acc: 0.895 - ETA: 3:22 - loss: 0.2942 - acc: 0.897 - ETA: 3:17 - loss: 0.2943 - acc: 0.896 - ETA: 3:13 - loss: 0.2939 - acc: 0.897 - ETA: 3:08 - loss: 0.2961 - acc: 0.896 - ETA: 3:03 - loss: 0.2964 - acc: 0.896 - ETA: 2:59 - loss: 0.2960 - acc: 0.896 - ETA: 2:54 - loss: 0.2962 - acc: 0.895 - ETA: 2:50 - loss: 0.2971 - acc: 0.893 - ETA: 2:45 - loss: 0.2984 - acc: 0.892 - ETA: 2:40 - loss: 0.2993 - acc: 0.891 - ETA: 2:36 - loss: 0.2999 - acc: 0.890 - ETA: 2:31 - loss: 0.3006 - acc: 0.890 - ETA: 2:26 - loss: 0.3004 - acc: 0.890 - ETA: 2:22 - loss: 0.2987 - acc: 0.892 - ETA: 2:17 - loss: 0.2982 - acc: 0.892 - ETA: 2:12 - loss: 0.2980 - acc: 0.892 - ETA: 2:07 - loss: 0.2961 - acc: 0.894 - ETA: 2:03 - loss: 0.2957 - acc: 0.894 - ETA: 1:58 - loss: 0.2966 - acc: 0.893 - ETA: 1:53 - loss: 0.2970 - acc: 0.892 - ETA: 1:49 - loss: 0.2973 - acc: 0.892 - ETA: 1:44 - loss: 0.2985 - acc: 0.890 - ETA: 1:39 - loss: 0.2986 - acc: 0.889 - ETA: 1:35 - loss: 0.2987 - acc: 0.889 - ETA: 1:30 - loss: 0.3002 - acc: 0.888 - ETA: 1:25 - loss: 0.3004 - acc: 0.888 - ETA: 1:20 - loss: 0.3014 - acc: 0.888 - ETA: 1:16 - loss: 0.3019 - acc: 0.887 - ETA: 1:11 - loss: 0.3017 - acc: 0.887 - ETA: 1:06 - loss: 0.3039 - acc: 0.885 - ETA: 1:01 - loss: 0.3020 - acc: 0.886 - ETA: 57s - loss: 0.3017 - acc: 0.886 - ETA: 52s - loss: 0.3014 - acc: 0.88 - ETA: 47s - loss: 0.3016 - acc: 0.88 - ETA: 42s - loss: 0.3010 - acc: 0.88 - ETA: 38s - loss: 0.3020 - acc: 0.88 - ETA: 33s - loss: 0.3011 - acc: 0.88 - ETA: 28s - loss: 0.3019 - acc: 0.88 - ETA: 23s - loss: 0.3021 - acc: 0.88 - ETA: 19s - loss: 0.3022 - acc: 0.88 - ETA: 14s - loss: 0.3018 - acc: 0.88 - ETA: 9s - loss: 0.3014 - acc: 0.8877 - ETA: 4s - loss: 0.3015 - acc: 0.8869\n",
      "Epoch 00096: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3015 - acc: 0.8866 - val_loss: 0.6899 - val_acc: 0.6503\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.2125 - acc: 0.968 - ETA: 6:06 - loss: 0.2490 - acc: 0.916 - ETA: 6:01 - loss: 0.2669 - acc: 0.892 - ETA: 5:56 - loss: 0.3002 - acc: 0.862 - ETA: 5:51 - loss: 0.3021 - acc: 0.868 - ETA: 5:46 - loss: 0.2896 - acc: 0.878 - ETA: 5:42 - loss: 0.3002 - acc: 0.876 - ETA: 5:37 - loss: 0.2987 - acc: 0.877 - ETA: 5:32 - loss: 0.2945 - acc: 0.885 - ETA: 5:27 - loss: 0.2919 - acc: 0.887 - ETA: 5:22 - loss: 0.2957 - acc: 0.883 - ETA: 5:18 - loss: 0.2902 - acc: 0.887 - ETA: 5:13 - loss: 0.2913 - acc: 0.886 - ETA: 5:08 - loss: 0.2923 - acc: 0.883 - ETA: 5:03 - loss: 0.2971 - acc: 0.879 - ETA: 4:58 - loss: 0.2958 - acc: 0.882 - ETA: 4:54 - loss: 0.2982 - acc: 0.883 - ETA: 4:49 - loss: 0.2972 - acc: 0.884 - ETA: 4:44 - loss: 0.2973 - acc: 0.885 - ETA: 4:39 - loss: 0.2983 - acc: 0.884 - ETA: 4:34 - loss: 0.2980 - acc: 0.885 - ETA: 4:30 - loss: 0.2964 - acc: 0.886 - ETA: 4:25 - loss: 0.2951 - acc: 0.889 - ETA: 4:20 - loss: 0.2968 - acc: 0.886 - ETA: 4:15 - loss: 0.2988 - acc: 0.884 - ETA: 4:10 - loss: 0.2994 - acc: 0.884 - ETA: 4:05 - loss: 0.2981 - acc: 0.886 - ETA: 4:01 - loss: 0.3017 - acc: 0.884 - ETA: 3:56 - loss: 0.2986 - acc: 0.887 - ETA: 3:51 - loss: 0.2994 - acc: 0.886 - ETA: 3:46 - loss: 0.2994 - acc: 0.885 - ETA: 3:41 - loss: 0.3002 - acc: 0.885 - ETA: 3:36 - loss: 0.2990 - acc: 0.885 - ETA: 3:32 - loss: 0.2991 - acc: 0.884 - ETA: 3:27 - loss: 0.2991 - acc: 0.883 - ETA: 3:22 - loss: 0.2979 - acc: 0.885 - ETA: 3:17 - loss: 0.3009 - acc: 0.883 - ETA: 3:12 - loss: 0.2999 - acc: 0.884 - ETA: 3:08 - loss: 0.3012 - acc: 0.882 - ETA: 3:03 - loss: 0.3008 - acc: 0.883 - ETA: 2:58 - loss: 0.3006 - acc: 0.883 - ETA: 2:53 - loss: 0.3015 - acc: 0.884 - ETA: 2:48 - loss: 0.3037 - acc: 0.883 - ETA: 2:43 - loss: 0.3026 - acc: 0.883 - ETA: 2:39 - loss: 0.3018 - acc: 0.884 - ETA: 2:34 - loss: 0.3025 - acc: 0.883 - ETA: 2:29 - loss: 0.3013 - acc: 0.885 - ETA: 2:24 - loss: 0.3002 - acc: 0.886 - ETA: 2:19 - loss: 0.3001 - acc: 0.886 - ETA: 2:15 - loss: 0.3006 - acc: 0.887 - ETA: 2:10 - loss: 0.3002 - acc: 0.887 - ETA: 2:05 - loss: 0.3002 - acc: 0.887 - ETA: 2:00 - loss: 0.3000 - acc: 0.887 - ETA: 1:55 - loss: 0.3007 - acc: 0.887 - ETA: 1:50 - loss: 0.2998 - acc: 0.886 - ETA: 1:46 - loss: 0.3010 - acc: 0.886 - ETA: 1:41 - loss: 0.3014 - acc: 0.885 - ETA: 1:36 - loss: 0.3007 - acc: 0.886 - ETA: 1:31 - loss: 0.3014 - acc: 0.885 - ETA: 1:26 - loss: 0.3019 - acc: 0.885 - ETA: 1:21 - loss: 0.3029 - acc: 0.885 - ETA: 1:17 - loss: 0.3034 - acc: 0.885 - ETA: 1:12 - loss: 0.3028 - acc: 0.885 - ETA: 1:07 - loss: 0.3025 - acc: 0.885 - ETA: 1:01 - loss: 0.3052 - acc: 0.880 - ETA: 57s - loss: 0.3040 - acc: 0.881 - ETA: 52s - loss: 0.3040 - acc: 0.88 - ETA: 47s - loss: 0.3032 - acc: 0.88 - ETA: 42s - loss: 0.3033 - acc: 0.88 - ETA: 38s - loss: 0.3036 - acc: 0.88 - ETA: 33s - loss: 0.3038 - acc: 0.88 - ETA: 28s - loss: 0.3035 - acc: 0.88 - ETA: 23s - loss: 0.3032 - acc: 0.88 - ETA: 19s - loss: 0.3027 - acc: 0.88 - ETA: 14s - loss: 0.3041 - acc: 0.88 - ETA: 9s - loss: 0.3039 - acc: 0.8831 - ETA: 4s - loss: 0.3032 - acc: 0.8833\n",
      "Epoch 00097: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3027 - acc: 0.8839 - val_loss: 0.6965 - val_acc: 0.6616\n",
      "Epoch 98/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.3583 - acc: 0.833 - ETA: 6:05 - loss: 0.3466 - acc: 0.849 - ETA: 6:01 - loss: 0.3466 - acc: 0.854 - ETA: 5:56 - loss: 0.3528 - acc: 0.849 - ETA: 5:51 - loss: 0.3427 - acc: 0.862 - ETA: 5:46 - loss: 0.3453 - acc: 0.861 - ETA: 5:42 - loss: 0.3368 - acc: 0.869 - ETA: 5:37 - loss: 0.3335 - acc: 0.871 - ETA: 5:32 - loss: 0.3273 - acc: 0.877 - ETA: 5:27 - loss: 0.3236 - acc: 0.880 - ETA: 5:22 - loss: 0.3245 - acc: 0.880 - ETA: 5:17 - loss: 0.3273 - acc: 0.875 - ETA: 5:13 - loss: 0.3213 - acc: 0.879 - ETA: 5:08 - loss: 0.3190 - acc: 0.879 - ETA: 5:03 - loss: 0.3158 - acc: 0.881 - ETA: 4:58 - loss: 0.3155 - acc: 0.882 - ETA: 4:53 - loss: 0.3146 - acc: 0.884 - ETA: 4:49 - loss: 0.3094 - acc: 0.887 - ETA: 4:44 - loss: 0.3074 - acc: 0.888 - ETA: 4:39 - loss: 0.3076 - acc: 0.887 - ETA: 4:34 - loss: 0.3081 - acc: 0.884 - ETA: 4:29 - loss: 0.3081 - acc: 0.885 - ETA: 4:25 - loss: 0.3076 - acc: 0.885 - ETA: 4:20 - loss: 0.3060 - acc: 0.885 - ETA: 4:06 - loss: 0.3018 - acc: 0.890 - ETA: 4:02 - loss: 0.3014 - acc: 0.890 - ETA: 3:57 - loss: 0.3017 - acc: 0.890 - ETA: 3:53 - loss: 0.3036 - acc: 0.888 - ETA: 3:49 - loss: 0.3017 - acc: 0.889 - ETA: 3:44 - loss: 0.2990 - acc: 0.891 - ETA: 3:40 - loss: 0.2994 - acc: 0.891 - ETA: 3:35 - loss: 0.2995 - acc: 0.889 - ETA: 3:31 - loss: 0.2971 - acc: 0.891 - ETA: 3:26 - loss: 0.2967 - acc: 0.892 - ETA: 3:22 - loss: 0.2972 - acc: 0.892 - ETA: 3:17 - loss: 0.2982 - acc: 0.891 - ETA: 3:13 - loss: 0.2984 - acc: 0.891 - ETA: 3:08 - loss: 0.2969 - acc: 0.892 - ETA: 3:03 - loss: 0.2970 - acc: 0.892 - ETA: 2:59 - loss: 0.2964 - acc: 0.892 - ETA: 2:54 - loss: 0.2957 - acc: 0.892 - ETA: 2:49 - loss: 0.2941 - acc: 0.894 - ETA: 2:45 - loss: 0.2947 - acc: 0.893 - ETA: 2:40 - loss: 0.2937 - acc: 0.894 - ETA: 2:36 - loss: 0.2946 - acc: 0.894 - ETA: 2:31 - loss: 0.2949 - acc: 0.893 - ETA: 2:26 - loss: 0.2965 - acc: 0.892 - ETA: 2:22 - loss: 0.2974 - acc: 0.892 - ETA: 2:17 - loss: 0.2972 - acc: 0.892 - ETA: 2:12 - loss: 0.2979 - acc: 0.891 - ETA: 2:07 - loss: 0.2997 - acc: 0.890 - ETA: 2:03 - loss: 0.2996 - acc: 0.889 - ETA: 1:58 - loss: 0.2992 - acc: 0.889 - ETA: 1:53 - loss: 0.2997 - acc: 0.889 - ETA: 1:49 - loss: 0.2993 - acc: 0.890 - ETA: 1:44 - loss: 0.3008 - acc: 0.889 - ETA: 1:39 - loss: 0.3010 - acc: 0.888 - ETA: 1:34 - loss: 0.3009 - acc: 0.889 - ETA: 1:30 - loss: 0.3007 - acc: 0.888 - ETA: 1:25 - loss: 0.3002 - acc: 0.888 - ETA: 1:20 - loss: 0.2984 - acc: 0.890 - ETA: 1:16 - loss: 0.2982 - acc: 0.890 - ETA: 1:11 - loss: 0.2978 - acc: 0.891 - ETA: 1:06 - loss: 0.2976 - acc: 0.891 - ETA: 1:01 - loss: 0.2969 - acc: 0.891 - ETA: 57s - loss: 0.2968 - acc: 0.891 - ETA: 52s - loss: 0.2969 - acc: 0.89 - ETA: 47s - loss: 0.2954 - acc: 0.89 - ETA: 42s - loss: 0.2957 - acc: 0.89 - ETA: 38s - loss: 0.2956 - acc: 0.89 - ETA: 33s - loss: 0.2949 - acc: 0.89 - ETA: 28s - loss: 0.2954 - acc: 0.89 - ETA: 23s - loss: 0.2955 - acc: 0.89 - ETA: 19s - loss: 0.2959 - acc: 0.89 - ETA: 14s - loss: 0.2962 - acc: 0.89 - ETA: 9s - loss: 0.2975 - acc: 0.8899 - ETA: 4s - loss: 0.2972 - acc: 0.8903\n",
      "Epoch 00098: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.2972 - acc: 0.8905 - val_loss: 0.6694 - val_acc: 0.6660\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/78 [============================>.] - ETA: 6:11 - loss: 0.3161 - acc: 0.885 - ETA: 6:06 - loss: 0.3208 - acc: 0.869 - ETA: 6:01 - loss: 0.2989 - acc: 0.888 - ETA: 5:57 - loss: 0.2853 - acc: 0.898 - ETA: 5:52 - loss: 0.2840 - acc: 0.893 - ETA: 5:47 - loss: 0.2908 - acc: 0.895 - ETA: 5:42 - loss: 0.3018 - acc: 0.886 - ETA: 5:37 - loss: 0.2959 - acc: 0.888 - ETA: 5:32 - loss: 0.2973 - acc: 0.886 - ETA: 5:27 - loss: 0.3008 - acc: 0.888 - ETA: 5:23 - loss: 0.2941 - acc: 0.893 - ETA: 5:18 - loss: 0.2974 - acc: 0.892 - ETA: 5:13 - loss: 0.3030 - acc: 0.885 - ETA: 5:08 - loss: 0.3076 - acc: 0.879 - ETA: 5:03 - loss: 0.3105 - acc: 0.876 - ETA: 4:58 - loss: 0.3096 - acc: 0.875 - ETA: 4:54 - loss: 0.3098 - acc: 0.875 - ETA: 4:49 - loss: 0.3077 - acc: 0.876 - ETA: 4:44 - loss: 0.3084 - acc: 0.876 - ETA: 4:39 - loss: 0.3095 - acc: 0.877 - ETA: 4:34 - loss: 0.3081 - acc: 0.879 - ETA: 4:30 - loss: 0.3116 - acc: 0.875 - ETA: 4:25 - loss: 0.3134 - acc: 0.875 - ETA: 4:20 - loss: 0.3107 - acc: 0.875 - ETA: 4:15 - loss: 0.3086 - acc: 0.877 - ETA: 4:10 - loss: 0.3091 - acc: 0.876 - ETA: 4:05 - loss: 0.3084 - acc: 0.877 - ETA: 4:01 - loss: 0.3081 - acc: 0.877 - ETA: 3:56 - loss: 0.3101 - acc: 0.876 - ETA: 3:51 - loss: 0.3100 - acc: 0.875 - ETA: 3:46 - loss: 0.3086 - acc: 0.876 - ETA: 3:41 - loss: 0.3112 - acc: 0.874 - ETA: 3:36 - loss: 0.3124 - acc: 0.872 - ETA: 3:32 - loss: 0.3124 - acc: 0.873 - ETA: 3:27 - loss: 0.3138 - acc: 0.871 - ETA: 3:22 - loss: 0.3136 - acc: 0.872 - ETA: 3:17 - loss: 0.3134 - acc: 0.872 - ETA: 3:12 - loss: 0.3145 - acc: 0.871 - ETA: 3:08 - loss: 0.3145 - acc: 0.870 - ETA: 3:03 - loss: 0.3153 - acc: 0.869 - ETA: 2:58 - loss: 0.3158 - acc: 0.867 - ETA: 2:53 - loss: 0.3135 - acc: 0.870 - ETA: 2:48 - loss: 0.3149 - acc: 0.869 - ETA: 2:43 - loss: 0.3136 - acc: 0.871 - ETA: 2:39 - loss: 0.3124 - acc: 0.871 - ETA: 2:34 - loss: 0.3122 - acc: 0.871 - ETA: 2:29 - loss: 0.3120 - acc: 0.871 - ETA: 2:24 - loss: 0.3136 - acc: 0.870 - ETA: 2:17 - loss: 0.3123 - acc: 0.871 - ETA: 2:12 - loss: 0.3129 - acc: 0.871 - ETA: 2:07 - loss: 0.3126 - acc: 0.871 - ETA: 2:03 - loss: 0.3110 - acc: 0.872 - ETA: 1:58 - loss: 0.3109 - acc: 0.871 - ETA: 1:53 - loss: 0.3090 - acc: 0.872 - ETA: 1:49 - loss: 0.3099 - acc: 0.873 - ETA: 1:44 - loss: 0.3104 - acc: 0.873 - ETA: 1:39 - loss: 0.3107 - acc: 0.872 - ETA: 1:35 - loss: 0.3105 - acc: 0.872 - ETA: 1:30 - loss: 0.3100 - acc: 0.872 - ETA: 1:25 - loss: 0.3098 - acc: 0.873 - ETA: 1:20 - loss: 0.3082 - acc: 0.874 - ETA: 1:16 - loss: 0.3075 - acc: 0.875 - ETA: 1:11 - loss: 0.3085 - acc: 0.874 - ETA: 1:06 - loss: 0.3081 - acc: 0.875 - ETA: 1:01 - loss: 0.3069 - acc: 0.875 - ETA: 57s - loss: 0.3068 - acc: 0.875 - ETA: 52s - loss: 0.3072 - acc: 0.87 - ETA: 47s - loss: 0.3061 - acc: 0.87 - ETA: 42s - loss: 0.3062 - acc: 0.87 - ETA: 38s - loss: 0.3060 - acc: 0.87 - ETA: 33s - loss: 0.3058 - acc: 0.87 - ETA: 28s - loss: 0.3063 - acc: 0.87 - ETA: 23s - loss: 0.3053 - acc: 0.87 - ETA: 19s - loss: 0.3043 - acc: 0.87 - ETA: 14s - loss: 0.3037 - acc: 0.87 - ETA: 9s - loss: 0.3046 - acc: 0.8779 - ETA: 4s - loss: 0.3035 - acc: 0.8788\n",
      "Epoch 00099: val_loss did not improve\n",
      "78/78 [==============================] - 460s 6s/step - loss: 0.3035 - acc: 0.8789 - val_loss: 0.6863 - val_acc: 0.6572\n",
      "Epoch 100/100\n",
      "77/78 [============================>.] - ETA: 6:10 - loss: 0.2136 - acc: 0.937 - ETA: 6:06 - loss: 0.2450 - acc: 0.937 - ETA: 6:01 - loss: 0.2676 - acc: 0.906 - ETA: 5:56 - loss: 0.2831 - acc: 0.885 - ETA: 5:52 - loss: 0.2877 - acc: 0.881 - ETA: 5:47 - loss: 0.2837 - acc: 0.883 - ETA: 5:42 - loss: 0.2988 - acc: 0.866 - ETA: 5:37 - loss: 0.3022 - acc: 0.863 - ETA: 5:32 - loss: 0.3058 - acc: 0.862 - ETA: 5:27 - loss: 0.3066 - acc: 0.863 - ETA: 5:23 - loss: 0.3143 - acc: 0.859 - ETA: 5:18 - loss: 0.3097 - acc: 0.864 - ETA: 5:13 - loss: 0.3048 - acc: 0.869 - ETA: 5:08 - loss: 0.3046 - acc: 0.870 - ETA: 5:03 - loss: 0.3055 - acc: 0.870 - ETA: 4:59 - loss: 0.3014 - acc: 0.875 - ETA: 4:54 - loss: 0.3027 - acc: 0.876 - ETA: 4:49 - loss: 0.2975 - acc: 0.880 - ETA: 4:44 - loss: 0.2952 - acc: 0.883 - ETA: 4:39 - loss: 0.2986 - acc: 0.882 - ETA: 4:34 - loss: 0.2995 - acc: 0.881 - ETA: 4:30 - loss: 0.3031 - acc: 0.878 - ETA: 4:25 - loss: 0.3021 - acc: 0.880 - ETA: 4:20 - loss: 0.2976 - acc: 0.883 - ETA: 4:15 - loss: 0.2955 - acc: 0.885 - ETA: 4:10 - loss: 0.2956 - acc: 0.886 - ETA: 4:05 - loss: 0.2941 - acc: 0.886 - ETA: 4:01 - loss: 0.2964 - acc: 0.884 - ETA: 3:56 - loss: 0.2939 - acc: 0.885 - ETA: 3:51 - loss: 0.2912 - acc: 0.887 - ETA: 3:46 - loss: 0.2920 - acc: 0.885 - ETA: 3:41 - loss: 0.2916 - acc: 0.886 - ETA: 3:37 - loss: 0.2902 - acc: 0.887 - ETA: 3:32 - loss: 0.2903 - acc: 0.887 - ETA: 3:27 - loss: 0.2948 - acc: 0.884 - ETA: 3:22 - loss: 0.2937 - acc: 0.886 - ETA: 3:17 - loss: 0.2923 - acc: 0.886 - ETA: 3:12 - loss: 0.2912 - acc: 0.887 - ETA: 3:08 - loss: 0.2913 - acc: 0.887 - ETA: 3:03 - loss: 0.2926 - acc: 0.886 - ETA: 2:58 - loss: 0.2906 - acc: 0.888 - ETA: 2:53 - loss: 0.2893 - acc: 0.889 - ETA: 2:48 - loss: 0.2896 - acc: 0.890 - ETA: 2:44 - loss: 0.2897 - acc: 0.891 - ETA: 2:39 - loss: 0.2885 - acc: 0.891 - ETA: 2:34 - loss: 0.2871 - acc: 0.891 - ETA: 2:29 - loss: 0.2861 - acc: 0.892 - ETA: 2:24 - loss: 0.2867 - acc: 0.891 - ETA: 2:19 - loss: 0.2864 - acc: 0.892 - ETA: 2:15 - loss: 0.2846 - acc: 0.893 - ETA: 2:10 - loss: 0.2839 - acc: 0.893 - ETA: 2:05 - loss: 0.2826 - acc: 0.894 - ETA: 2:00 - loss: 0.2856 - acc: 0.891 - ETA: 1:55 - loss: 0.2871 - acc: 0.890 - ETA: 1:50 - loss: 0.2874 - acc: 0.889 - ETA: 1:46 - loss: 0.2877 - acc: 0.889 - ETA: 1:41 - loss: 0.2873 - acc: 0.890 - ETA: 1:36 - loss: 0.2868 - acc: 0.891 - ETA: 1:31 - loss: 0.2869 - acc: 0.891 - ETA: 1:26 - loss: 0.2881 - acc: 0.889 - ETA: 1:21 - loss: 0.2879 - acc: 0.889 - ETA: 1:17 - loss: 0.2873 - acc: 0.890 - ETA: 1:12 - loss: 0.2871 - acc: 0.890 - ETA: 1:07 - loss: 0.2870 - acc: 0.890 - ETA: 1:02 - loss: 0.2865 - acc: 0.890 - ETA: 57s - loss: 0.2872 - acc: 0.890 - ETA: 53s - loss: 0.2869 - acc: 0.89 - ETA: 48s - loss: 0.2864 - acc: 0.89 - ETA: 43s - loss: 0.2870 - acc: 0.89 - ETA: 38s - loss: 0.2874 - acc: 0.89 - ETA: 33s - loss: 0.2867 - acc: 0.89 - ETA: 28s - loss: 0.2862 - acc: 0.89 - ETA: 24s - loss: 0.2864 - acc: 0.89 - ETA: 19s - loss: 0.2871 - acc: 0.89 - ETA: 14s - loss: 0.2874 - acc: 0.89 - ETA: 9s - loss: 0.2887 - acc: 0.8901 - ETA: 4s - loss: 0.2898 - acc: 0.8893\n",
      "Epoch 00100: val_loss did not improve\n",
      "78/78 [==============================] - 464s 6s/step - loss: 0.2890 - acc: 0.8901 - val_loss: 0.6513 - val_acc: 0.6723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bd505072b0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "#model.fit_generator(...)\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    #samples_per_epoch=nb_train_samples,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks = [checkpoint, history ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"deep_transfer_3_class_ResNet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-c5b0905c4ac9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train losses\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val losses\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bd54c4f3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses, val_losses = history.losses, history.val_losses\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(model.history['loss'], 'g', label=\"train losses\")\n",
    "plt.plot(model.history['val_loss'], 'r', label=\"val losses\")\n",
    "plt.grid(True)\n",
    "plt.title('Training loss vs. Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\01\\\\44_1_rgb_191.png'\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\06\\\\44_6_rgb_191.png'\n",
    "img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\10\\\\41_10_rgb_191.png'\n",
    "\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\01\\\\42_1_rgb_191.png'\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\06\\\\42_6_rgb_191.png'\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\10\\\\42_10_rgb_191.png'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.623360 [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "result = model.predict(x)\n",
    "b = datetime.datetime.now()\n",
    "print(b-a, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.241165 [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "result = model.predict(x)\n",
    "b = datetime.datetime.now()\n",
    "print(b-a, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.248045 [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "result = model.predict(x)\n",
    "b = datetime.datetime.now()\n",
    "print(b-a, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
