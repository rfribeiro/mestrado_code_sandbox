{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os \n",
    "import glob\n",
    "#import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "from random import shuffle\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafae\\Anaconda3\\envs\\tf_vggface\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.applications\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "model_weights_checkpoint = 'vgg_face_eor_classes.h5'\n",
    "\n",
    "## Callback for loss logging per epoch\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "history = LossHistory()\n",
    "\n",
    "## Callback for early stopping the training\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=4,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(model_weights_checkpoint, monitor='val_loss', verbose=1, save_best_only=False, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "n_classes = 2\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could also be the output a different Keras model or layer\n",
    "input_tensor = Input(shape=(img_width, img_height, 3))  # this assumes K.image_data_format() == 'channels_last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = keras.applications.InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "#base_model = keras.applications.ResNet50(include_top=False, weights='imagenet')#, input_tensor=input_tensor)\n",
    "#base_model = keras.applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "base_model = VGGFace(include_top=False, input_shape=(img_width, img_height, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and save features from ir and depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IMAGES = \"C:\\\\Users\\\\rafae\\\\Desktop\\\\Coleta\\\\data\"\n",
    "\n",
    "IR_FOLDER = \"ir-subset-eor\"\n",
    "DEPTH_FOLDER =\"depth-subset-eor\"\n",
    "\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "\n",
    "EOR = 'EOR'\n",
    "NEOR = 'NEOR'\n",
    "\n",
    "DASH = \"\\\\\"\n",
    "\n",
    "IR1_PATTERN = '{:s}_{:s}_ir1_{:s}'\n",
    "IR2_PATTERN = '{:s}_{:s}_ir2_{:s}'\n",
    "DEP_PATTERN = '{:s}_{:s}_depth_{:s}'\n",
    "DEPC_PATTERN = '{:s}_{:s}_color_depth_color_{:s}'\n",
    "PNG = '.png'\n",
    "\n",
    "EOR_VALUE = 1\n",
    "NEOR_VALUE = 0\n",
    "\n",
    "def load_img(img_path):\n",
    "    global base_model\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = utils.preprocess_input(x, version=1)\n",
    "    return base_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "def load_features(folder, eor, eor_value):\n",
    "    PATTERN_IR = PATH_IMAGES + DASH + IR_FOLDER + DASH + folder + DASH + eor + DASH \n",
    "    PATTERN_DEP = PATH_IMAGES + DASH + DEPTH_FOLDER + DASH + folder + DASH + eor + DASH\n",
    "\n",
    "    features = pd.DataFrame(columns=[\"eor\", \"id\", \"point\", \"image\", \"ir1\", \"ir2\", \"dep_bw\", \"dep_rgb\"])\n",
    "    if debug: print(PATTERN_IR)\n",
    "\n",
    "    files_list = glob.glob(PATTERN_IR + \"*_*_ir1_*.png\")\n",
    "    if debug: print(len(files_list))\n",
    "\n",
    "    for f in files_list:\n",
    "        f1 = f.split('\\\\')[-1]\n",
    "        f = f1.split('_')\n",
    "        f[-1] = f[-1].split('.')[0]\n",
    "        if debug: print(f)\n",
    "\n",
    "        if not os.path.exists(PATTERN_IR + IR1_PATTERN.format(f[0], f[1], f[3]) + PNG) \\\n",
    "            or not os.path.exists(PATTERN_IR + IR2_PATTERN.format(f[0], f[1], f[3]) + PNG) \\\n",
    "            or not os.path.exists(PATTERN_DEP + DEP_PATTERN.format(f[0], f[1], f[3]) + PNG) \\\n",
    "            or not os.path.exists(PATTERN_DEP + DEPC_PATTERN.format(f[0], f[1], f[3]) + PNG):\n",
    "                continue\n",
    "        \n",
    "        features_ir1 = load_img(PATTERN_IR + IR1_PATTERN.format(f[0], f[1], f[3]) + PNG)\n",
    "        features_ir2 = load_img(PATTERN_IR + IR2_PATTERN.format(f[0], f[1], f[3]) + PNG)\n",
    "        features_dep = load_img(PATTERN_DEP + DEP_PATTERN.format(f[0], f[1], f[3]) + PNG)\n",
    "        features_depc = load_img(PATTERN_DEP + DEPC_PATTERN.format(f[0], f[1], f[3]) + PNG)\n",
    "\n",
    "        if debug: print(PATTERN_IR + IR1_PATTERN.format(f[0], f[1], f[3]) + PNG, features_ir1.shape)\n",
    "            \n",
    "        features = features.append({\"eor\": eor_value, \"id\": f[0], \"point\": f[1], \"image\": f[3], \\\n",
    "                        \"ir1\": features_ir1[0], \"ir2\": features_ir2[0], \"dep_bw\": features_dep[0], \"dep_rgb\": features_depc[0]},\\\n",
    "                        ignore_index=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_eor = load_features(TRAIN, EOR, EOR_VALUE)\n",
    "features_train_neor = load_features(TRAIN, NEOR, NEOR_VALUE)\n",
    "\n",
    "features_val_eor = load_features(VAL, EOR, EOR_VALUE)\n",
    "features_val_neor = load_features(VAL, NEOR, NEOR_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.concat([features_train_eor, features_train_neor])\n",
    "features_val = pd.concat([features_val_eor, features_val_neor])\n",
    "\n",
    "features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.to_csv('features_train.csv')\n",
    "#features_train.to_hdf('features_train.h5','df')\n",
    "\n",
    "features_val.to_csv('features_val.csv')\n",
    "#features_val.to_hdf('features_val.h5','df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.read_csv('features_train.csv')\n",
    "features_val = pd.read_csv('features_val.csv')\n",
    "\n",
    "features_train['eor'] = features_train['eor'].astype('category')\n",
    "features_val['eor'] = features_val['eor'].astype('category')\n",
    "\n",
    "features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train['eor'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train and val data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 2500\n",
    "nb_validation_samples = 560\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "samples_per_epoch= 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add a fully-connected layer\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "#model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"deep_transfer_eor_class_vggface.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a f1-score and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\01\\\\44_1_rgb_191.png'\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\06\\\\44_6_rgb_191.png'\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\10\\\\41_10_rgb_191.png'\n",
    "\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\01\\\\42_1_rgb_191.png'\n",
    "#img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\06\\\\42_6_rgb_291.png'3\n",
    "img_path = 'E:\\\\MestradoData\\\\preprocessed\\\\all\\\\10\\\\42_10_rgb_191.png'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "result = model.predict(x)\n",
    "b = datetime.datetime.now()\n",
    "print(b-a, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "result = model.predict(x)\n",
    "b = datetime.datetime.now()\n",
    "print(b-a, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "a = datetime.datetime.now()\n",
    "result = model.predict(x)\n",
    "b = datetime.datetime.now()\n",
    "print(b-a, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
