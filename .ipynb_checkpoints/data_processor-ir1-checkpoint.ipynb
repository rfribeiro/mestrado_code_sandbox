{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shift = 4320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'C:\\\\Users\\\\rafae\\\\Desktop\\\\Coleta\\\\features\\\\features_hog_*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "files_list = glob.glob(prefix)\n",
    "print(len(files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read features\n",
    "frames = [pd.read_csv(f, header=0) for f in files_list ]\n",
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7680, 17282)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17272</th>\n",
       "      <th>17273</th>\n",
       "      <th>17274</th>\n",
       "      <th>17275</th>\n",
       "      <th>17276</th>\n",
       "      <th>17277</th>\n",
       "      <th>17278</th>\n",
       "      <th>17279</th>\n",
       "      <th>17280</th>\n",
       "      <th>17281</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071793</td>\n",
       "      <td>0.090339</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.042653</td>\n",
       "      <td>0.053141</td>\n",
       "      <td>0.059538</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.127822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069514</td>\n",
       "      <td>0.119957</td>\n",
       "      <td>0.119957</td>\n",
       "      <td>0.119957</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.051084</td>\n",
       "      <td>0.053452</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.010533</td>\n",
       "      <td>0.119957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103166</td>\n",
       "      <td>0.037546</td>\n",
       "      <td>0.044686</td>\n",
       "      <td>0.122575</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.104095</td>\n",
       "      <td>0.087389</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.009884</td>\n",
       "      <td>0.017647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.091092</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.123109</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.132490</td>\n",
       "      <td>0.050142</td>\n",
       "      <td>0.042432</td>\n",
       "      <td>0.028560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>p002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052366</td>\n",
       "      <td>0.038015</td>\n",
       "      <td>0.034714</td>\n",
       "      <td>0.061695</td>\n",
       "      <td>0.040908</td>\n",
       "      <td>0.070168</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.023819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 17282 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1   2   3   4   5   6   7   8   9    ...        17272     17273  \\\n",
       "0  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.071793  0.090339   \n",
       "1  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.069514  0.119957   \n",
       "2  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.103166  0.037546   \n",
       "3  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.132490  0.091092   \n",
       "4  1  p002 NaN NaN NaN NaN NaN NaN NaN NaN    ...     0.052366  0.038015   \n",
       "\n",
       "      17274     17275     17276     17277     17278     17279     17280  \\\n",
       "0  0.127822  0.127822  0.042653  0.053141  0.059538  0.003019  0.019839   \n",
       "1  0.119957  0.119957  0.024254  0.051084  0.053452  0.014108  0.010533   \n",
       "2  0.044686  0.122575  0.033838  0.104095  0.087389  0.003405  0.009884   \n",
       "3  0.132490  0.123109  0.038147  0.132490  0.132490  0.050142  0.042432   \n",
       "4  0.034714  0.061695  0.040908  0.070168  0.032586  0.004734  0.009425   \n",
       "\n",
       "      17281  \n",
       "0  0.127822  \n",
       "1  0.119957  \n",
       "2  0.017647  \n",
       "3  0.028560  \n",
       "4  0.023819  \n",
       "\n",
       "[5 rows x 17282 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yData = ds['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IR1\n",
    "xData = pd.concat([ds.iloc[:,2+shift:2+shift+shift]])\n",
    "\n",
    "#IR1 + IR2\n",
    "xData = pd.concat([ds.iloc[:,2+shift:2+shift+shift], ds.iloc[:,2+shift+shift:2+shift+shift+shift]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yData.iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7680,) (7680, 4320) <class 'pandas.core.series.Series'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(yData.shape, xData.shape, type(yData), type(xData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, rotation=45)\n",
    "    plt.yticks(tick_marks)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Compute confusion matrix\n",
    "def plot_confusion(yTest, yTestPred, name):\n",
    "    cm = confusion_matrix(yTest, yTestPred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "    cm_normalized = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "    print('Classification report')\n",
    "    print(classification_report(yTest, yTestPred))\n",
    "    print('Normalized confusion matrix')\n",
    "    print(cm_normalized)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix (%s)' % (name))\n",
    "\n",
    "    plt.show()\n",
    "    # plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer,precision_recall_fscore_support\n",
    "\n",
    "def search(X_train, X_test, y_train, y_test, group_data_train):\n",
    "    \n",
    "    # normalize data\n",
    "    print(\"Normalizing data!\")\n",
    "    stdScale = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = stdScale.transform(X_train)\n",
    "    X_test = stdScale.transform(X_test)\n",
    "    \n",
    "    print(\"Grid Search Classifiers!\")\n",
    "    \n",
    "    knc = KNeighborsClassifier()\n",
    "    svc = SVC()\n",
    "    rfc = RandomForestClassifier()\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    clf1 = SVC()\n",
    "    clf2 = RandomForestClassifier(random_state=1)\n",
    "    clf3 = GaussianNB()\n",
    "    vt = VotingClassifier(estimators=[('svc', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "    kncp = [{'n_neighbors': [3, 5, 7],\n",
    "             'weights': ['uniform','distance'],\n",
    "              'algorithm': ['auto','ball_tree', 'kd_tree', 'brute']}]\n",
    "    svcp = [{'kernel': ['rbf'], #{'kernel': ['rbf','linear'], \n",
    "             'class_weight':['balanced'],\n",
    "             'gamma': ['auto',1e-2, 1e-3, 1e-4],\n",
    "             'C': [0.01, 0.1, 1.0, 1, 10, 50, 100, 500, 1000]}]\n",
    "    rfcp = [{'n_estimators': [10, 20, 50, 100,200,300], \n",
    "            'max_depth': [None, 1, 10, 100],\n",
    "            'bootstrap': [True, False],\n",
    "            'criterion': [\"gini\", \"entropy\"]}]\n",
    "    gbp = [{#'loss' : ['deviance', 'exponential'],\n",
    "           'n_estimators': [50,100],\n",
    "           'learning_rate': [0.1,1.0,10],\n",
    "           'max_depth' : [3,5,10]\n",
    "            }]\n",
    "           #'min_impurity_decrease': [0.0]}]#, 0.1]}]\n",
    "    vtp = [{'svc__C': [1.0, 100.0], \n",
    "            'rf__n_estimators': [20, 200],}]\n",
    "   \n",
    "    classifiers = [#('kNN', knc, kncp),                                 \n",
    "                    ('Support Vector', svc, svcp),\n",
    "                    #('Random Forest', rfc, rfcp),\n",
    "                    #('Gradient Boosting', gb, gbp),\n",
    "                    #('Vooting', vt, vtp)\n",
    "    ]\n",
    "    \n",
    "    for name, classifier, params in classifiers:\n",
    "        print(name)\n",
    "        clf = GridSearchCV(classifier, params, cv=5, n_jobs=4, scoring=['f1_weighted','accuracy','precision_weighted', 'recall_weighted'], refit='f1_weighted' , verbose = 10)\n",
    "\n",
    "        clf.fit(X_train, y_train, groups=group_data_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print(clf.best_score_)\n",
    "        print()\n",
    "        print(clf.cv_results_.keys())\n",
    "        means = clf.cv_results_['mean_test_accuracy']\n",
    "        stds = clf.cv_results_['std_test_accuracy']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        yTrue, yPred = y_test, clf.predict(X_test)\n",
    "        print(classification_report(yTrue, yPred))\n",
    "        plot_confusion(yTrue, yPred, name)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(yData)\n",
    "print(le.classes_)\n",
    "yDataBin = le.transform(yData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4322</th>\n",
       "      <th>4323</th>\n",
       "      <th>4324</th>\n",
       "      <th>4325</th>\n",
       "      <th>4326</th>\n",
       "      <th>4327</th>\n",
       "      <th>4328</th>\n",
       "      <th>4329</th>\n",
       "      <th>4330</th>\n",
       "      <th>4331</th>\n",
       "      <th>...</th>\n",
       "      <th>8632</th>\n",
       "      <th>8633</th>\n",
       "      <th>8634</th>\n",
       "      <th>8635</th>\n",
       "      <th>8636</th>\n",
       "      <th>8637</th>\n",
       "      <th>8638</th>\n",
       "      <th>8639</th>\n",
       "      <th>8640</th>\n",
       "      <th>8641</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.063815</td>\n",
       "      <td>0.763937</td>\n",
       "      <td>1.236068</td>\n",
       "      <td>1.104165</td>\n",
       "      <td>1.358894</td>\n",
       "      <td>1.143303</td>\n",
       "      <td>0.199255</td>\n",
       "      <td>0.640129</td>\n",
       "      <td>0.215195</td>\n",
       "      <td>1.358894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.024278</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.153853</td>\n",
       "      <td>0.153853</td>\n",
       "      <td>0.040803</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.328088</td>\n",
       "      <td>0.976935</td>\n",
       "      <td>1.191321</td>\n",
       "      <td>0.823355</td>\n",
       "      <td>1.191321</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.099104</td>\n",
       "      <td>0.511228</td>\n",
       "      <td>0.325728</td>\n",
       "      <td>1.191321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063040</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.005897</td>\n",
       "      <td>0.012065</td>\n",
       "      <td>0.014490</td>\n",
       "      <td>0.158639</td>\n",
       "      <td>0.157743</td>\n",
       "      <td>0.056158</td>\n",
       "      <td>0.015814</td>\n",
       "      <td>0.012611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972757</td>\n",
       "      <td>0.503622</td>\n",
       "      <td>1.029105</td>\n",
       "      <td>0.545039</td>\n",
       "      <td>0.644840</td>\n",
       "      <td>0.312670</td>\n",
       "      <td>0.443096</td>\n",
       "      <td>0.166702</td>\n",
       "      <td>0.102734</td>\n",
       "      <td>0.598913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047488</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.022533</td>\n",
       "      <td>0.058796</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>0.171280</td>\n",
       "      <td>0.171280</td>\n",
       "      <td>0.085843</td>\n",
       "      <td>0.039564</td>\n",
       "      <td>0.030823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.634255</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>1.099252</td>\n",
       "      <td>0.730014</td>\n",
       "      <td>1.288830</td>\n",
       "      <td>0.269786</td>\n",
       "      <td>0.467282</td>\n",
       "      <td>0.513752</td>\n",
       "      <td>0.247441</td>\n",
       "      <td>1.288830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093524</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.021031</td>\n",
       "      <td>0.079221</td>\n",
       "      <td>0.150425</td>\n",
       "      <td>0.150425</td>\n",
       "      <td>0.029271</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.001955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.779654</td>\n",
       "      <td>1.229032</td>\n",
       "      <td>0.367538</td>\n",
       "      <td>0.836201</td>\n",
       "      <td>1.158373</td>\n",
       "      <td>0.383974</td>\n",
       "      <td>0.431526</td>\n",
       "      <td>0.233317</td>\n",
       "      <td>0.333384</td>\n",
       "      <td>1.208466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059620</td>\n",
       "      <td>0.068514</td>\n",
       "      <td>0.023649</td>\n",
       "      <td>0.067251</td>\n",
       "      <td>0.012750</td>\n",
       "      <td>0.181533</td>\n",
       "      <td>0.181533</td>\n",
       "      <td>0.102462</td>\n",
       "      <td>0.027024</td>\n",
       "      <td>0.074032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4320 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       4322      4323      4324      4325      4326      4327      4328  \\\n",
       "0  1.063815  0.763937  1.236068  1.104165  1.358894  1.143303  0.199255   \n",
       "1  0.328088  0.976935  1.191321  0.823355  1.191321  0.517857  0.099104   \n",
       "2  0.972757  0.503622  1.029105  0.545039  0.644840  0.312670  0.443096   \n",
       "3  0.634255  0.502674  1.099252  0.730014  1.288830  0.269786  0.467282   \n",
       "4  0.779654  1.229032  0.367538  0.836201  1.158373  0.383974  0.431526   \n",
       "\n",
       "       4329      4330      4331    ...         8632      8633      8634  \\\n",
       "0  0.640129  0.215195  1.358894    ...     0.012610  0.024278  0.003198   \n",
       "1  0.511228  0.325728  1.191321    ...     0.063040  0.018779  0.005897   \n",
       "2  0.166702  0.102734  0.598913    ...     0.047488  0.016200  0.022533   \n",
       "3  0.513752  0.247441  1.288830    ...     0.093524  0.008718  0.002170   \n",
       "4  0.233317  0.333384  1.208466    ...     0.059620  0.068514  0.023649   \n",
       "\n",
       "       8635      8636      8637      8638      8639      8640      8641  \n",
       "0  0.001430  0.011652  0.153853  0.153853  0.040803  0.007770  0.001548  \n",
       "1  0.012065  0.014490  0.158639  0.157743  0.056158  0.015814  0.012611  \n",
       "2  0.058796  0.017773  0.171280  0.171280  0.085843  0.039564  0.030823  \n",
       "3  0.021031  0.079221  0.150425  0.150425  0.029271  0.004757  0.001955  \n",
       "4  0.067251  0.012750  0.181533  0.181533  0.102462  0.027024  0.074032  \n",
       "\n",
       "[5 rows x 4320 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (5430,) (2250,)\n",
      "TRAIN: [   0    1    2 ..., 7677 7678 7679] TEST: [ 570  571  572 ..., 7122 7123 7124]\n",
      "['p002' 'p003' 'p005' 'p006' 'p008' 'p011' 'p012' 'p013' 'p014' 'p020'\n",
      " 'p021' 'p022' 'p023' 'p024' 'p026' 'p027' 'p028' 'p029' 'p030' 'p032'\n",
      " 'p033']\n",
      "['p004' 'p007' 'p009' 'p015' 'p016' 'p019' 'p025' 'p031']\n",
      "1     315\n",
      "0     306\n",
      "4     300\n",
      "3     300\n",
      "5     300\n",
      "2     298\n",
      "6     297\n",
      "11    285\n",
      "12    285\n",
      "15    285\n",
      "10    285\n",
      "8     284\n",
      "13    270\n",
      "9     270\n",
      "14    270\n",
      "16    270\n",
      "18    270\n",
      "7     270\n",
      "17    270\n",
      "Name: 0, dtype: int64\n",
      "17    120\n",
      "16    120\n",
      "2     120\n",
      "6     120\n",
      "8     120\n",
      "10    120\n",
      "12    120\n",
      "14    120\n",
      "18    120\n",
      "1     120\n",
      "3     120\n",
      "5     120\n",
      "7     120\n",
      "9     120\n",
      "11    120\n",
      "13    120\n",
      "0     120\n",
      "15    105\n",
      "4     105\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "\n",
      "Normalizing data!\n",
      "Grid Search Classifiers!\n",
      "Support Vector\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed: 56.6min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed: 84.9min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed: 145.8min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed: 186.5min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed: 303.5min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 364.4min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed: 444.0min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 497.8min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed: 604.5min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed: 690.1min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed: 791.5min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed: 876.4min\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed: 1014.0min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 1134.0min\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed: 1275.0min\n",
      "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed: 1303.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.377981610982\n",
      "\n",
      "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_C', 'param_class_weight', 'param_gamma', 'param_kernel', 'params', 'split0_test_f1_weighted', 'split1_test_f1_weighted', 'split2_test_f1_weighted', 'split3_test_f1_weighted', 'split4_test_f1_weighted', 'mean_test_f1_weighted', 'std_test_f1_weighted', 'rank_test_f1_weighted', 'split0_train_f1_weighted', 'split1_train_f1_weighted', 'split2_train_f1_weighted', 'split3_train_f1_weighted', 'split4_train_f1_weighted', 'mean_train_f1_weighted', 'std_train_f1_weighted', 'split0_test_accuracy', 'split1_test_accuracy', 'split2_test_accuracy', 'split3_test_accuracy', 'split4_test_accuracy', 'mean_test_accuracy', 'std_test_accuracy', 'rank_test_accuracy', 'split0_train_accuracy', 'split1_train_accuracy', 'split2_train_accuracy', 'split3_train_accuracy', 'split4_train_accuracy', 'mean_train_accuracy', 'std_train_accuracy', 'split0_test_precision_weighted', 'split1_test_precision_weighted', 'split2_test_precision_weighted', 'split3_test_precision_weighted', 'split4_test_precision_weighted', 'mean_test_precision_weighted', 'std_test_precision_weighted', 'rank_test_precision_weighted', 'split0_train_precision_weighted', 'split1_train_precision_weighted', 'split2_train_precision_weighted', 'split3_train_precision_weighted', 'split4_train_precision_weighted', 'mean_train_precision_weighted', 'std_train_precision_weighted', 'split0_test_recall_weighted', 'split1_test_recall_weighted', 'split2_test_recall_weighted', 'split3_test_recall_weighted', 'split4_test_recall_weighted', 'mean_test_recall_weighted', 'std_test_recall_weighted', 'rank_test_recall_weighted', 'split0_train_recall_weighted', 'split1_train_recall_weighted', 'split2_train_recall_weighted', 'split3_train_recall_weighted', 'split4_train_recall_weighted', 'mean_train_recall_weighted', 'std_train_recall_weighted'])\n",
      "0.079 (+/-0.045) for {'C': 0.01, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.054 (+/-0.005) for {'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.065 (+/-0.035) for {'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.081 (+/-0.038) for {'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.272 (+/-0.084) for {'C': 0.1, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.053 (+/-0.005) for {'C': 0.1, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.095 (+/-0.041) for {'C': 0.1, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.269 (+/-0.077) for {'C': 0.1, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.360 (+/-0.094) for {'C': 1.0, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.058 (+/-0.002) for {'C': 1.0, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.237 (+/-0.104) for {'C': 1.0, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.362 (+/-0.083) for {'C': 1.0, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.360 (+/-0.094) for {'C': 1, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.058 (+/-0.002) for {'C': 1, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.237 (+/-0.104) for {'C': 1, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.362 (+/-0.083) for {'C': 1, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.370 (+/-0.090) for {'C': 10, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.058 (+/-0.000) for {'C': 10, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.241 (+/-0.104) for {'C': 10, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.385 (+/-0.083) for {'C': 10, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.370 (+/-0.090) for {'C': 50, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.058 (+/-0.000) for {'C': 50, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.241 (+/-0.104) for {'C': 50, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.384 (+/-0.085) for {'C': 50, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.370 (+/-0.090) for {'C': 100, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.058 (+/-0.000) for {'C': 100, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.241 (+/-0.104) for {'C': 100, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.384 (+/-0.085) for {'C': 100, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.370 (+/-0.090) for {'C': 500, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.058 (+/-0.000) for {'C': 500, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.241 (+/-0.104) for {'C': 500, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.384 (+/-0.085) for {'C': 500, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.370 (+/-0.090) for {'C': 1000, 'class_weight': 'balanced', 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.058 (+/-0.000) for {'C': 1000, 'class_weight': 'balanced', 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.241 (+/-0.104) for {'C': 1000, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.384 (+/-0.085) for {'C': 1000, 'class_weight': 'balanced', 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.87      0.65       120\n",
      "          1       0.46      0.15      0.23       120\n",
      "          2       0.80      0.63      0.71       120\n",
      "          3       0.25      0.37      0.30       120\n",
      "          4       0.31      0.37      0.34       105\n",
      "          5       0.10      0.07      0.08       120\n",
      "          6       0.17      0.09      0.12       120\n",
      "          7       0.23      0.20      0.21       120\n",
      "          8       0.45      0.44      0.44       120\n",
      "          9       0.37      0.37      0.37       120\n",
      "         10       0.39      0.42      0.41       120\n",
      "         11       0.29      0.33      0.31       120\n",
      "         12       0.32      0.54      0.40       120\n",
      "         13       0.22      0.33      0.26       120\n",
      "         14       0.10      0.17      0.13       120\n",
      "         15       0.22      0.14      0.17       105\n",
      "         16       0.14      0.07      0.09       120\n",
      "         17       0.00      0.00      0.00       120\n",
      "         18       0.45      0.24      0.32       120\n",
      "\n",
      "avg / total       0.30      0.31      0.29      2250\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.87      0.65       120\n",
      "          1       0.46      0.15      0.23       120\n",
      "          2       0.80      0.63      0.71       120\n",
      "          3       0.25      0.37      0.30       120\n",
      "          4       0.31      0.37      0.34       105\n",
      "          5       0.10      0.07      0.08       120\n",
      "          6       0.17      0.09      0.12       120\n",
      "          7       0.23      0.20      0.21       120\n",
      "          8       0.45      0.44      0.44       120\n",
      "          9       0.37      0.37      0.37       120\n",
      "         10       0.39      0.42      0.41       120\n",
      "         11       0.29      0.33      0.31       120\n",
      "         12       0.32      0.54      0.40       120\n",
      "         13       0.22      0.33      0.26       120\n",
      "         14       0.10      0.17      0.13       120\n",
      "         15       0.22      0.14      0.17       105\n",
      "         16       0.14      0.07      0.09       120\n",
      "         17       0.00      0.00      0.00       120\n",
      "         18       0.45      0.24      0.32       120\n",
      "\n",
      "avg / total       0.30      0.31      0.29      2250\n",
      "\n",
      "Normalized confusion matrix\n",
      "[[ 86.67  12.5    0.     0.     0.     0.     0.     0.     0.     0.83\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [ 80.83  15.     0.     0.     0.     0.     0.     0.     0.     1.67\n",
      "    0.     0.     0.     0.     0.     1.67   0.     0.     0.83]\n",
      " [  0.83   5.    63.33   0.     0.     0.     0.     0.     0.     8.33\n",
      "    0.83   2.5   13.33   5.83   0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    36.67  20.     0.     0.83   0.     0.     0.     0.\n",
      "   30.83   0.     0.     4.17   0.     0.83   6.67   0.  ]\n",
      " [  0.     0.     0.    24.76  37.14  20.95   1.9    0.     0.     0.     0.\n",
      "    1.9    0.     0.     0.     0.     0.     0.95  12.38]\n",
      " [  0.     0.     0.    16.67  32.5    6.67   8.33   0.     0.     0.     0.\n",
      "   13.33   0.     0.    15.     0.     0.     4.17   3.33]\n",
      " [  0.     0.     0.     8.33   8.33   1.67   9.17  23.33   0.     0.\n",
      "    3.33   5.     0.     0.    36.67   0.     0.     4.17   0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     2.5   20.     0.     0.\n",
      "    0.83   0.     0.    23.33  27.5    0.    10.83  15.     0.  ]\n",
      " [  0.     0.     0.     0.     3.33   0.     0.83   0.    44.17   3.33\n",
      "    0.     0.    15.     0.83   0.    26.67   0.     0.     5.83]\n",
      " [  0.     0.    11.67   0.     0.     0.     0.     0.     0.    36.67\n",
      "    0.     0.    45.     1.67   0.     5.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    12.5    0.     0.     4.17   0.     0.     0.    42.5\n",
      "    3.33   0.    14.17  11.67   0.    10.83   0.83   0.  ]\n",
      " [  0.     0.     0.    32.5    4.17   2.5    0.83   0.     0.     0.\n",
      "   15.83  32.5    0.     1.67   5.     0.     0.83   4.17   0.  ]\n",
      " [  0.     0.     1.67   0.     0.     0.     0.     0.83   0.    25.     0.\n",
      "    0.    54.17  13.33   0.     0.83   3.33   0.     0.83]\n",
      " [  0.     0.     0.83   0.     0.    10.     8.33   0.     0.     0.\n",
      "    3.33   0.    12.5   33.33   6.67   0.    12.5   12.5    0.  ]\n",
      " [  0.     0.     0.    11.67   1.67   9.17  14.17   7.5    0.     0.    10.\n",
      "    3.33   0.     5.    17.5    0.     0.    12.5    7.5 ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.    22.86  25.71\n",
      "    0.     0.    32.38   4.76   0.    14.29   0.     0.     0.  ]\n",
      " [  0.     0.     1.67   0.     0.     9.17   0.     1.67   6.67   0.    12.5\n",
      "    0.     0.83  44.17  14.17   0.83   6.67   1.67   0.  ]\n",
      " [  0.     0.     0.     4.17   0.    10.     4.17  20.83   0.     0.\n",
      "   15.83  10.     0.     2.5   32.5    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     1.67   0.     0.    14.17  28.33   0.\n",
      "    4.17   8.33   0.     3.33   4.17   9.17   2.5    0.    24.17]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAFgCAYAAADgjFEzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzIHVzKzNHFjNzNrs/wPeISXiUqMUdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2080e0ba7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'numpy.ndarray'> (5448,) (2232,)\n",
      "TRAIN: [   0    1    2 ..., 7677 7678 7679] TEST: [ 855  856  857 ..., 7407 7408 7409]\n",
      "['p002' 'p003' 'p004' 'p006' 'p008' 'p009' 'p011' 'p012' 'p013' 'p014'\n",
      " 'p015' 'p016' 'p020' 'p023' 'p024' 'p025' 'p027' 'p029' 'p030' 'p031'\n",
      " 'p033']\n",
      "['p005' 'p007' 'p019' 'p021' 'p022' 'p026' 'p028' 'p032']\n",
      "1     315\n",
      "0     306\n",
      "5     300\n",
      "4     300\n",
      "3     300\n",
      "2     300\n",
      "6     297\n",
      "8     285\n",
      "12    285\n",
      "15    285\n",
      "11    285\n",
      "10    285\n",
      "14    285\n",
      "13    270\n",
      "9     270\n",
      "16    270\n",
      "18    270\n",
      "7     270\n",
      "17    270\n",
      "Name: 0, dtype: int64\n",
      "17    120\n",
      "3     120\n",
      "6     120\n",
      "10    120\n",
      "12    120\n",
      "16    120\n",
      "1     120\n",
      "18    120\n",
      "5     120\n",
      "7     120\n",
      "9     120\n",
      "11    120\n",
      "13    120\n",
      "0     120\n",
      "8     119\n",
      "2     118\n",
      "15    105\n",
      "14    105\n",
      "4     105\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "\n",
      "Normalizing data!\n",
      "Grid Search Classifiers!\n",
      "Support Vector\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed: 62.1min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed: 93.3min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed: 154.9min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed: 185.6min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed: 272.0min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 323.7min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed: 398.8min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 446.2min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed: 535.3min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed: 615.5min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b4aa8341b261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     search(xData.iloc[train_index,:], xData.iloc[test_index,:], \n\u001b[0;32m     20\u001b[0m            \u001b[0myDataBin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myDataBin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m            ds.iloc[train_index,1])    \n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-36b5a91dde7f>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(X_train, X_test, y_train, y_test, group_data_train)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'precision_weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'recall_weighted'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_data_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters set found on development set:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 638\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit,GroupShuffleSplit\n",
    "from scipy import stats\n",
    "\n",
    "#gss = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=0)\n",
    "gss = GroupShuffleSplit(n_splits=2, test_size=0.25, random_state=10)\n",
    "\n",
    "for train_index, test_index in gss.split(xData, yDataBin, groups=ds['1']):\n",
    "    print(type(train_index), train_index.shape, test_index.shape)\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(np.unique(ds.iloc[train_index,1]))\n",
    "    print(np.unique(ds.iloc[test_index,1]))\n",
    "    dt = pd.DataFrame(yDataBin[train_index])\n",
    "    print(dt[0].value_counts())\n",
    "    dte = pd.DataFrame(yDataBin[test_index])\n",
    "    print(dte[0].value_counts())\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    search(xData.iloc[train_index,:], xData.iloc[test_index,:], \n",
    "           yDataBin[train_index], yDataBin[test_index], \n",
    "           ds.iloc[train_index,1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(xData, yDataBin, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape,\n",
    "      type(X_train.shape), type(X_test.shape), type(y_train.shape), type(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(nds.iloc[:,2:])\n",
    "nnds = pca.transform(nds.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
